{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\fnil\fcharset0 LucidaGrande;\f2\fswiss\fcharset0 Helvetica;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 MANNING Doug Hudgeon Richard Nichol Using Amazon SageMaker and Jupyter Each scenario chapter covers an operational area of a typical company. Chapters 3 and 4 (retention and support) deal with customers. Chapters 2 and 5 (purchase approval and invoice audit) deal with suppliers. And chapters 6 and 7 deal with facilities management (power consumption forecasting). Your company Customers Support (ch04) Retention (ch03) Suppliers Facilities Purchase approval (ch02) Invoice audit (ch05) Power consumption forecasting (ch06 & ch07) Machine Learning for Business USING AMAZON SAGEMAKER AND JUPYTER DOUG HUDGEON AND RICHARD NICHOL MANNING SHELTER ISLAND For online information and ordering of this and other Manning books, please visit www.manning.com. The publisher offers discounts on this book when ordered in quantity. For more information, please contact Special Sales Department Manning Publications Co. 20 Baldwin Road PO Box 761 Shelter Island, NY 11964 Email: orders@manning.com \'a92020 by Manning Publications Co. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher. Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps. Recognizing the importance of preserving what has been written, it is Manning\'92s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine. Development editor: Toni Arritola Technical development editor: Arthur Zubarev Manning Publications Co. Review editor: Ivan Martinovic\'b4 20 Baldwin Road Production editor: Deirdre Hiam PO Box 761 Copy editor: Frances Buran Shelter Island, NY 11964 Proofreader: Katie Tennant Technical proofreader: Karsten Str\'f8b\'e6k Typesetter: Dennis Dalinnik Cover designer: Marija Tudor ISBN: 9781617295836 Printed in the United States of America iii brief contents PART 1MACHINE LEARNING FOR BUSINESS ................................1 1 
\f1 \uc0\u9632 
\f0  How machine learning applies to your business 3 PART 2SIX SCENARIOS: MACHINE LEARNING FOR BUSINESS ......23 2 
\f1 \uc0\u9632 
\f0  Should you send a purchase order to a technical approver? 25 3 
\f1 \uc0\u9632 
\f0  Should you call a customer because they are at risk of churning? 49 4 
\f1 \uc0\u9632 
\f0  Should an incident be escalated to your support team? 76 5 
\f1 \uc0\u9632 
\f0  Should you question an invoice sent by a supplier? 99 6 
\f1 \uc0\u9632 
\f0  Forecasting your company\'92s monthly power usage 128 7 
\f1 \uc0\u9632 
\f0  Improving your company\'92s monthly power usage forecast 161 PART 3MOVING MACHINE LEARNING INTO PRODUCTION.......185 8 
\f1 \uc0\u9632 
\f0  Serving predictions over the web 187 9 
\f1 \uc0\u9632 
\f0  Case studies 211 v contents preface xiii acknowledgments xv about this book xvii about the authors xx about the cover illustration xxi PART 1MACHINE LEARNING FOR BUSINESS ......................1 1 How machine learning applies to your business 3 1.1 Why are our business systems so terrible? 4 1.2 Why is automation important now? 8 What is productivity? 9 
\f1 \uc0\u9632 
\f0  How will machine learning improve productivity? 9 1.3 How do machines make decisions? 10 People: Rules-based or not? 10 
\f1 \uc0\u9632 
\f0  Can you trust a pattern-based answer? 11 
\f1 \uc0\u9632 
\f0  How can machine learning improve your business systems? 12 1.4 Can a machine help Karen make decisions? 12 Target variables 13 
\f1 \uc0\u9632 
\f0  Features 13 1.5 How does a machine learn? 14 vi CONTENTS 1.6 Getting approval in your company to use machine learning to make decisions 17 1.7 The tools 18 What are AWS and SageMaker, and how can they help you? 18 What is a Jupyter notebook? 19 1.8 Setting up SageMaker in preparation for tackling the scenarios in chapters 2 through 7 19 1.9 The time to act is now 20 PART 2SIX SCENARIOS: MACHINE LEARNING FOR BUSINESS....................................................23 2 Should you send a purchase order to a technical approver? 25 2.1 The decision 26 2.2 The data 27 2.3 Putting on your training wheels 28 2.4 Running the Jupyter notebook and making predictions 29 Part 1: Loading and examining the data 32 
\f1 \uc0\u9632 
\f0  Part 2: Getting the data into the right shape 36 
\f1 \uc0\u9632 
\f0  Part 3: Creating training, validation, and test datasets 39 
\f1 \uc0\u9632 
\f0  Part 4: Training the model 41 Part 5: Hosting the model 43 
\f1 \uc0\u9632 
\f0  Part 6: Testing the model 44 2.5 Deleting the endpoint and shutting down your notebook instance 46 Deleting the endpoint 46 
\f1 \uc0\u9632 
\f0  Shutting down the notebook instance 47 3 Should you call a customer because they are at risk of churning? 49 3.1 What are you making decisions about? 50 3.2 The process flow 50 3.3 Preparing the dataset 52 Transformation 1: Normalizing the data 53 
\f1 \uc0\u9632 
\f0  Transformation 2: Calculating the change from week to week 54 3.4 XGBoost primer 54 How XGBoost works 54 
\f1 \uc0\u9632 
\f0  How the machine learning model determines whether the function is getting better or getting worse AUC 57 CONTENTS vii 3.5 Getting ready to build the model 58 Uploading a dataset to S3 59 
\f1 \uc0\u9632 
\f0  Setting up a notebook on SageMaker 60 3.6 Building the model 61 Part 1: Loading and examining the data 62 
\f1 \uc0\u9632 
\f0  Part 2: Getting the data into the right shape 65 
\f1 \uc0\u9632 
\f0  Part 3: Creating training, validation, and test datasets 65 
\f1 \uc0\u9632 
\f0  Part 4: Training the model 67 
\f1 \uc0\u9632 
\f0  Part 5: Hosting the model 70 
\f1 \uc0\u9632 
\f0  Part 6: Testing the model 70 3.7 Deleting the endpoint and shutting down your notebook instance 73 Deleting the endpoint 73 
\f1 \uc0\u9632 
\f0  Shutting down the notebook instance 74 3.8 Checking to make sure the endpoint is deleted 74 4 Should an incident be escalated to your support team? 76 4.1 What are you making decisions about? 77 4.2 The process flow 77 4.3 Preparing the dataset 78 4.4 NLP (natural language processing) 79 Creating word vectors 80 
\f1 \uc0\u9632 
\f0  Deciding how many words to include in each group 82 4.5 What is BlazingText and how does it work? 83 4.6 Getting ready to build the model 84 Uploading a dataset to S3 85 
\f1 \uc0\u9632 
\f0  Setting up a notebook on SageMaker 86 4.7 Building the model 86 Part 1: Loading and examining the data 87 
\f1 \uc0\u9632 
\f0  Part 2: Getting the data into the right shape 90 
\f1 \uc0\u9632 
\f0  Part 3: Creating training and validation datasets 93 
\f1 \uc0\u9632 
\f0  Part 4: Training the model 93 Part 5: Hosting the model 95 
\f1 \uc0\u9632 
\f0  Part 6: Testing the model 96 4.8 Deleting the endpoint and shutting down your notebook instance 97 Deleting the endpoint 97 
\f1 \uc0\u9632 
\f0  Shutting down the notebook instance 97 4.9 Checking to make sure the endpoint is deleted 97 viii CONTENTS 5 Should you question an invoice sent by a supplier? 99 5.1 What are you making decisions about? 100 5.2 The process flow 101 5.3 Preparing the dataset 103 5.4 What are anomalies 104 5.5 Supervised vs. unsupervised machine learning 105 5.6 What is Random Cut Forest and how does it work? 106 Sample 1 106 
\f1 \uc0\u9632 
\f0  Sample 2 109 5.7 Getting ready to build the model 114 Uploading a dataset to S3 114 
\f1 \uc0\u9632 
\f0  Setting up a notebook on SageMaker 115 5.8 Building the model 115 Part 1: Loading and examining the data 116 
\f1 \uc0\u9632 
\f0  Part 2: Getting the data into the right shape 120 
\f1 \uc0\u9632 
\f0  Part 3: Creating training and validation datasets 121 
\f1 \uc0\u9632 
\f0  Part 4: Training the model 121 Part 5: Hosting the model 122 
\f1 \uc0\u9632 
\f0  Part 6: Testing the model 123 5.9 Deleting the endpoint and shutting down your notebook instance 126 Deleting the endpoint 126 
\f1 \uc0\u9632 
\f0  Shutting down the notebook instance 126 5.10 Checking to make sure the endpoint is deleted 126 6 Forecasting your company\'92s monthly power usage 128 6.1 What are you making decisions about? 129 Introduction to time-series data 130 
\f1 \uc0\u9632 
\f0  Kiara\'92s time-series data: Daily power consumption 132 6.2 Loading the Jupyter notebook for working with time-series data 133 6.3 Preparing the dataset: Charting time-series data 134 Displaying columns of data with a loop 137 
\f1 \uc0\u9632 
\f0  Creating multiple charts 138 6.4 What is a neural network? 139 6.5 Getting ready to build the model 140 Uploading a dataset to S3 141 
\f1 \uc0\u9632 
\f0  Setting up a notebook on SageMaker 141 CONTENTS ix 6.6 Building the model 141 Part 1: Loading and examining the data 142 
\f1 \uc0\u9632 
\f0  Part 2: Getting the data into the right shape 144 
\f1 \uc0\u9632 
\f0  Part 3: Creating training and testing datasets 147 
\f1 \uc0\u9632 
\f0  Part 4: Training the model 150 Part 5: Hosting the model 152 
\f1 \uc0\u9632 
\f0  Part 6: Making predictions and plotting results 153 6.7 Deleting the endpoint and shutting down your notebook instance 158 Deleting the endpoint 158 
\f1 \uc0\u9632 
\f0  Shutting down the notebook instance 158 6.8 Checking to make sure the endpoint is deleted 159 7 Improving your company\'92s monthly power usage forecast 161 7.1 DeepAR\'92s ability to pick up periodic events 161 7.2 DeepAR\'92s greatest strength: Incorporating related time series 163 7.3 Incorporating additional datasets into Kiara\'92s power consumption model 164 7.4 Getting ready to build the model 165 Downloading the notebook we prepared 165 
\f1 \uc0\u9632 
\f0  Setting up the folder on SageMaker 166 
\f1 \uc0\u9632 
\f0  Uploading the notebook to SageMaker 166 Downloading the datasets from the S3 bucket 166 
\f1 \uc0\u9632 
\f0  Setting up a folder on S3 to hold your data 167 
\f1 \uc0\u9632 
\f0  Uploading the datasets to your AWS bucket 167 7.5 Building the model 168 Part 1: Setting up the notebook 168 
\f1 \uc0\u9632 
\f0  Part 2: Importing the datasets 169 
\f1 \uc0\u9632 
\f0  Part 3: Getting the data into the right shape 170 Part 4: Creating training and test datasets 172 
\f1 \uc0\u9632 
\f0  Part 5: Configuring the model and setting up the server to build the model 175 
\f1 \uc0\u9632 
\f0  Part 6: Making predictions and plotting results 179 7.6 Deleting the endpoint and shutting down your notebook instance 182 Deleting the endpoint 182 
\f1 \uc0\u9632 
\f0  Shutting down the notebook instance 183 7.7 Checking to make sure the endpoint is deleted 183 x CONTENTS PART 3MOVING MACHINE LEARNING INTO PRODUCTION ..................................................185 8 Serving predictions over the web 187 8.1 Why is serving decisions and predictions over the web so difficult? 188 8.2 Overview of steps for this chapter 189 8.3 The SageMaker endpoint 189 8.4 Setting up the SageMaker endpoint 190 Uploading the notebook 191 
\f1 \uc0\u9632 
\f0  Uploading the data 193 Running the notebook and creating the endpoint 194 8.5 Setting up the serverless API endpoint 197 Setting up your AWS credentials on your AWS account 198 Setting up your AWS credentials on your local computer 199 Configuring your credentials 200 8.6 Creating the web endpoint 201 Installing Chalice 202 
\f1 \uc0\u9632 
\f0  Creating a Hello World API 204 Adding the code that serves the SageMaker endpoint 205 Configuring permissions 207 
\f1 \uc0\u9632 
\f0  Updating requirements.txt 207 Deploying Chalice 208 8.7 Serving decisions 208 9 Case studies 211 9.1 Case study 1: WorkPac 212 Designing the project 214 
\f1 \uc0\u9632 
\f0  Stage 1: Preparing and testing the model 215 
\f1 \uc0\u9632 
\f0  Stage 2: Implementing proof of concept (POC) 216 Stage 3: Embedding the process into the company\'92s operations 216 Next steps 217 
\f1 \uc0\u9632 
\f0  Lessons learned 217 9.2 Case study 2: Faethm 217 AI at the core 217 
\f1 \uc0\u9632 
\f0  Using machine learning to improve processes at Faethm 217 
\f1 \uc0\u9632 
\f0  Stage 1: Getting the data 219 
\f1 \uc0\u9632 
\f0  Stage 2: Identifying the features 220 
\f1 \uc0\u9632 
\f0  Stage 3: Validating the results 220 
\f1 \uc0\u9632 
\f0  Stage 4: Implementing in production 220 9.3 Conclusion 220 Perspective 1: Building trust 221 
\f1 \uc0\u9632 
\f0  Perspective 2: Geting the data right 221 
\f1 \uc0\u9632 
\f0  Perspective 3: Designing your operating model to make the most of your machine learning capability 221 
\f1 \uc0\u9632 
\f0  Perspective 4: What does your company look like once you are using machine learning everywhere? 221 CONTENTS xi appendix A Signing up for Amazon AWS 222 appendix B Setting up and using S3 to store files 229 appendix C Setting up and using AWS SageMaker to build a machine learning system 238 appendix D Shutting it all down 243 appendix E Installing Python 247 index 249 xiii preface This book shows you how to apply machine learning in your company to make your business processes faster and more resilient to change. This book is for people beginning their journey in machine learning or for those who are more experienced with machine learning but want to see how it can be applied in practice. Based on our experiences with automating business processes and implementing machine learning applications, we wanted to write a book that would allow anyone to start using machine learning in their company. The caveat to anyone isn\'92t that you need to have a certain technical background, it\'92s that you\'92re willing to put in the time when you run the code to understand what\'92s happening and why. We look at a variety of different functions within various companies ranging across accounts payable (supplier invoices), facilities management (power consumption forecasting), customer support (support tickets), and sales (customer retention). The intent is that this will give you some insight into the range and scale of potential applications of machine learning and encourage you to discover new business applications on your own. A secondary focus of this book is to demonstrate how you can use the Amazon SageMaker cloud service to rapidly and cost effectively bring your business ideas to life. Most of the ideas we present can be implemented using other services (such as Google Cloud or Microsoft Azure); however, the differences are significant enough that to cover multiple providers would be beyond the scope of this book. xiv PREFACE We hope you enjoy our book and that you\'92re able to dramatically improve the productivity of your company by applying the techniques inside. Please hit us up in liveBook if you have questions, comments, suggestions, or examples of how you\'92ve tackled certain problems. See page xxi for access to the liveBook site. We\'92d love to hear from you. xv acknowledgments Writing this book was a lot of work but would have been a lot more without Richie cranking out the notebook code and contributing to chapter ideas. My advice to anyone looking to write a technical book is to find a coauthor and break up the work. Richie and I have different coding styles, and I learned to appreciate his way of tackling certain problems during my documentation of his code. I\'92d like to acknowledge the team at Manning for their help and guidance through the process, and Toni Arritola, in particular, for accommodating the different time zones and having the flexibility to deal with two very busy people in putting this book together. Thank you to everyone at Manning: Deirdre Hiam, our production editor, Frances Buran, our copy editor, Katie Tennant, our proofreader, Arthur Zubarev, our technical development editor, Ivan Martinovic\'b4, our review editor, and Karsten Str\'f8b\'e6k, our technical proofreader. To all of our reviewers\'97Aleksandr Novomlinov, Arpit Khandelwal, Burkhard Nestmann, Clemens Baader, Conor Redmond, Dana Arritola, Dary Merckens, Dhivya Sivasubramanian, Dinesh Ghanta, Gerd Klevesaat, James Black, James Nyika, Jeff Smith, Jobinesh Purushothaman Manakkattil, John Bassil, Jorge Ezequiel Bo, Kevin Kraus, Laurens Meulman, Madhavan Ramani, Mark Poler, Muhammad Sohaib Arif, Nikos Kanakaris, Paulo Nuin, Richard Tobias, Ryan Kramer, Sergio Fernandez Gonzalez, Shawn Eion Smith, Syed Nouman Hasany, and Taylor Delehanty\'97thank you, your suggestions helped make this a better book. And, of course, I\'92d like to thank my spouse and family for their patience and understanding. \'97Doug Hudgeon xvi ACKNOWLEDGMENTS I\'92m very grateful to Doug for asking me to join him as coauthor in writing this book, but also for his creativity, positivity, friendship, and sense of humor. Although it was a lot of work, it was also a pleasure. I\'92d also like to offer my special thanks to my parents, family, and friends for putting up with the long hours and lost weekends. Most of all, I\'92d like to thank my wife, Xenie, who could not have been more supportive and understanding during the years I completed my studies as well as this book. No husband could hope for a better wife, and I can\'92t believe how lucky I am to be spending my days beside her. \'97Richard Nichol xvii about this book Companies are on the cusp of a massive leap in productivity. Today, thousands of people are involved in process work, where they take information from one source and put it into another place. For example, take procurement and accounts payable: 
\f1 \uc0\u9632 
\f0  Procurement staff help a customer create a purchase order, and then send it to a supplier. 
\f1 \uc0\u9632 
\f0  The supplier\'92s order-processing staff then take the purchase order and enter it into the order-processing system, where it\'92s fulfilled and shipped to the customer that placed the order. 
\f1 \uc0\u9632 
\f0  Staff on the customer\'92s loading dock receive the goods, and the finance staff enters the invoice into the customer\'92s finance system. Over the next decade, all of these processes will be completely automated in almost every company, and machine learning will play a big part in automating the decision points at each stage of the process. It will help businesses make the following decisions: 
\f1 \uc0\u9632 
\f0  Does the person approving the order have the authority to do so? 
\f1 \uc0\u9632 
\f0  Is it OK to substitute a product for an out-of-stock item? 
\f1 \uc0\u9632 
\f0  If a supplier has substituted a product, will the receiver accept it? 
\f1 \uc0\u9632 
\f0  Is the invoice OK to pay as is or should it be queried? The real benefit of machine learning for business is that it allows you to build decisionmaking applications that are resilient to change. Instead of programming dozens or hundreds of rules into your systems, you feed in past examples of good and bad xviii ABOUT THIS BOOK decisions, and then let the machine make a determination based on how similar the current scenario is to past examples. The benefit of this is that the system doesn\'92t break when it comes across novel input. The challenge is that it takes a different mindset and approach to deliver a machine learning project than it does to deliver a normal IT project. In a normal IT project, you can test each of the rules to ensure they work. In a machine learning project, you can only test to see whether the algorithm has responded appropriately to the test scenarios. And you don\'92t know how it will react to novel input. Trusting in safeguards that catch it when it\'92s not reacting appropriately requires you and your stakeholders to be comfortable with this uncertainty. Who should read this book This book is targeted at people who may be more comfortable using Excel than using a programming language such as Python. Each chapter contains a fully working Jupyter notebook that creates the machine learning model, deploys it, and runs it against a dataset prepared for the chapter. You don\'92t need to do any coding to see the code in action. Each chapter then takes you through the code step by step so that you understand how it works. With minor modifications, you can apply the code directly to your own data. By the end of the book, you should be able to tackle a wide range of machine learning projects within your company. How this book is organized: A roadmap This book has three parts: Part 1 starts with a description of why businesses need to become a lot more productive to remain competitive, and it explains how effective decision-making plays a role in this. You\'92ll then move on to why machine learning is a good way to make business decisions, and how, using open source tools and tools from AWS, you can start applying machine learning to making decisions in your business. In part 2, you\'92ll then work through six scenarios (one scenario per chapter) that show how machine learning can be used to make decisions in your business. The scenarios focus on how an ordinary company can use machine learning, rather than on how Facebook or Google or Amazon use machine learning. Finally, in part 3, you\'92ll learn how to set up and share your machine learning models on the web so your company can make decisions using machine learning. You\'92ll then go through some case studies that show how companies manage the change that comes along with using machine learning to make decisions. About the code In each chapter in part 2, we provide you a Jupyter notebook and one or more sample datasets that you can upload to AWS SageMaker and run. In part 3, we provide the code to set up a serverless API to serve your predictions to users across the web. ABOUT THIS BOOK xix You run and write the code used in part 2 of the book on AWS SageMaker. You don\'92t need to install anything locally. You can use any type of computer with internet access for this code\'97even a Google Chromebook. To set up the serverless API in part 3 of the book, you need to install Python on a laptop running macOS, Windows, or Linux operating systems. This book contains many examples of source code both in numbered listings and in line with normal text. In both cases, source code is formatted in a fixed-width font like this to separate it from ordinary text. In many cases, the original source code has been reformatted; we\'92ve added line breaks and reworked indentation to accommodate the available page width in the book. Code annotations (comments) accompany many of the listings, highlighting important concepts. Additionally, comments in the source code have often been removed from the listings when the code is described in the text. The code for the examples in this book is available for download from the Manning website at https://www.manning.com/books/machine-learning-for-business?query =hudgeon and from GitHub at https://git.manning.com/agileauthor/hudgeon/tree/ master/manuscript. liveBook discussion forum The purchase of Machine Learning for Business includes free access to a private web forum run by Manning Publications, where you can make comments about the book, ask technical questions, and receive help from the authors and from other users. To access the forum, go to https://livebook.manning.com/book/machine-learning-forbusiness/welcome/v-6/. You can also learn more about Manning\'92s forums and the rules of conduct at https://livebook.manning.com/#!/discussion. Manning\'92s commitment to our readers is to provide a venue where a meaningful dialog between individual readers and between readers and authors can take place. It is not a commitment to any specific amount of participation on the part of the authors, whose contribution to the forum remains voluntary (and unpaid). We suggest you try asking them some challenging questions lest their interest stray! The forum and the archives of previous discussions will be accessible from the publisher\'92s website as long as the book is in print. xx about the authors Richard (Richie) and Doug worked together at a procurement software company. Doug became CEO not long after Richie was hired as a data engineer to help the company categorize millions of products. After leaving the company, Doug built Managed Functions (https://managedfunctions .com), an integration/machine learning platform that uses Python and Jupyter notebooks to automate business processes. Richie went on to complete a Master of Data Science at the University of Sydney, Australia, and is now working as Senior Data Scientist for Faethm (http://www.faethm.ai). xxi about the cover illustration The figure on the cover of Machine Learning for Business is captioned \'93Costumes civils actuels de tous les peuples connus,\'94 meaning \'93current civilian costumes of all known peoples.\'94 The illustration is taken from a collection of dress costumes from various countries by Jacques Grasset de Saint-Sauveur (1757-1810), titled Costumes de Diff\'e9rents Pays, published in France in 1797. Each illustration is finely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur\'92s collection reminds us vividly of how culturally apart the world\'92s towns and regions were just 200 years ago. Isolated from each other, people spoke different dialects and languages. In the streets or in the countryside, it was easy to identify where they lived and what their trade or station in life was just by their dress. The way we dress has changed since then, and the diversity by region, so rich at the time, has faded away. It is now hard to tell apart the inhabitants of different continents, let alone different towns, regions, or countries. Perhaps we have traded cultural diversity for a more varied personal life\'97certainly for a more varied and fast-paced technological life. At a time when it is hard to tell one computer book from another, Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by Grasset de Saint-Sauveur\'92s pictures. Part 1 Machine learning for business The coming decade will see a massive surge in business productivity as companies automate tasks that are important but time consuming for people to do. Examples of such tasks include approving purchase orders, evaluating which customers are at risk of churning, identifying support requests that should be escalated immediately, auditing invoices from suppliers, and forecasting operational trends, such as power consumption. Part one looks at why this trend is occurring and the role of machine learning in creating the surge. Companies that are not able to accelerate to catch this surge will quickly find themselves outdistanced by their competitors. 3 How machine learning applies to your business Technologists have been predicting for decades that companies are on the cusp of a surge in productivity, but so far, this has not happened. Most companies still use people to perform repetitive tasks in accounts payable, billing, payroll, claims management, customer support, facilities management, and more. For example, all of the following small decisions create delays that make you (and your colleagues) less responsive than you want to be and less effective than your company needs you to be: 
\f2 \uc0\u61601 
\f0  To submit a leave request, you have to click through a dozen steps, each one requiring you to enter information that the system should already This chapter covers 
\f2 \uc0\u61601 
\f0  Why our business systems are so terrible 
\f2 \uc0\u61601 
\f0  What machine learning is 
\f2 \uc0\u61601 
\f0  Machine learning as a key to productivity 
\f2 \uc0\u61601 
\f0  Fitting machine learning with business automation 
\f2 \uc0\u61601 
\f0  Setting up machine learning within your company 4 CHAPTER 1 How machine learning applies to your business know or to make a decision that the system should be able to figure out from your objective. 
\f2 \uc0\u61601 
\f0  To determine why your budget took a hit this month, you have to scroll through a hundred rows in a spreadsheet that you\'92ve manually extracted from your finance system. Your systems should be able to determine which rows are anomalous and present them to you. 
\f2 \uc0\u61601 
\f0  When you submit a purchase order for a new chair, you know that Bob in procurement has to manually make a bunch of small decisions to process the form, such as whether your order needs to be sent to HR for ergonomics approval or whether it can be sent straight to the financial approver. We believe that you will soon have much better systems at work\'97machine learning applications will automate all of the small decisions that currently hold up processes. It is an important topic because, over the coming decade, companies that are able to become more automated and more productive will overtake those that cannot. And machine learning will be one of the key enablers of this transition. This book shows you how to implement machine learning, decision-making systems in your company to speed up your business processes. \'93But how can I do that?\'94 you say. \'93I\'92m technically minded and I\'92m pretty comfortable using Excel, and I\'92ve never done any programming.\'94 Fortunately for you, we are at a point in time where any technically minded person can learn how to help their company become dramatically more productive. This book takes you on that journey. On that journey, you\'92ll learn 
\f2 \uc0\u61601 
\f0  How to identify where machine learning will create the greatest benefits within your company in areas such as \'96 Back-office financials (accounts payable and billing) \'96 Customer support and retention \'96 Sales and marketing \'96 Payroll and human resources 
\f2 \uc0\u61601 
\f0  How to build machine learning applications that you can implement in your company 1.1 Why are our business systems so terrible? \'93The man who goes alone can start today; but he who travels with another must wait till that other is ready.\'94 Henry David Thoreau Before we get into how machine learning can make your company more productive, let\'92s look at why implementing systems in your company is more difficult than adopting systems in your personal life. Take your personal finances as an example. You might use a money management app to track your spending. The app tells you how much you spend and what you spend it on, and it makes recommendations on how you Why are our business systems so terrible? 5 could increase your savings. It even automatically rounds up purchases to the nearest dollar and puts the spare change into your savings account. At work, expense management is a very different experience. To see how your team is tracking against their budget, you send a request to the finance team, and they get back to you the following week. If you want to drill down into particular line items in your budget, you\'92re out of luck. There are two reasons why our business systems are so terrible. First, although changing our own behavior is not easy, changing the behavior of a group of people is really hard. In your personal life, if you want to use a new money management app, you just start using it. It\'92s a bit painful because you need to learn how the new app works and get your profile configured, but still, it can be done without too much effort. However, when your company wants to start using an expense management system, everyone in the company needs to make the shift to the new way of doing things. This is a much bigger challenge. Second, managing multiple business systems is really hard. In your personal life, you might use a few dozen systems, such as a banking system, email, calendar, maps, and others. Your company, however, uses hundreds or even thousands of systems. Although managing the interactions between all these systems is hard for your IT department, they encourage you to use their end-to-end enterprise software system for as many tasks as possible. The end-to-end enterprise software systems from software companies like SAP and Oracle are designed to run your entire company. These end-to-end systems handle your inventory, pay staff, manage the finance department, and handle most other aspects of your business. The advantage of an end-to-end system is that everything is integrated. When you buy something from your company\'92s IT catalog, the catalog uses your employee record to identify you. This is the same employee record that HR uses to store your leave request and send you paychecks. The problem with end-to-end systems is that, because they do everything, there are better systems available for each thing that they do. Those systems are called best-of-breed systems. Best-of-breed systems do one task particularly well. For example, your company might use an expense management system that rivals your personal money management application for ease of use. The problem is that this expense management system doesn\'92t fit neatly with the other systems your company uses. Some functions duplicate existing functions in other systems (figure 1.1). For example, the expense management system has a built-in approval process. This approval process duplicates the approval process you use in other aspects of your work, such as approving employee leave. When your company implements the best-of-breed expense management system, it has to make a choice: does it use the expense management approval workflow and train you to use two different approval processes? Or does it integrate the expense management system with the end-to-end system so you can approve expenses in the end-to-end system and then pass the approval back into the expense management system? To get a feel for the pros and cons of going with an end-to-end versus a best-ofbreed system, imagine you\'92re a driver in a car rally that starts on paved roads, then 6 CHAPTER 1 How machine learning applies to your business goes through desert, and finally goes through mud. You have to choose between putting all-terrain tires on your car or changing your tires when you move from pavement to sand and from sand to mud. If you choose to change your tires, you can go faster through each of the sections, but you lose time when you stop and change the tires with each change of terrain. Which would you choose? If you could change tires quickly, and it helped you go much faster through each section, you\'92d change tires with each change of terrain. Now imagine that, instead of being the driver, your job is to support the drivers by providing them with tires during the race. You\'92re the Chief Tire Officer (CTO). And imagine that instead of three different types of terrain, you have hundreds, and instead of a few drivers in the race, you have thousands. As CTO, the decision is easy: you\'92ll choose the all-terrain tires for all but the most specialized terrains, where you\'92ll reluctantly concede that you need to provide specialty tires. As a driver, the CTO\'92s decision sometimes leaves you dissatisfied because you end up with a system that is clunkier than the systems you use in your personal life. We believe that over the coming decade, machine learning will solve these types of problems. Going back to our metaphor about the race, a machine learning application would automatically change the characteristics of your tires as you travel through different terrains. It would give you the best of both worlds by rivaling best-of-breed performance while utilizing the functionality in your company\'92s end-to-end solution. As another example, instead of implementing a best-of-breed expense management system, your company could implement a machine learning application to 
\f2 \uc0\u61601 
\f0  Identify information about the expense, such as the amount spent and the vendor name 
\f2 \uc0\u61601 
\f0  Decide which employee the expense belongs to 
\f2 \uc0\u61601 
\f0  Decide which approver to submit the expense claim to End-to-end system Best-of-breed system Overlapping functionality (approvals, for example) Figure 1.1 Best-of-breed approval function overlaps the end-to-end system approval function. Why are our business systems so terrible? 7 Returning to the example of overlapping approval functions, by using machine learning in conjunction with your end-to-end systems, you can automate and improve your company\'92s processes without implementing a patchwork of best-of-breed systems (figure 1.2). Is there no role for best-of-breed systems in the enterprise? There is a role for best-of-breed systems in the enterprise, but it is probably different than the role these systems have filled over the past 20 years or so. As you\'92ll see in the next section, the computer era (1970 to the present) has been unsuccessful in improving the productivity of businesses. If best-of-breed systems were successful at improving business productivity, we should have seen some impact on the performance of businesses that use best-of-breed systems. But we haven\'92t. So what will happen to the best-of-breed systems? In our view, the best-of-breed systems will become 
\f2 \uc0\u61601 
\f0  More integrated into a company\'92s end-to-end system 
\f2 \uc0\u61601 
\f0  More modular so that a company can adopt some of the functions, but not others Vendors of these best-of-breed systems will base their business cases on the use of problem-specific machine learning applications to differentiate their offerings from those of their competitors or on solutions built in-house by their customers. Conversely, their profit margins will get squeezed as more companies develop the skills to build machine learning applications themselves rather than buying a best-of-breed solution. End-to-end system Machine learning application Ordering functionality built into end-to-end system by incorporating machine learning to automate decisions Figure 1.2 Machine learning enhances the functionality of end-to-end systems. 8 CHAPTER 1 How machine learning applies to your business 1.2 Why is automation important now? We are on the cusp of a dramatic improvement in business productivity. Since 1970, business productivity in mature economies such as the US and Europe has barely moved, compared to the change in the processing power of computers, and this trend has been clearly visible for decades now. Over that period of time, business productivity has merely doubled, whereas the processing power of computers is 20 million times greater! If computers were really helping us become more productive, why is it that much faster computers don\'92t lead to much greater productivity? This is one of mysteries of modern economics. Economists call this mystery the Solow Paradox. In 1987, Robert Solow, an American economist, quipped: \'93You can see the computer age everywhere but in the productivity statistics.\'94 Is the failure of businesses to become more productive just a feature of business? Are businesses at maximum productivity now? We don\'92t think so. Some companies have found a solution to the Solow Paradox and are rapidly improving their productivity. And we think that they will be joined by many others\'97hopefully, yours as well. Figure 1.3 is from a 2017 speech on productivity given by Andy Haldane, Chief Economist for the Bank of England.1 It shows that since 2002, the top 5% of companies 1 Andy Haldane, \'93Productivity Puzzles,\'94 https://www.bis.org/review/r170322b.pdf. 50% 40% 30% 20% 10% 2001 2002 2003 2004 2005 2007 2008 2009 2010 2011 2012 2013 Top 5% of companies (Frontier Firms) All companies Figure 1.3 Comparison of productivity across frontier firms (the top 5%) versus all companies Why is automation important now? 9 have increased productivity by 40%, while the other 95% of companies have barely increased productivity at all.2 This low-growth trend is found across nearly all countries with mature economies. 1.2.1 What is productivity? Productivity is measured at a country level by dividing the annual Gross Domestic Product (GDP) by the number of hours worked in a year. The GDP per hour worked in the UK and the US is currently just over US$100. In 1970, it was between US$45 and US$50. But the GDP per hour worked by the top 5% of firms (the frontier firms) is over US$700 and rising. The frontier firms were able to hit such a high GDP per hour by minimizing human effort to generate each dollar of revenue. Or, to put it another way, these firms automate everything that can be automated. We predict that productivity growth will improve rapidly as more companies figure out how to replicate what the top companies are doing and will make the jump from their current level of productivity to the top levels of productivity. We believe that we\'92re at the end of the Solow Paradox; that machine learning will enable many companies to hit the productivity levels we see in the top 5% of companies. And we believe that those companies that do not join them, that don\'92t dramatically improve their productivity, will wither and die. 1.2.2 How will machine learning improve productivity? In the preceding sections, we looked at why companies struggle to become more automated and the evidence showing that, while company productivity has not improved much over the past 50 years, there is a group of frontier firms becoming more productive by automating everything that can be automated. Next we\'92ll look at how machine learning can help your company become a frontier firm before showing you how you can help your company make the shift. For our purposes, automation is the use of software to perform a repetitive task. In the business world, repetitive tasks are everywhere. A typical retail business, for example, places orders with suppliers, sends marketing material to customers, manages products in inventory, creates entries in their accounting system, makes payments to their staff, and hundreds of other things. Why is it so hard to automate these processes? From a higher level, these processes look pretty simple. Sending marketing material is just preparing content and emailing it to customers. Placing orders is simply selecting product from a catalog, getting it approved, and sending the order to a supplier. How hard can it be? The reason automation is hard to implement is because, even though these processes look repetitive, there are small decisions that need to be made at several steps along the way. This is where machine learning fits in. You can use machine learning to 2 Andy Haldane dubbed the top 5% of companies frontier firms. 10 CHAPTER 1 How machine learning applies to your business make these decisions at each point in the process in much the same way a human currently does. 1.3 How do machines make decisions? For the purposes of this book, think of machine learning as a way to arrive at a decision, based on patterns in a dataset. We\'92ll call this pattern-based decision making. This is in contrast to most software development these days, which is rules-based decision making--where programmers write code that employs a series of rules to perform a task. When your marketing staff sends out an email newsletter, the marketing software contains code that queries a database and pulls out only those customers selected by the query (for example, males younger than 25 who live within 20 kilometers of a certain clothing outlet store). Each person in the marketing database can be identified as being in this group or not in this group. Contrast this with machine learning where the query for your database might be to pull out all users who have a purchasing history similar to that of a specific 23-year-old male who happens to live close to one of your outlet stores. This query will get a lot of the same people that the rules-based query gets, but it will also return those who have a similar purchasing pattern and are willing to drive further to get to your store. 1.3.1 People: Rules-based or not? Many businesses rely on people rather than software to perform routine tasks like sending marketing material and placing orders with suppliers. They do so for a number of reasons, but the most prevalent is that it\'92s easier to teach a person how to do a task than it is to program a computer with the rules required to perform the same task. Let\'92s take Karen, for example. Her job is to review purchase orders, send them to an approver, and then email the approved purchase orders to the supplier. Karen\'92s job is both boring and tricky. Every day, Karen makes dozens of decisions about who should approve which orders. Karen has been doing this job for several years, so she knows the simple rules, like IT products must be approved by the IT department. But she also knows the exceptions. For example, she knows that when Jim orders toner from the stationery catalog, she needs to send the order to IT for approval, but when Jim orders a new mouse from the IT catalog, she does not. The reason Karen\'92s role hasn\'92t been automated is because programming all of these rules is hard. But harder still is maintaining these rules. Karen doesn\'92t often apply her \'93fax machine\'94 rule anymore, but she is increasingly applying her \'93tablet stylus\'94 rule, which she has developed over the past several years. She considers a tablet stylus to be more like a mouse than a laptop computer, so she doesn\'92t send stylus orders to IT for approval. If Karen really doesn\'92t know how to classify a particular product, she\'92ll call IT to discuss it; but for most things, she makes up her own mind. Using our concepts of rules-based decision making versus pattern-based decision making, you can see that Karen incorporates a bit of both. Karen applies rules most of How do machines make decisions? 11 the time but occasionally makes decisions based on patterns. It\'92s the pattern-based part of Karen\'92s work that makes it hard to automate using a rules-based system. That\'92s why, in the past, it has been easier to have Karen perform these tasks than to program a computer with the rules to perform the same tasks. 1.3.2 Can you trust a pattern-based answer? Lots of companies have manual processes. Often this is the case because there\'92s enough variation in the process to make automation difficult. This is where machine learning comes in. Any point in a process where a person needs to make a decision is an opportunity to use machine learning to automate the decision or to present a restricted choice of options for the person to consider. Unlike rules-based programming, machine learning uses examples rather than rules to determine how to respond in a given situation. This allows it to be more flexible than rules-based systems. Instead of breaking when faced with a novel situation, machine learning simply makes a decision with a lower level of confidence. Let\'92s look at the example of a new product coming into Karen\'92s catalog. The product is a voice-controlled device like Amazon Echo or Google Home. The device looks somewhat like an IT product, which means the purchase requires IT approval. But, because it\'92s also a way to get information into a computer, it kind of looks like an accessory such as a stylus or a mouse, which means the purchase doesn\'92t require IT approval. In a rules-based system, this product would be unknown, and when asked to determine which approver to send the product to, the system could break. In a machine learning system, a new product won\'92t break the system. Instead, the system provides an answer with a lower level of confidence than it does for products it has seen before. And just like Karen could get it wrong, the machine learning application could get it wrong too. Accepting this level of uncertainty might be challenging for your company\'92s management and risk teams, but it\'92s no different than having Karen make those same decisions when a new product comes across her desk. In fact, a machine learning system for business automation workflow can be designed to perform better than a human acting on their own. The optimal workflow often involves both systems and people. The system can be configured to cater to the vast majority of cases but have a mechanism where, when it has a low confidence level, it passes the case to a human operator for a decision. Ideally, this decision is fed back into the machine learning application so that, in the future, the application has a higher level of confidence in its decision. It\'92s all well and good for you to say you\'92re comfortable with the result. In many instances, in order to make pattern-based decisions in your company, you\'92ll need the approval of your risk and management teams. In a subsequent section, once we take a look at the output of a pattern-based decision, you\'92ll see some potential ways of getting this approval. 12 CHAPTER 1 How machine learning applies to your business 1.3.3 How can machine learning improve your business systems? So far in this chapter, we have been referring to the system that can perform multiple functions in your company as an end-to-end system. Commonly, these systems are referred to as ERP (Enterprise Resource Planning) systems. ERP systems rose to prominence in the 1980s and 1990s. An ERP system is used by many medium and large enterprises to manage most of their business functions like payroll, purchasing, inventory management, capital depreciation, and others. SAP and Oracle dominate the ERP market, but there are several smaller players as well. In a perfect world, all of your business processes would be incorporated into your ERP system. But we don\'92t live in a perfect world. Your company likely does things slightly differently than your ERP\'92s default configuration, which creates a problem. You have to get someone to program your ERP to work the way your business does. This is expensive and time consuming, and can make your company less able to adjust to new opportunities as they arise. And, if ERP systems were the answer to all enterprise problems, then we should have seen productivity improvements during the uptake of ERP systems in the 1980s and 1990s. But there was little uptake in productivity during this period. When you implement machine learning to support Karen\'92s decisions, there\'92s little change in the management process involved for your internal customers. They continue to place orders in the same ways they always have. The machine learning algorithms simply make some of the decisions automatically, and the orders get sent to approvers and suppliers appropriately and automatically. In our view, unless the process can be cleanly separated from the other processes in your company, the optimal approach is to first implement a machine learning automation solution and then, over time, migrate these processes to your ERP systems. TIP Automation is not the only way to become more productive. Before automating, you should ask whether you need to do the process at all. Can you create the required business value without automating? 1.4 Can a machine help Karen make decisions? Machine learning concepts are difficult to get one\'92s head around. This is, in part, due to the breadth of topics encompassed by the term machine learning. For the purposes of this book, think of machine learning as a tool that identifies patterns in data and, when you provide it with new data, it tells you which pattern the new data most closely fits. As you read through other resources on machine learning, you will see that machine learning can cover many other things. But most of these things can be broken down into a series of decisions. Take machine learning systems for autonomous cars, for example. On the face of it, this sounds very different from the machine learning we are looking at. But it is really just a series of decisions. One machine learning algorithm looks at a scene and decides how to draw boxes around each of the objects in the scene. Another machine learning algorithm decides whether these boxes are Can a machine help Karen make decisions? 13 things that need to be driven around. And, if so, a third algorithm decides the best way to drive around them. To determine whether you can use machine learning to help out Karen, let\'92s look at the decisions made in Karen\'92s process. When an order comes in, Karen needs to decide whether to send it straight to the requester\'92s financial approver or whether she should send it to a technical approver first. She needs to send an order to a technical approver if the order is for a technical product like a computer or a laptop. She does not need to send it to a technical approver if it is not a technical product. And she does not need to send the order for technical approval if the requester is from the IT department. Let\'92s assess whether Karen\'92s example is suitable for machine learning. In Karen\'92s case, the question she asks for every order is, \'93Should I send this for technical approval?\'94 Her decision will either be yes or no. The things she needs to consider when making her decision are 
\f2 \uc0\u61601 
\f0  Is the product a technical product? 
\f2 \uc0\u61601 
\f0  Is the requester from the IT department? In machine learning lingo, Karen\'92s decision is called the target variable, and the types of things she considers when making the decision are called features. When you have a target variable and features, you can use machine learning to make a decision. 1.4.1 Target variables Target variables come in two flavors: 
\f2 \uc0\u61601 
\f0  Categorical 
\f2 \uc0\u61601 
\f0  Continuous Categorical variables include things like yes or no; and north, south, east, or west. An important distinction in our machine learning work in this book is whether the categorical variable has only two categories or has more than two categories. If it has only two categories, it is called a binary target variable. If it has more than two categories, it is called a multiclass target variable. You will set different parameters in your machine learning applications, depending on whether the variable is binary or multiclass. This will be covered in more detail later in the book. Continuous variables are numbers. For example, if your machine learning application predicts house prices based on features such as neighborhood, number of rooms, distance from schools, and so on, your target variable (the predicted price of the house) is a continuous variable. The price of a house could be any value from tens of thousands of dollars to tens of millions of dollars. 1.4.2 Features In this book, features are perhaps the most important machine learning concept to understand. We use features all the time in our own decision making. In fact, the things you\'92ll learn in this book about features can help you better understand your own decision-making process. 14 CHAPTER 1 How machine learning applies to your business As an example, let\'92s return to Karen as she makes a decision about whether to send a purchase order to IT for approval. The things that Karen considers when making this decision are its features. One thing Karen can consider when she comes across a product she hasn\'92t seen before is who manufactured the product. If a product is from a manufacturer that only produces IT products, then, even though she has never seen that product before, she considers it likely to be an IT product. Other types of features might be harder for a human to consider but are easier for a machine learning application to incorporate into its decision making. For example, you might want to find out which customers are likely to be more receptive to receiving a sales call from your sales team. One feature that can be important for your repeat customers is whether the sales call would fit in with their regular buying schedule. For example, if the customer normally makes a purchase every two months, is it approximately two months since their last purchase? Using machine learning to assist your decision making allows these kinds of patterns to be incorporated into the decision to call or not call; whereas, it would be difficult for a human to identify such patterns. Note that there can be several levels to the things (features) Karen considers when making her decision. For example, if she doesn\'92t know whether a product is a technical product or not, then she might consider other information such as who the manufacturer is and what other products are included on the requisition. One of the great things about machine learning is that you don\'92t need to know all the features; you\'92ll see which features are the most important as you put together the machine learning system. If you think it might be relevant, include it in your dataset. 1.5 How does a machine learn? A machine learns the same way you do. It is trained. But how? Machine learning is a process of rewarding a mathematical function for getting answers right and punishing the function for getting answers wrong. But what does it mean to reward or punish a function? You can think of a function as a set of directions on how to get from one place to another. In figure 1.4, to get from point A to point B, the directions might read thus: 1 Go right. 2 Go a bit up. 3 Go a bit down. 4 Go down sharply. 5 Go up! 6 Go right. A machine learning application is a tool that can determine when the function gets it right (and tells the function to do more of that) or gets it wrong (and tells the function to do less of that). The function knows it got it right because it becomes more successful at predicting the target variable based on the features. How does a machine learn? 15 Let\'92s pull a dataset out of figure 1.4 to look at a bigger sample in figure 1.5. You can see that the dataset comprises two types of circles: dark circles and light circles. In figure 1.5, there is a pattern that we can see in the data. There are lots of light circles at the edges of the dataset and lots of dark circles near the middle. This means that our function, which provides the directions on how to separate the dark circles from light circles, will start at the left of the diagram and do a big loop around the dark circles before returning to its starting point. When we are training the process to reward the function for getting it right, we could think of this as a process that rewards a function for having a dark circle on the right and punishes it for having a dark circle on the left. You could train it even faster if you also reward the function for having a light circle on the left and punish it for having a light circle on the right. So, with this as a background, when you\'92re training a machine learning application, what you\'92re doing is showing a bunch of examples to a system that builds a mathematical function to separate certain things in the data. The thing it is separating in the data is the target variable. When the function separates more of the target variables, it gets a reward, and when it separates fewer target variables, it gets punished. Machine learning problems can be broken down into two types: 
\f2 \uc0\u61601 
\f0  Supervised machine learning 
\f2 \uc0\u61601 
\f0  Unsupervised machine learning A. B. Function rewarded for keeping dark circles on the bottom Figure 1.4 Machine learning function to identify a pattern in the data 16 CHAPTER 1 How machine learning applies to your business In addition to features, the other important concept in machine learning as far as this book is concerned is the distinction between supervised and unsupervised machine learning. Like its name suggests, unsupervised machine learning is where we point a machine learning application at a bunch of data and tell it to do its thing. Clustering is an example of unsupervised machine learning. We provide the machine learning application with some customer data, for example, and it determines how to group that customer data into clusters of similar customers. In contrast, classification is an example of supervised machine learning. For example, you could use your sales team\'92s historical success rate for calling customers as a way of training a machine learning application how to recognize customers who are most likely to be receptive to receiving a sales call. A. Function rewarded for keeping dark circles on the right and in the middle Figure 1.5 Machine learning functions to identify a group of similar items in a dataset Getting approval in your company to use machine learning to make decisions 17 NOTE In most of the chapters in this book, you\'92ll focus on supervised machine learning where, instead of letting the machine learning application pick out the patterns, you provide the application with a historical dataset containing samples that show the right decision. One of the big advantages of tackling business automation projects using machine learning is that you can usually get your hands on a good dataset fairly easy. In Karen\'92s case, she has thousands of previous orders to draw from, and for each order, she knows whether it was sent to a technical approver or not. In machine learning lingo, you say that the dataset is labeled, which means that each sample shows what the target variable should be for that sample. In Karen\'92s case, the historical dataset she needs is a dataset that shows what product was purchased, whether it was purchased by someone from the IT department or not, and whether Karen sent it to a technical approver or not. 1.6 Getting approval in your company to use machine learning to make decisions Earlier in the chapter, we described how you could learn enough about decision making using machine learning to help your company. But what does your company need in order to take full advantage of your good work? In theory, it\'92s not that hard. Your company just needs four things: 
\f2 \uc0\u61601 
\f0  It needs a person who can identify opportunities to automate and use machine learning, and someone who can put together a proof of concept that shows the opportunity is worth pursuing. That\'92s you, by the way. 
\f2 \uc0\u61601 
\f0  You need to be able to access the data required to feed your machine learning applications. Your company will likely require you to complete a number of internal forms describing why you want access to that data. 
\f2 \uc0\u61601 
\f0  Your risk and management teams need to be comfortable with using patternbased approaches to making decisions. 
\f2 \uc0\u61601 
\f0  Your company needs a way to turn your work into an operational system. In many organizations, the third of these four points is the most difficult. One way to tackle this is to involve your risk team in the process and provide them with the ability to set a threshold on when a decision needs to be reviewed by Karen. For example, some orders that cross Karen\'92s desk very clearly need to be sent to a technical approver, and the machine learning application must be 100% confident that it should go to a technical approver. Other orders are less clear cut, and instead of returning a 1 (100% confidence), the application might return a 0.72 (a lower level of confidence). You could implement a rule that if the application has less than 75% confidence that the decision is correct, then route the request to Karen for a decision. If your risk team is involved in setting the confidence level whereby orders must be reviewed by a human, this provides them with a way to establish clear guidelines for 18 CHAPTER 1 How machine learning applies to your business which pattern-based decisions can be managed in your company. In chapter 2, you\'92ll read more about Karen and will help her with her work. 1.7 The tools In the old days (a.k.a. 2017), setting up a scalable machine learning system was very challenging. In addition to identifying features and creating a labeled dataset, you needed to have a wide range of skills, encompassing those of an IT infrastructure administrator, a data scientist, and a back-end web developer. Here are the steps that used to be involved in setting up your machine learning system. (In this book, you\'92ll see how to set up your machine learning systems without doing all these steps.) 1 Set up your development environment to build and run a machine learning application (IT infrastructure administrator) 2 Train the machine learning application on your data (data scientist) 3 Validate the machine learning application (data scientist) 4 Host the machine learning application (IT infrastructure administrator) 5 Set up an endpoint that takes your new data and returns a prediction (back-end web developer) It\'92s little wonder that machine learning is not yet in common use in most companies! Fortunately, nowadays some of these steps can be carried out using cloud-based servers. So although you need to understand how it all fits together, you don\'92t need to know how to set up a development environment, build a server, or create secure endpoints. In each of the following seven chapters, you\'92ll set up (from scratch) a machine learning system that solves a common business problem. This might sound daunting, but it\'92s not because you\'92ll use a service from Amazon called AWS SageMaker. 1.7.1 What are AWS and SageMaker, and how can they help you? AWS is Amazon\'92s cloud service. It lets companies of all sizes set up servers and interact with services in the cloud rather than building their own data centers. AWS has dozens of services available to you. These range from compute services such as cloud-based servers (EC2), to messaging and integration services such as SNS (Simple Notification Service) messaging, to domain-specific machine learning services such as Amazon Transcribe (for converting voice to text) and AWS DeepLens (for machine learning from video feeds). SageMaker is Amazon\'92s environment for building and deploying machine learning applications. Let\'92s look at the functionality it provides using the same five steps discussed earlier (section 1.7). SageMaker is revolutionary because it 
\f2 \uc0\u61601 
\f0  Serves as your development environment in the cloud so you don\'92t have to set up a development environment on your computer 
\f2 \uc0\u61601 
\f0  Uses a preconfigured machine learning application on your data 
\f2 \uc0\u61601 
\f0  Uses inbuilt tools to validate the results from your machine learning application Setting up SageMaker in preparation for tackling the scenarios in chapters 2 through 7 19 
\f2 \uc0\u61601 
\f0  Hosts your machine learning application 
\f2 \uc0\u61601 
\f0  Automatically sets up an endpoint that takes in new data and returns predictions One of the best aspects of SageMaker, aside from the fact that it handles all of the infrastructure for you, is that the development environment it uses is a tool called the Jupyter Notebook, which uses Python as one of its programming languages. But the things you\'92ll learn in this book working with SageMaker will serve you well in whatever machine learning environment you work in. Jupyter notebooks are the de facto standard for data scientists when interacting with machine learning applications, and Python is the fastest growing programming language for data scientists. Amazon\'92s decision to use Jupyter notebooks and Python to interact with machine learning applications benefits both experienced practitioners as well as people new to data science and machine learning. It\'92s good for experienced machine learning practitioners because it enables them to be immediately productive in SageMaker, and it\'92s good for new practitioners because the skills you learn using SageMaker are applicable everywhere in the fields of machine learning and data science. 1.7.2 What is a Jupyter notebook? Jupyter notebooks are one of the most popular tools for data science. These combine text, code, and charts in a single document that allows a user to consistently repeat data analysis, from loading and preparing the data to analyzing and displaying the results. The Jupyter Project started in 2014. In 2017, the Jupyter Project steering committee members were awarded the prestigious ACM Software System award \'93for developing a software system that has had a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both.\'94 This award is a big deal because previous awards were for things like the internet. In our view, Jupyter notebooks will become nearly as ubiquitous as Excel for business analysis. In fact, one of the main reasons we selected SageMaker as our tool of choice for this book is because when you\'92re learning SageMaker, you\'92re learning Jupyter. 1.8 Setting up SageMaker in preparation for tackling the scenarios in chapters 2 through 7 The workflow that you\'92ll follow in each chapter is as follows: 1 Download the prepared Jupyter notebook and dataset from the links listed in the chapter. Each chapter has one Jupyter notebook and one or more datasets. 2 Upload the dataset to S3, your AWS file storage bucket. 3 Upload the Jupyter notebook to SageMaker. At this point, you can run the entire notebook, and your machine learning model will be built. The remainder of each chapter takes you through each cell in the notebook and explains how it works. 20 CHAPTER 1 How machine learning applies to your business If you already have an AWS account, you are ready to go. Setting up SageMaker for each chapter should only take a few minutes. Appendixes B and C show you how to do the setup for chapter 2. If you don\'92t have an AWS account, start with appendix A and progress through to appendix C. These appendixes will step you through signing up for AWS, setting up and uploading your data to the S3 bucket, and creating your notebook in SageMaker. The topics are as follows: 
\f2 \uc0\u61601 
\f0  Appendix A: How to sign up for AWS 
\f2 \uc0\u61601 
\f0  Appendix B: How to set up S3 to store files 
\f2 \uc0\u61601 
\f0  Appendix C: How to set up and run SageMaker After working your way through these appendixes (to the end of appendix C), you\'92ll have your dataset stored in S3 and a Jupyter notebook set up and running on SageMaker. Now you\'92re ready to tackle the scenarios in chapter 2 and beyond. 1.9 The time to act is now You saw earlier in this chapter that there is a group of frontier firms that are rapidly increasing their productivity. Right now these firms are few and far between, and your company might not be competing with any of them. However, it\'92s inevitable that other firms will learn to use techniques like machine learning for business automation to dramatically improve their productivity, and it\'92s inevitable that your company will eventually compete with them. We believe it is a case of eat or be eaten. The next section of the book consists of six chapters that take you through six scenarios that will equip you for tackling many of the scenarios you might face in your own company, including the following: 
\f2 \uc0\u61601 
\f0  Should you send a purchase order to a technical approver? 
\f2 \uc0\u61601 
\f0  Should you call a customer because they are at risk of churning? 
\f2 \uc0\u61601 
\f0  Should a customer support ticket be handled by a senior support person? 
\f2 \uc0\u61601 
\f0  Should you query an invoice sent to you by a supplier? 
\f2 \uc0\u61601 
\f0  How much power will your company use next month based on historical trends? 
\f2 \uc0\u61601 
\f0  Should you add additional data such as planned holidays and weather forecasts to your power consumption prediction to improve your company\'92s monthly power usage forecast? After working your way through these chapters, you should be equipped to tackle many of the machine learning decision-making scenarios you\'92ll face in your work and in your company. This book takes you on the journey from being a technically minded non-developer to someone who can set up a machine learning application within your own company. Summary 21 Summary 
\f2 \uc0\u61601 
\f0  Companies that don\'92t become more productive will be left behind by those that do. 
\f2 \uc0\u61601 
\f0  Machine learning is the key to your company becoming more productive because it automates all of the little decisions that hold your company back. 
\f2 \uc0\u61601 
\f0  Machine learning is simply a way of creating a mathematical function that best fits previous decisions and that can be used to guide current decisions. 
\f2 \uc0\u61601 
\f0  Amazon SageMaker is a service that lets you set up a machine learning application that you can use in your business. 
\f2 \uc0\u61601 
\f0  Jupyter Notebook is one of the most popular tools for data science and machine learning. Part 2 Six scenarios: Machine learning for business Each chapter in this part covers the use of machine learning to enhance the productivity of a single operational area. The areas covered are 
\f1 \uc0\u9632 
\f0  Approving purchase orders 
\f1 \uc0\u9632 
\f0  Evaluating which customers are at risk of churning 
\f1 \uc0\u9632 
\f0  Identifying support requests that should be escalated immediately 
\f1 \uc0\u9632 
\f0  Auditing invoices from suppliers 
\f1 \uc0\u9632 
\f0  Forecasting operational trends, such as power consumption, using timeseries data 
\f1 \uc0\u9632 
\f0  Incorporating additional features into time-series forecasting In each chapter, you download a Jupyter notebook and a dataset and upload it to your AWS SageMaker account. You can then run the notebook to create and test the model. The text in the chapter takes you through the notebook in detail so you understand how it works. You can then use the notebook with datasets from your company to accomplish the same task. 25 Should you send a purchase order to a technical approver? In this chapter, you\'92ll work end to end through a machine learning scenario that makes a decision about whether to send an order to a technical approver or not. And you\'92ll do it without writing any rules! All you\'92ll feed into the machine learning system is a dataset consisting of 1,000 historical orders and a flag that indicates whether that order was sent to a technical approver or not. The machine learning system figures out the patterns from those 1,000 examples and, when given a new order, will be able to correctly determine whether to send the order to a technical approver. In the first chapter, you\'92ll read about Karen, the person who works in the purchasing department. Her job is to receive requisitions from staff to buy a product or service. For each request, Karen decides which approver needs to review and approve the order; then, after getting approval, she sends the request to the supplier. Karen might not think of herself this way, but she\'92s a decision maker. As requests to This chapter covers 
\f2 \uc0\u61601 
\f0  Identifying a machine learning opportunity 
\f2 \uc0\u61601 
\f0  Identifying what and how much data is required 
\f2 \uc0\u61601 
\f0  Building a machine learning system 
\f2 \uc0\u61601 
\f0  Using machine learning to make decisions 26 CHAPTER 2 Should you send a purchase order to a technical approver? buy products and services come in, Karen decides who needs to approve each request. For some products, such as computers, Karen needs to send the request to a technical advisor, who determines if the specification is suitable for the person buying the computer. Does Karen need to send this order to a technical approver, or not? This is the decision you\'92ll work on in this chapter. By the end of this chapter, you\'92ll be able to help Karen out. You\'92ll be able to put together a solution that will look at the requests before they get to Karen and then recommend whether she should send the request to a technical approver. As you work through the examples, you\'92ll become familiar with how to use machine learning to make a decision. 2.1 The decision Figure 2.1 shows Karen\'92s process from a requester placing an order to the supplier receiving the order. Each person icon in the workflow represents a person taking some action. If they have more than one arrow pointing away from them, they need to make a decision. Requester sends an order to Karen Karen decides whether the order requires technical approval Technical approver decides whether to approve or reject Financial approver decides whether to approve or reject Requester Karen Technical Approver 1 2 3 Financial Approver Supplier Figure 2.1 Approval workflow for purchasing technical equipment in Karen\'92s company The data 27 In Karen\'92s process, there are three decisions (numbered 1, 2, and 3): 1 The first decision is the one we are going to look at in this chapter: should Karen send this order to a technical approver? 2 The second decision is made by the technical approver after Karen routes an order to them: should the technical approver accept the order and send it to finance, or should it be rejected and sent back to the requester? 3 The third decision is made by the financial approver: should they approve the order and send it to the supplier, or should it be rejected and sent back to the requester? Each of these decisions may be well suited for machine learning\'97and, in fact, they are. Let\'92s look at the first decision (Karen\'92s decision) in more detail to understand why it\'92s suitable for machine learning. 2.2 The data In discussions with Karen, you\'92ve learned that the approach she generally takes is that if a product looks like an IT product, she\'92ll send it to a technical approver. The exception to her rule is that if it\'92s something that can be plugged in and used, such as a mouse or a keyboard, she doesn\'92t send it for technical approval. Nor does she send it for technical approval if the requester is from the IT department. Table 2.1 shows the dataset you will work with in this scenario. This dataset contains the past 1,000 orders that Karen has processed. It\'92s good practice when preparing a labeled dataset for supervised machine learning to put the target variable in the first column. In this scenario, your target variable is this: should Karen send the order to a technical approver? In your dataset, if Karen sent the order for technical approval, you put a 1 in the tech_approval_required column. If she did not send the order for technical approval, you put a 0 in that column. The rest of the columns are features. These are things that you think are going to be useful in determining whether an item should be sent to an approver. Just like target variables come in two types, categorical and continuous, features also come in two types: categorical and continuous. Categorical features in table 2.1 are those in the requester_id, role, and product columns. A categorical feature is something that can be divided into a number of distinct groups. These are often text rather than numbers, as you can see in the following columns: 
\f2 \uc0\u61601 
\f0  requester_id\'97ID of the requester. 
\f2 \uc0\u61601 
\f0  role\'97If the requester is from the IT department, these are labeled tech. 
\f2 \uc0\u61601 
\f0  product\'97The type of product. Continuous features are those in the last three columns in table 2.1. Continuous features are always numbers. The continuous features in this dataset are quantity, price, and total. 28 CHAPTER 2 Should you send a purchase order to a technical approver? The fields selected for this dataset are those that will allow you to replicate Karen\'92s decision-making process. There are many other fields that you could have selected, and there are some very sophisticated tools being released that help in selecting those features. But for the purposes of this scenario, you\'92ll use your intuition about the problem you\'92re solving to select your features. As you\'92ll see, this approach can quickly lead to some excellent results. Now you\'92re ready to do some machine learning: 
\f2 \uc0\u61601 
\f0  Your end goal is to be able to submit an order to the machine learning model and have it return a result that recommends sending the order to a technical approver or not. 
\f2 \uc0\u61601 
\f0  You have identified the features you\'92ll use to make the decision (the type of product and whether the requester is from the IT department). 
\f2 \uc0\u61601 
\f0  You have created your labeled historical dataset (the dataset shown in table 2.1). 2.3 Putting on your training wheels Now that you have your labeled dataset, you can train a machine learning model to make decisions. But what is a model and how do you train it? You\'92ll learn more about how machine learning works in the following chapters. For now, all you need to know is that a machine learning model is a mathematical function that is rewarded for guessing right and punished for guessing wrong. In order to get more guesses right, the function associates certain values in each feature with right guesses or wrong guesses. As it works through more and more samples, it gets better at guessing. When it\'92s run through all the samples, you say that the model is trained. Table 2.1 Technical Approval Required dataset contains information from prior orders received by Karen. tech_approval_required requester_id role product quantity price total 0 E2300 tech Desk 1 664 664 0 E2300 tech Keyboard 9 649 5841 0 E2374 non-tech Keyboard 1 821 821 1 E2374 non-tech Desktop Computer 24 655 15720 0 E2327 non-tech Desk 1 758 758 0 E2354 non-tech Desk 1 576 576 1 E2348 non-tech Desktop Computer 21 1006 21126 0 E2304 tech Chair 3 155 465 0 E2363 non-tech Keyboard 1 1028 1028 0 E2343 non-tech Desk 3 487 1461 Running the Jupyter notebook and making predictions 29 The mathematical function that underlies a machine learning model is called the machine learning algorithm. Each machine learning algorithm has a number of parameters you can set to get a better-performing model. In this chapter, you are going to accept all of the default settings for the algorithm you\'92ll use. In subsequent chapters, we\'92ll discuss how to fine tune the algorithm to get better results. One of the most confusing aspects for machine learning beginners is deciding which machine learning algorithm to use. In the supervised machine learning exercises in this book, we focus on just one algorithm: XGBoost. XGBoost is a good choice because 
\f2 \uc0\u61601 
\f0  It is fairly forgiving; it works well across a wide range of problems without significant tuning. 
\f2 \uc0\u61601 
\f0  It doesn\'92t require a lot of data to provide good results. 
\f2 \uc0\u61601 
\f0  It is easy to explain why it returns a particular prediction in a certain scenario. 
\f2 \uc0\u61601 
\f0  It is a high-performing algorithm and the go-to algorithm for many participants in machine learning competitions with small datasets. In a later chapter, you\'92ll learn how XGBoost works under the hood, but for now, let\'92s discuss how to use it. If you want to read more about it on the AWS site, you can do so here: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html. NOTE If you haven\'92t already installed and configured all the tools you\'92ll need as you work through this chapter and the book, visit appendixes A, B, and C, and follow the instructions you find there. After working your way through the appendixes (to the end of appendix C), you\'92ll have your dataset stored in S3 (AWS\'92s file storage service) and a Jupyter notebook set up and running in SageMaker. 2.4 Running the Jupyter notebook and making predictions Let\'92s step through the Jupyter notebook and make predictions about whether to send an order to a technical approver or not. In this chapter, we will look at the notebook in six parts: 
\f2 \uc0\u61601 
\f0  Load and examine the data. 
\f2 \uc0\u61601 
\f0  Get the data into the right shape. 
\f2 \uc0\u61601 
\f0  Create training, validation, and test datasets. 
\f2 \uc0\u61601 
\f0  Train the machine learning model. 
\f2 \uc0\u61601 
\f0  Host the machine learning model. 
\f2 \uc0\u61601 
\f0  Test the model and use it to make decisions. To follow along, you should have completed two things: 
\f2 \uc0\u61601 
\f0  Load the dataset orders_with_predicted_value.csv into S3. 
\f2 \uc0\u61601 
\f0  Upload the Jupyter notebook tech_approval_required.ipynb to SageMaker. 30 CHAPTER 2 Should you send a purchase order to a technical approver? Appendixes B and C take you through how to do each of these steps in detail for the dataset you\'92ll use in this chapter. In summary: 
\f2 \uc0\u61601 
\f0  Download the dataset at https://s3.amazonaws.com/mlforbusiness/ch02/orders _with_predicted_value.csv. 
\f2 \uc0\u61601 
\f0  Upload the dataset to your S3 bucket that you have set up to hold the datasets for this book. 
\f2 \uc0\u61601 
\f0  Download the Jupyter notebook at https://s3.amazonaws.com/mlforbusiness/ ch02/tech_approval_required.ipynb. 
\f2 \uc0\u61601 
\f0  Upload the Jupyter notebook to your SageMaker notebook instance. Don\'92t be frightened by the code in the Jupyter notebook. As you work through this book, you\'92ll become familiar with each aspect of it. In this chapter, you\'92ll run the code rather than edit it. In fact, in this chapter, you do not need to modify any of the code with the exception of the first two lines, where you tell the code which S3 bucket to use and which folder in that bucket contains your dataset. To start, open the SageMaker service from the AWS console in your browser by logging into the AWS console at http:// console.aws.amazon.com. Click Notebook Instances in the left-hand menu on SageMaker (figure 2.2). This takes you to a screen that shows your notebook instances. If the notebook you uploaded in appendix C is not running, you will see a screen like the one shown in figure 2.3. Click the Start link to start the notebook instance. Once you have started your notebook instance, or if your notebook was already started, you\'92ll see a screen like that shown in figure 2.4. Click the Open link. Select Notebook instances. Figure 2.2 Selecting a notebook instance from the Amazon SageMaker menu Select Start. mlforbusiness-ch02 Figure 2.3 Notebook instance with a Stopped status Running the Jupyter notebook and making predictions 31 When you click the Open link, a new tab opens in your browser, and you\'92ll see the ch02 folder you created in appendix C (figure 2.5). Finally, when you click ch02, you\'92ll see the notebook you uploaded in appendix C: tech-approval-required.ipynb. Click this notebook to open it in a new browser tab (figure 2.6). Figure 2.7 shows a Jupyter notebook. Jupyter notebooks are an amazing coding environment that combines text with code in sections. An example of the text cell is the Select Open Jupyter. mlforbusiness-ch02 Figure 2.4 Opening Jupyter from a notebook instance Select to open the ch02 folder. Figure 2.5 Selecting the ch02 folder Select this link. Figure 2.6 Opening the tech-approval-required notebook 32 CHAPTER 2 Should you send a purchase order to a technical approver? text for heading 2.4.1, Part 1: Load and examine the data. An example of a code cell are the following lines: data_bucket = 'mlforbusiness' subfolder = 'ch02' dataset = 'orders_with_predicted_value.csv' To run the code in a Jupyter notebook, press C+R when your cursor is in a code cell. 2.4.1 Part 1: Loading and examining the data The code in listings 2.1 through 2.4 loads the data so you can look at it. The only two values in this entire notebook that you need to modify are the data_bucket and the subfolder. You should use the bucket and subfolder names you set up as per the instructions in appendix B. NOTE This walkthrough will just familiarize you with the code so that, when you see it again in subsequent chapters, you\'92ll know how it fits into the SageMaker workflow. The following listing shows how to identify the bucket and subfolder where the data is stored. Code cell Figure 2.7 Inside the Jupyter notebook Running the Jupyter notebook and making predictions 33 data_bucket = 'mlforbusiness' subfolder = 'ch02' dataset = 'orders_with_predicted_value.csv' As you\'92ll recall, a Jupyter notebook is where you can write and run code. There are two ways you can run code in a Jupyter notebook. You can run the code in one of the cells, or you can run the code in more than one of the cells. To run the code in one cell, click the cell to select it, and then press C+R. When you do so, you\'92ll see an asterisk (*) appear to left of the cell. This means that the code in the cell is running. When the asterisk is replaced by a number, the code has finished running. The number shows how many cells have been run since you opened the notebook. If you want, after you have updated the name of the data bucket and the subfolder (listing 2.1), you can run the notebook. This loads the data, builds and trains the machine learning model, sets up the endpoint, and generates predictions from the test data. SageMaker takes about 10 min to complete these actions for the datasets you\'92ll use in this book. It may take longer if you load large datasets from your company. To run the entire notebook, click Cell in the toolbar at the top of the Jupyter notebook, then click Run All (figure 2.8). Listing 2.1 Setting up the S3 bucket and subfolder S3 bucket where the data is stored Subfolder of S3 bucket where the data is stored Dataset used to train and test the model Select Run All. Figure 2.8 Running the Jupyter notebook 34 CHAPTER 2 Should you send a purchase order to a technical approver? SETTING UP THE NOTEBOOK Next, you\'92ll set up the Python libraries required by the notebook (listing 2.2). To run the notebook, you don\'92t need to change any of these values: 
\f2 \uc0\u61601 
\f0  pandas\'97A Python library commonly used in data science projects. In this book, we\'92ll touch only the surface of what pandas can do. You\'92ll load pandas as pd. As you\'92ll see later in this chapter, this means that we will preface any use of any module in the pandas library with pd. 
\f2 \uc0\u61601 
\f0  boto3 and sagemaker\'97The libraries created by Amazon to help Python users interact with AWS resources: boto3 is used to interact with S3, and sagemaker, unsurprisingly, is used to interact with SageMaker. You will also use a module called s3fs, which makes it easier to use boto3 with S3. 
\f2 \uc0\u61601 
\f0  sklearn\'97The final library you\'92ll import. It is short for scikit-learn, which is a comprehensive library of machine learning algorithms that is used widely in both the commercial and scientific communities. Here we only import the train_test_split function that we\'92ll use later. You\'92ll also need to create a role on SageMaker that allows the sagemaker library to use the resources it needs to build and serve the machine learning application. You do this by calling the sagemaker function get_execution_role. import pandas as pd import boto3 import sagemaker import s3fs from sklearn.model_selection \\ import train_test_split role = sagemaker.get_execution_role() s3 = s3fs.S3FileSystem(anon=False) As a reminder, as you walk through each of the cells in the Jupyter notebook, to run the code in a cell, click the cell and press C+R. LOADING AND VIEWING THE DATA Now that you\'92ve identified the bucket and subfolder and set up the notebook, you can take a look at the data. The best way to view the data is to use the pandas library you imported in listing 2.2. The code in listing 2.3 creates a pandas data structure called a DataFrame. You can think of a DataFrame as a table like a spreadsheet. The first line assigns the name df to the DataFrame. The data in the DataFrame is the orders data from S3. It is read into Listing 2.2 Importing modules Imports the pandas Python library Imports the boto3 AWS library Imports SageMaker Imports the s3fs module to make working with S3 files easier Imports only the train_test_split module from the sklearn library Creates a role in SageMaker Establishes the connection with S3 Running the Jupyter notebook and making predictions 35 the DataFrame by using the pandas function read_csv. The line df.head() displays the first five rows of the df DataFrame. df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/\{dataset\}') df.head() Running the code displays the top five rows in the dataset. (To run the code, insert your cursor in the cell and press C+R.) The dataset will look similar to the dataset in table 2.2. To recap, the dataset you uploaded to S3 and are now displaying in the df DataFrame lists the last 1,000 orders that Karen processed. She sent some of those orders to a technical approver, and some she did not. The code in listing 2.4 displays how many rows are in the dataset and how many of the rows were sent for technical approval. Running this code shows that out of 1,000 rows in the dataset, 807 were not sent to a technical approver, and 193 were sent. print(f'Number of rows in dataset: \{df.shape[0]\}') print(df[df.columns[0]].value_counts()) In listing 2.4, the shape property of a DataFrame provides information about the number of rows and the number of columns. Here df.shape[0] shows the number of rows, and df.shape[1] shows the number of columns. The value_counts property of the df DataFrame shows the number of rows in the dataset where the order was sent to a technical approver. It contains a 1 if it was sent for technical approval and a 0 if it was not. Listing 2.3 Viewing the dataset Table 2.2 Technical Approval Required dataset displayed in Excel tech_approval_required requester_id role product quantity price total 0 E2300 tech Desk 1 664 664 0 E2300 tech Keyboard 9 649 5841 0 E2374 non-tech Keyboard 1 821 821 1 E2374 non-tech Desktop Computer 24 655 15720 0 E2327 non-tech Desk 1 758 758 Listing 2.4 Determining how many rows were sent for technical approval Reads the S3 orders_with_predicted_value .csv dataset in listing 2.1 Displays the top five rows of the DataFrame loaded in the line above Displays the number of rows You don\'92t need to understand this line of code. The output displays the number of rows that went to a technical approver and the number of rows that did not. 36 CHAPTER 2 Should you send a purchase order to a technical approver? 2.4.2 Part 2: Getting the data into the right shape For this part of the notebook, you\'92ll prepare the data for use in the machine learning model. You\'92ll learn more about this topic in later chapters but, for now, it is enough to know that there are standard approaches to preparing data, and we are using one we\'92ll apply to each of our machine learning exercises. One important point to understand about most machine learning models is that they typically work with numbers rather than text-based data. We\'92ll discuss why this is so in a subsequent chapter when we go into the details of the XGBoost algorithm. For now, it is enough to know that you need to convert the text-based data to numerical data before you can use it to train your machine learning model. Fortunately, there are easy-to-use tools that will help with this. First, we\'92ll use the pandas get_dummies function to convert all of the text data into numbers. It does this by creating a separate column for every unique text value. For example, the product column contains text values such as Desk, Keyboard, and Mouse. When you use the get_dummies function, it turns every value into a column and places a 0 or 1 in the row, depending on whether the row contains a value or not. Table 2.3 shows a simple table with three rows. The table shows the price for a desk, a keyboard, and a mouse. When you use the get_dummies function, it takes each of the unique values in the non-numeric columns and creates new columns from those. In our example, this looks like the values in table 2.4. Notice that the get_dummies function removes the product column and creates three columns from the three unique values in the dataset. It also places a 1 in the new column that contains the value from that row and zeros in every other column. Table 2.3 Simple three-row dataset with prices for a desk, a keyboard, and a mouse product price Desk 664 Keyboard 69 Mouse 89 Table 2.4 The three-row dataset after applying the get_dummies function price product_Desk product_Keyboard product_Mouse 664 1 0 0 69 0 1 0 89 0 0 1 Running the Jupyter notebook and making predictions 37 Listing 2.5 shows the code that creates table 2.4. To run the code, insert your cursor in the cell, and press C+R. You can see that this dataset is very wide (111 columns in our example). encoded_data = pd.get_dummies(df) encoded_data.head() A machine learning algorithm can now work with this data because it is all numbers. But there is a problem. Your dataset is now probably very wide. In our sample dataset in figures 2.3 and 2.4, the dataset went from 2 columns wide to 4 columns wide. In a real dataset, it can go to thousands of columns wide. Even our sample dataset in the SageMaker Jupyter notebook goes to 111 columns when you run the code in the cells. This is not a problem for the machine learning algorithm. It can easily handle datasets with thousands of columns. It\'92s a problem for you because it becomes more difficult to reason about the data. For this reason, and for the types of machine learning decision-making problems we look at in this book, you can often get results that are just as accurate by reducing the number of columns to only the most relevant ones. This is important for your ability to explain to others what is happening in the algorithm in a way that is convincing. For example, in the dataset you work with in this chapter, the most highly correlated columns are the ones relating to technical product types and the ones relating to whether the requester has a tech role or not. This makes sense, and it can be explained concisely to others. A relevant column for this machine learning problem is a column that contains values that are correlated to the value you are trying to predict. You say that two values are correlated when a change in one value is accompanied by a change in another value. If these both increase or decrease together, you say that they are positively correlated\'97 they both move in the same direction. And when one goes up and the other goes down (or vice versa), you say that they are negatively correlated\'97they move in opposite directions. For our purposes, the machine learning algorithm doesn\'92t really care whether a column is positively or negatively correlated, just that it is correlated. Correlation is important because the machine learning algorithm is trying to predict a value based on the values in other columns in the dataset. The values in the dataset that contribute most to the prediction are those that are correlated to the predicted value. You\'92ll find the most correlated columns by applying another pandas function called corr. You apply the corr function by appending .corr() to the pandas DataFrame you named encoded_data in listing 2.5. After the function, you need to provide the name of the column you are attempting to predict. In this case, the column you are attempting to predict is the tech_approval_required column. Listing 2.6 shows the Listing 2.5 Converting text values to columns Creates a new pandas DataFrame to store the table with columns for each unique text value in the original table The pandas function to display the first five rows of the table 38 CHAPTER 2 Should you send a purchase order to a technical approver? code that does this. Note that the .abs() function at the end of the listing is simply turning all of the correlations positive. corrs = encoded_data.corr()[ 'tech_approval_required' ].abs() columns = corrs[corrs > .1].index corrs = corrs.filter(columns) corrs The code in listing 2.6 identifies the columns that have a correlation greater than 10%. You don\'92t need to know exactly how this code works. You are simply finding all of the columns that have a correlation greater than 10% with the tech_approval_required column. Why 10%? It removes the irrelevant noise from the dataset which, while this step does not help the machine learning algorithm, improves your ability to talk about the data in a meaningful way. With fewer columns to consider, you can more easily prepare an explanation of what the algorithm is doing. Table 2.5 shows the columns with a correlation greater than 10%. Now that you have identified the most highly correlated columns, you need to filter the encoded_data table to contain just those columns. You do so with the code in Listing 2.6 Identifying correlated columns Table 2.5 Correlation with predicted value Column name Correlation with predicted value tech_approval_required 1.000000 role_non-tech 0.122454 role_tech 0.122454 product_Chair 0.134168 product_Cleaning 0.191539 product_Desk 0.292137 product_Desktop Computer 0.752144 product_Keyboard 0.242224 product_Laptop Computer 0.516693 product_Mouse 0.190708 Creates a series (a DataFrame with just one column) called corrs that lists all the columns in the 111-column dataset you created with the code in listing 2.5 Identifies the columns that have a correlation greater than 10% Filters corrs to just the columns with a correlation greater than 10% Jupyter notebooks display the output of the last line in a cell. Because the last line is the name of a variable, it displays the variable when you run the code in a cell. Running the Jupyter notebook and making predictions 39 listing 2.7. The first line filters the columns to just the correlated columns, and the second line displays the table when you run the code. (Remember, to run the code, insert your cursor in the cell and press C+R.) encoded_data = encoded_data[columns] encoded_data.head() 2.4.3 Part 3: Creating training, validation, and test datasets The next step in the machine learning process is to create a dataset that you\'92ll use to train the algorithm. While you\'92re at it, you\'92ll also create the dataset you\'92ll use to validate the results of the training and the dataset you\'92ll use to test the results. To do this, you\'92ll split the dataset into three parts: 
\f2 \uc0\u61601 
\f0  Train 
\f2 \uc0\u61601 
\f0  Validate 
\f2 \uc0\u61601 
\f0  Test The machine learning algorithm uses the training data to train the model. You should put the largest chunk of data into the training dataset. The validation data is used by the algorithm to determine whether the algorithm is improving. This should be the next-largest chunk of data. Finally, the test data, the smallest chunk, is used by you to determine how well the algorithm performs. Once you have the three datasets, you will convert them to CSV format and then save them to S3. In listing 2.8, you create two datasets: a dataset with 70% of the data for training and a dataset with 30% of the data for validation and testing. Then you split the validation and test data into two separate datasets: a validation dataset and a test dataset. The validation dataset will contain 20% of the total rows, which equals 66.7% of the validation and test dataset. The test dataset will contain 10% of the total rows, which equals 33.3% of the validation and test dataset. To run the code, insert your cursor in the cell and press C+R. train_df, val_and_test_data = train_test_split( encoded_data, test_size=0.3, random_state=0) val_df, test_df = train_test_split( val_and_test_data, test_size=0.333, random_state=0) Listing 2.7 Showing only correlated columns Listing 2.8 Splitting data into train, validate, and test datasets Filters columns to only those that are correlated with the tech_approval_required column Displays the table when you run the code in the cell Puts 70% of the data into the train dataset Puts 20% of the data in to the validation data and 10% of the data into the test dataset 40 CHAPTER 2 Should you send a purchase order to a technical approver? NOTE The random_state argument ensures that repeating the command splits the data in the same way. In listing 2.8, you split the data into three DataFrames. In listing 2.9, you\'92ll convert the datasets to CSV format. Listing 2.9 shows how to use the pandas function to_csv to create a CSV dataset from the pandas DataFrames you created in listing 2.8. To run the code, insert your cursor in the cell and press C+R. train_data = train_df.to_csv(None, header=False, index=False).encode() val_data = val_df.to_csv(None, header=False, index=False).encode() test_data = test_df.to_csv(None, header=True, index=False).encode() The None argument in the to_csv function indicates that you do not want to save to a file. The header argument indicates whether the column names will be included in Input and CSV formats CSV is one of the two formats you can use as input to the XGBoost machine learning model. The code in this book uses CSV format. That\'92s because if you want to view the data in a spreadsheet, it\'92s easy to import into spreadsheet applications like Microsoft Excel. The drawback of using CSV format is that it takes up a lot of space if you have a dataset with lots of columns (like our encoded_data dataset after using the get_dummies function in listing 2.5). The other format that XBGoost can use is libsvm. Unlike a CSV file, where even the columns containing zeros need to be filled out, the libsvm format only includes the columns that do not contain zeros. It does this by concatenating the column number and the value together. So the data you looked at in table 2.2 would look like this: 1:664 2:1 1:69 3:1 1:89 4:1 The first entry in each row shows the price (664, 69, or 89). The number in front of the price indicates that this is in the first column of the dataset. The second entry in each row contains the column number (2, 3, or 4) and the non-zero value of the entry (which in our case is always 1). So 1:89 4:1 means that the first column in the row contains the number 89, and the fourth column contains the number 1. All the other values are zero. You can see that using libsvm over CSV has changed the width of our dataset from four columns to two columns. But don\'92t get too hung up on this. SageMaker and XGBoost work just fine with CSV files with thousands of columns; but if your dataset is very wide (tens of thousands of columns), you might want to use libsvm instead of CSV. Otherwise, use CSV because it\'92s easier to work with. Listing 2.9 Converting the data to CSV Running the Jupyter notebook and making predictions 41 the CSV file or not. For the train_data and val_data datasets, you don\'92t include the column headers (header=False) because the machine learning algorithm is expecting each column to contain only numbers. For the test_data dataset, it is best to include headers because you\'92ll be running the trained algorithm against the test data, and it is helpful to have column names in the data when you do so. The index=False argument tells the function to not include a column with the row numbers. The encode() function ensures that the text in the CSV file is in the right format. NOTE Encoding text in the right format can be one of the most frustrating parts of machine learning. Fortunately, a lot of the complexity of this is handled by the pandas library, so you generally won\'92t have to worry about that. Just remember to always use the encode() function if you save the file to CSV. In listing 2.9, you created CSV files from the train, val, and test DataFrames. However, the CSV files are not stored anywhere yet other than in the memory of the Jupyter notebook. In listing 2.10, you will save the CSV files to S3. In listing 2.2 in the fourth line, you imported a Python module called s3fs. This module makes it easy to work with S3. In the last line of the same listing, you assigned the variable s3 to the S3 filesystem. You will now use this variable to work with S3. To do this, you\'92ll use Python\'92s with...open syntax to indicate the filename and location, and the write function to write the contents of a variable to that location (listing 2.10). Remember to use 'wb' when creating the file to indicate that you are writing the contents of the file in binary mode rather than text mode. (You don\'92t need to know how this works, just that it allows the file to be read back exactly as it was saved.) To run the code, insert your cursor in the cell and press C+R. with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/train.csv', 'wb') as f: f.write(train_data) with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/val.csv', 'wb') as f: f.write(val_data) with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/test.csv', 'wb') as f: f.write(test_data) 2.4.4 Part 4: Training the model Now you can start training the model. You don\'92t need to understand in detail how this works at this point, just what it is doing, so the code in this part will not be annotated to the same extent as the code in the earlier listings. First, you need to load your CSV data into SageMaker. This is done using a SageMaker function called s3_input. In listing 2.11, the s3_input files are called train_input and test_input. Note that you don\'92t need to load the test.csv file into SageMaker because Listing 2.10 Saving the CSV file to S3 Writes train.csv to S3 Writes val.csv to S3 Writes test.csv to S3 42 CHAPTER 2 Should you send a purchase order to a technical approver? it is not used to train or validate the model. Instead, you will use it at the end to test the results. To run the code, insert your cursor in the cell and press C+R. train_input = sagemaker.s3_input( s3_data=f's3://\{data_bucket\}/\{subfolder\}/processed/train.csv', content_type='csv') val_input = sagemaker.s3_input( s3_data=f's3://\{data_bucket\}/\{subfolder\}/processed/val.csv', content_type='csv') The next listing (listing 2.12) is truly magical. It is what allows a person with no systems engineering experience to do machine learning. In this listing, you train the machine learning model. This sounds simple, but the ease with which you can do this using SageMaker is a massive step forward from having to set up your own infrastructure to train a machine learning model. In listing 2.12, you 1 Set up a variable called sess to store the SageMaker session. 2 Define in which container AWS will store the model (use the containers given in listing 2.12). 3 Create the model (which is stored in the variable estimator in listing 2.12). 4 Set hyperparameters for the estimator. You will learn more about each of these steps in subsequent chapters, so you don\'92t need to understand this deeply at this point in the book. You just need to know that this code will create your model, start a server to run your model, and then train the model on the data. If you click C+R in this notebook cell, the model runs. sess = sagemaker.Session() container = sagemaker.amazon.amazon_estimator.get_image_uri( boto3.Session().region_name, 'xgboost', 'latest') estimator = sagemaker.estimator.Estimator( container, role, train_instance_count=1, train_instance_type='ml.m5.large', output_path= \\ f's3://\{data_bucket\}/\{subfolder\}/output', sagemaker_session=sess) estimator.set_hyperparameters( max_depth=5, subsample=0.7, objective='binary:logistic', Listing 2.11 Preparing the CSV data for SageMaker Listing 2.12 Training the model Sets the type of server that SageMaker uses to run your model Stores the output of the model at this location in S3 SageMaker has some very sophisticated hyperparameter tuning capability. To use the tuning function, you just need to make sure you set the objective correctly. For the current dataset, where you are trying to predict a 1 or 0, you should set this as binary:logistic. We\'92ll cover this in more detail in later chapters. Running the Jupyter notebook and making predictions 43 eval_metric = 'auc', num_round=100, early_stopping_rounds=10) estimator.fit(\{'train': train_input, 'validation': val_input\}) It takes about 5 minutes to train the model, so you can sit back and reflect on how happy you are to not be manually configuring a server and installing the software to train your model. The server only runs for about a minute, so you will only be charged for about a minute of compute time. At the time of writing of this book, the m5-large server was priced at under US$0.10 per hour. Once you have stored the model on S3, you can use it again whenever you like without retraining the model. More on this in later chapters. 2.4.5 Part 5: Hosting the model The next section of the code is also magical. In this section, you will launch another server to host the model. This is the server that you will use to make predictions from the trained model. Again, at this point in the book, you don\'92t need to understand how the code in the listing in this section works\'97just that it\'92s creating a server that you\'92ll use to make predictions. The code in listing 2.13 calls this endpoint order-approval and uses Python\'92s try-except block to create it. A try-except block tries to run some code, and if there is an error, it runs the code after the except line. You do this because you only want to set up the endpoint if you haven\'92t already set one up with that name. The listing tries to set up an endpoint called order-approval. If there is no endpoint named order-approval, then it sets one up. If there is an order-approval endpoint, the try code generates an error, and the except code runs. The except code in this case simply says to use the endpoint named order-approval. endpoint_name = 'order-approval' try: sess.delete_endpoint( sagemaker.predictor.RealTimePredictor( endpoint=endpoint_name).endpoint) print('Warning: Existing endpoint deleted\\ to make way for your new endpoint.') except: pass Listing 2.13 Hosting the model Tells SageMaker to tune the hyperparameters to achieve the best area under curve result. Again, we\'92ll cover this in more detail in later chapters. The maximum number of rounds of training that will occur. This is covered in more detail in chapter 3. The number of rounds that will occur where there is no improvement in the model before training is terminated 44 CHAPTER 2 Should you send a purchase order to a technical approver? predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium', endpoint_name=endpoint_name) from sagemaker.predictor import csv_serializer, json_serializer predictor.content_type = 'text/csv' predictor.serializer = csv_serializer predictor.deserializer = None The code in listing 2.13 sets up a server sized as a t2.medium server. This is a smaller server than the m5.large server we used to train the model because making predictions from a model is less computationally intensive than creating the model. Both the try block and the except block create a variable called predictor that you\'92ll draw on to test and use the model. The final four lines in the code set up the predictor to work with the CSV file input so you can work with it more easily. Note that when you click C+R in the notebook cell, the code will take another 5 minutes or so to run the first time. It takes time because it is setting up a server to host the model and to create an endpoint so you can use the model. 2.4.6 Part 6: Testing the model Now that you have the model trained and hosted on a server (the endpoint named predictor), you can start using it to make predictions. The first three lines of the next listing create a function that you\'92ll apply to each row of the test data. def get_prediction(row): prediction = round(float(predictor.predict(row[1:]).decode('utf-8'))) return prediction with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/test.csv') as f: test_data = pd.read_csv(f) test_data['prediction'] = test_data.apply(get_prediction, axis=1) test_data.set_index('prediction', inplace=True) test_data The function get_prediction in listing 2.14 takes every column in the test data (except for the first column, because that is the value you are trying to predict), sends it to the predictor, and returns the prediction. In this case, the prediction is 1 if the order should be sent to an approver, and 0 if it should not be sent to an approver. The next two lines open the test.csv file and read the contents into a pandas DataFrame called test_data. You can now work with this DataFrame in the same way you worked with the original dataset in listing 2.7. The final three lines apply the function created in the first three lines of the listing. Listing 2.14 Getting the predictions The type of server you are using, in this case, an ml.t2.medium server. Running the Jupyter notebook and making predictions 45 When you click C+R in the cell containing the code in listing 2.14, you see the results of each row in the test file. Table 2.6 shows the top two rows of the test data. Each row represents a single order. For example, if an order for a desk was placed by a person in a technical role, the role_tech and the product_desk columns would have a 1. All other columns would be 0. The 1 in the prediction column in first row says that the model predicts that this order should be sent to a technical approver. The 1 in the tech_approval_required column says that in your test data, this order was labeled as requiring technical approval. This means that the machine learning model predicted this order correctly. To see why, look at the values in the columns to the right of the tech_approval_ required column. You can see that this order was placed by someone who is not in a technical role because there is a 1 in the role_non-tech column and a 0 in the role_tech column. And you can see that the product ordered was a desktop computer because the product_Desktop Computer column has a 1. The 0 in the prediction column in the second row says that the model predicts this order does not require technical approval. The 0 in the tech_approval_required column, because it is the same value as that in the prediction column, says that the model predicted this correctly. The 1 in the role_non-tech column says this order was also placed by a non-technical person. However, the 1 in the product_Cleaning column indicates that the order was for cleaning products; therefore, it does not require technical approval. As you look through the results, you can see that your machine learning model got almost every test result correct! You have just created a machine learning model that can correctly decide whether to send orders to a technical approver, all without writing any rules. To determine how accurate the results are, you can test how many of the predictions match the test data, as shown in the following listing. (test_data['prediction'] == \\ test_data['tech_approval_required']).mean() Table 2.6 Test results from the predictor prediction tech_ approval_ required role_ non-tech role_ tech product_ Chair product_ Cleaning product_ Desk product_ Desktop Computer product_ Keyboard product_ Laptop Computer product_ Mouse 1 1 1 00 0 0 1 0 0 0 0 0 1 00 1 0 0 0 0 0 Listing 2.15 Testing the model Displays the percentage of predictions that match the test dataset 46 CHAPTER 2 Should you send a purchase order to a technical approver? 2.5 Deleting the endpoint and shutting down your notebook instance It is important that you shut down your notebook instance and delete your endpoints when you are not using them. If you leave them running, you will be charged for each second they are up. The charges for the machines we use in this book are not large (if you were to leave a notebook instance or endpoint on for a month, it would cost about US$20). But, there is no point in paying for something you are not using. 2.5.1 Deleting the endpoint To delete the endpoint, click Endpoints on the left-hand menu you see when you are looking at the SageMaker tab (figure 2.9). You will see a list of all of your running endpoints (figure 2.10). To ensure you are not charged for endpoints you are not using, you should delete all of the endpoints you Select Endpoints. Figure 2.9 Selecting the endpoint for deletion Shows endpoint is active Figure 2.10 Viewing the active endpoint(s) Deleting the endpoint and shutting down your notebook instance 47 are not using (remember that endpoints are easy to create, so even if you will not use the endpoint for only a few hours, you might want to delete it). To delete the endpoint, click the radio button to the left of order-approval, click the Actions menu item, then click the Delete menu item that appears (figure 2.11). You have now deleted the endpoint, so you\'92ll no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you see the text \'93There are currently no resources\'94 displayed on the Endpoints page (figure 2.12). 2.5.2 Shutting down the notebook instance The final step is to shut down the notebook instance. Unlike endpoints, you do not delete the notebook. You just shut it down so you can start it again, and it will have all of the code in your Jupyter notebook ready to go. To shut down the notebook instance, click Notebook instances in the left-hand menu on SageMaker (figure 2.13). To shut down the notebook, select the radio button next to the notebook instance name and click Stop on the Actions menu (figure 2.14). After your notebook instance has shut down, you can confirm that it is no longer running by checking the Status to ensure it says Stopped. 1. Select the radio button. 2. Select Delete. Figure 2.11 Deleting the endpoint Endpoint successfully deleted Figure 2.12 Endpoint deleted 48 CHAPTER 2 Should you send a purchase order to a technical approver? This chapter was all about helping Karen decide whether to send an order to a technical approver. In this chapter, you worked end to end through a machine learning scenario. The scenario you worked through involved how to decide whether to send an order to a technical approver. The skills you learned in this chapter will be used throughout the rest of the book as you work through other examples of using machine learning to make decisions in business automation. Summary 
\f2 \uc0\u61601 
\f0  You can uncover machine learning opportunities by identifying decision points. 
\f2 \uc0\u61601 
\f0  It\'92s simple to set up SageMaker and build a machine learning system using AWS SageMaker and Jupyter notebooks. 
\f2 \uc0\u61601 
\f0  You send data to the machine learning endpoints to make predictions. 
\f2 \uc0\u61601 
\f0  You can test the predictions by sending the data to a CSV file for viewing. 
\f2 \uc0\u61601 
\f0  To ensure you are not charged for endpoints you are not using, you should delete all unused endpoints. Select Notebook instances. Figure 2.13 Selecting the notebook instance to prepare for shutdown 1. Select the radio button. 2. Select Stop. mlforbusiness-ch02 Figure 2.14 Shutting down the notebook 49 Should you call a customer because they are at risk of churning? Carlos takes it personally when a customer stops ordering from his company. He\'92s the Head of Operations for a commercial bakery that sells high-quality bread and other baked goods to restaurants and hotels. Most of his customers have used his bakery for a long time, but he still regularly loses customers to his competitors. To help retain customers, Carlos calls those who have stopped using his bakery. He hears a similar story from each of these customers: they like his bread, but it\'92s expensive and cuts into their desired profit margins, so they try bread from another, less expensive bakery. After this trial, his customers conclude that the quality of their meals would still be acceptable even if they served a lower quality bread. Churn is the term used when you lose a customer. It\'92s a good word for Carlos\'92s situation because it indicates that a customer probably hasn\'92t stopped ordering bread; they\'92re just ordering it from someone else. This chapter covers 
\f2 \uc0\u61601 
\f0  Identifying customers who are about to churn 
\f2 \uc0\u61601 
\f0  How to handle imbalanced data in your analysis 
\f2 \uc0\u61601 
\f0  How the XGBoost algorithm works 
\f2 \uc0\u61601 
\f0  Additional practice in using S3 and SageMaker 50 CHAPTER 3 Should you call a customer because they are at risk of churning? Carlos comes to you for help in identifying those customers who are in the process of trying another bakery. Once he\'92s identified the customer, he can call them to determine if there\'92s something he can do to keep them. In Carlos\'92s conversations with his lost customers, he sees a common pattern: 
\f2 \uc0\u61601 
\f0  Customers place orders in a regular pattern, typically daily. 
\f2 \uc0\u61601 
\f0  A customer tries another bakery, thus reducing the number of orders from Carlos\'92s bakery. 
\f2 \uc0\u61601 
\f0  The customer negotiates an agreement with the other bakery, which may or may not result in a temporary resurgence in orders placed with Carlos\'92s bakery. 
\f2 \uc0\u61601 
\f0  Customers stop ordering from his bakery altogether. In this chapter, you are going to help Carlos understand which customers are at risk of churning so he can call them to determine whether there is some way to address their move to another supplier. To help Carlos, you\'92ll look at the business process in a similar way that you looked at Karen\'92s process in chapter 2. For Karen\'92s process, you looked at how orders moved from requester to approver, and the features that Karen used to make a decision about whether to send the order to a technical approver or not. You then built a SageMaker XGBoost application that automated the decision. Similarly, with Carlos\'92s decision about whether to call a customer because they are at risk of churning, you\'92ll build a SageMaker XGBoost application that looks at Carlos\'92s customers each week and makes a decision about whether Carlos should call them. 3.1 What are you making decisions about? At first glance, it looks like you are working with ordering data like you did in chapter 2, where Karen looks at an order and decides whether to send it to a technical approver or not. In this chapter, Carlos reviews a customer\'92s orders and decides whether to call that customer. The difference between Karen\'92s process flow in chapter 2 and Carlos\'92s process flow in this chapter is that, in chapter 2, you made decisions about orders: should Karen send this order to an approver. In this chapter, you make decisions about customers: should Carlos call a customer. This means that instead of just taking the order data and using that as your dataset, you need to first transform the order data into customer data. In later chapters, you\'92ll learn how to use some automated tools to do this, but in this chapter, you\'92ll learn about the process conceptually, and you\'92ll be provided with the transformed dataset. But before we look at the data, let\'92s look at the process we want to automate. 3.2 The process flow Figure 3.1 shows the process flow. You start with the Orders database, which contains records of which customers have bought which products and when. Carlos believes that there is a pattern to customers\'92 orders before they decide to move to a competitor. This means that you need to turn order data into customer The process flow 51 data. One of the easiest ways to think about this is to picture the data as a table like it might be displayed in Excel. Your order data has a single row for each of the orders. If there are 1,000 orders, there will be 1,000 rows in your table. If these 1,000 orders were from 100 customers, when you turn your order data into customer data, your table with 1,000 rows will become a table with 100 rows. This is shown in step 1 in figure 3.1: transform the orders dataset into a customer dataset. You\'92ll see how to do this in the next section. Let\'92s move on to step 2 now, which is the primary focus of this chapter. In step 2, you answer the question, should Carlos call a customer? After you\'92ve prepared the customer database, you\'92ll use that data to prepare a SageMaker notebook. When the notebook is complete, you\'92ll send data about a customer to the SageMaker endpoint and return a decision about whether Carlos should call that customer. Transform the orders dataset into a customers dataset. Answer the question: \'93Should Carlos call the customer?\'94 Carlos calls the customer. Carlos calls the customer Orders database 1 2 3 Customer dataset for machine learning Call the customer? Don\'92t call Yes No Figure 3.1 Carlos\'92s process flow for deciding which customers to call 52 CHAPTER 3 Should you call a customer because they are at risk of churning? 3.3 Preparing the dataset The base dataset is very simple. It has the customer code, customer name, date of the order, and the value of the order. Carlos has 3,000 customers who, on average, place 3 orders per week. This means that, over the course of the past 3 months, Carlos received 117,000 orders (3,000 customer \'d7 3 orders per week \'d7 13 weeks). NOTE Throughout this book, the datasets you\'92ll use are simplified examples of datasets you might encounter in your work. We have done this to highlight certain machine learning techniques rather than to devote significant parts of each chapter to understanding the data. To turn 117,000 rows into a 3,000-row table (one row per customer), you need to group the non-numerical data and summarize the numerical data. In the dataset shown in table 3.1, the non-numerical fields are customer_code, customer_name, and date. The only numerical field is amount. Grouping customer_code and customer_name is easy. You want a single row per customer_code. And you can simply use the customer name associated with each customer code. In table 3.1, there are two different customer_codes in the rows 393 and 840, and each has a company associated with it: Gibson Group and Meadows, Carroll, and Cunningham. Grouping the dates is the interesting part of the dataset preparation in this chapter. In discussions with Carlos, you learned that he believes there is a pattern to the customers that stop using his bakery. The pattern looks like this: 1 A customer believes they can use a lower quality product without impacting their business. 2 They try another bakery\'92s products. 3 They set up a contract with the other bakery. 4 They stop using Carlos\'92s bakery. Table 3.1 Order dataset for Carlos\'92s customers customer_code customer_name date amount 393 Gibson Group 2018-08-18 264.18 393 Gibson Group 2018-08-17 320.14 393 Gibson Group 2018-08-16 145.95 393 Gibson Group 2018-08-15 280.59 840 Meadows, Carroll, and Cunningham 2018-08-18 284.12 840 Meadows, Carroll, and Cunningham 2018-08-17 232.41 840 Meadows, Carroll, and Cunningham 2018-08-16 235.95 840 Meadows, Carroll, and Cunningham 2018-08-15 184.59 Preparing the dataset 53 Carlos\'92s ordering pattern will be stable over time, then drop while his customers try a competitor\'92s products, and then return to normal while a contract with his competitor is negotiated. Carlos believes that this pattern should be reflected in the customers\'92 ordering behavior. In this chapter, you\'92ll use XGBoost to see if you can identify which customers will stop using Carlos\'92s bakery. Although several tools exist to help you prepare the data, in this chapter, you won\'92t use those tools because the focus of this chapter is on machine learning rather than data preparation. In a subsequent chapter, however, we\'92ll show you how to use these tools with great effect. In this chapter, you\'92ll take Carlos\'92s advice that most of his customers follow a weekly ordering pattern, so you\'92ll summarize the data by week. You\'92ll apply two transformations to the data: 
\f2 \uc0\u61601 
\f0  Normalize the data 
\f2 \uc0\u61601 
\f0  Calculate the change from week to week The first transformation is to calculate the percentage spend, relative to the average week. This normalizes all of the data so that instead of dollars, you are looking at a weekly change relative to the average sales. The second transformation is to show the change from week to week. You do this because you want the machine learning algorithm to see the patterns in the weekly changes as well as the relative figures for the same time period. Note that for this chapter, we\'92ll apply these transformations for you, but later chapters will go more into how to transform data. Because the purpose of this chapter is to learn more about XGBoost and machine learning, we\'92ll tell you what the data looks like so you won\'92t have to do the transformations yourself. 3.3.1 Transformation 1: Normalizing the data For our dataset, we\'92ll do the following: 1 Take the sum of the total spent over the year for each of Carlos\'92s customers and call that total_spend. 2 Find the average spent per week by dividing total_spend by 52. 3 For each week, calculate the total spent per week divided by the average spent per week to get a weekly spend as a percentage of an average spend. 4 Create a column for each week. Table 3.2 shows the results of this transformation. Table 3.2 Customer dataset grouped by week after normalizing the data customer_ code customer_ name total_sales week_ minus_4 week_ minus_3 week_ minus_2 last_week 393 Gibson Group 6013.96 1.13 1.18 0.43 2.09 840 Meadows, Carroll, and Cunningham 5762.40 0.52 1.43 0.87 1.84 54 CHAPTER 3 Should you call a customer because they are at risk of churning? 3.3.2 Transformation 2: Calculating the change from week to week As table 3.3 shows, for each week from the column named week_minus_3 to last_week, you subtract the value from the preceding week and call it the delta between the weeks. For example, in week_minus_3, the Gibson Group has sales that are 1.18 times their average week. In week_minus_4, their sales are 1.13 times their average sales. This means that their weekly sales rose by 0.05 of their normal sales from week_minus_4 to week_minus_3. This is the delta between week_minus_3 and week_minus_4 and is recorded as 0.05 in the 4-3_delta column. The following week was a disaster in sales for the Gibson Group: sales decreased by 0.75 times their average weekly sales. This is shown by a \'960.75 in the 3-2_delta column. Their sales rebounded in the last week though, as they went to 2.09 times their average weekly sales. This is shown by the 1.66 in the 2-1_delta column. Now that you\'92ve prepared the data, let\'92s move on to setting up the machine learning application by first looking at how XGBoost works. 3.4 XGBoost primer In chapter 2, you used XGBoost to help Karen decide which approver to send an order to, but we didn\'92t go into much detail on how it works. We\'92ll cover this now. 3.4.1 How XGBoost works XGBoost can be understood at a number of levels. How deep you go in your understanding depends on your needs. A high-level person will be satisfied with a high-level answer. A more detailed person will require a more detailed understanding. Carlos and Karen will both need to understand the model enough to show their managers they know what\'92s going on. How deep they have to go really depends on their managers. At the highest level, in the circle example from chapter 1 (reproduced in figure 3.2), we separated the dark circles from the light circles using two approaches: 
\f2 \uc0\u61601 
\f0  Rewarding the function for getting a dark circle on the right and punishing it for getting a dark circle on the left. 
\f2 \uc0\u61601 
\f0  Rewarding the function for getting a light circle on the left and punishing it for getting a light circle on the right. Table 3.3 Customer dataset grouped by week, showing changes per week customer_ code customer_ name total_ sales week_ minus_4 week_ minus_3 week_ minus_2 last_ week 4-3_ delta 3-2_ delta 2-1_ delta 393 Gibson Group 6013.96 1.13 1.18 0.43 2.09 0.05 -0.75 1.66 840 Meadows, Carroll, and Cunningham 5762.40 0.52 1.43 0.87 1.84 0.91 -0.56 0.97 XGBoost primer 55 This could be considered an ensemble machine learning model, which is a model that uses multiple approaches when it learns. In a way, XGBoost is also an ensemble machine learning model, which means it uses a number of different approaches to improve the effectiveness of its learning. Let\'92s go another level deeper into the explanation. XGBoost stands for Extreme Gradient Boosting. Consider the name in two parts: 
\f2 \uc0\u61601 
\f0  Gradient boosting 
\f2 \uc0\u61601 
\f0  Extreme Gradient boosting is a technique where different learners are used to improve a function. You can think of this like ice hockey players\'92 sticks handling the puck down the A. Function rewarded for keeping dark circles in the middle and on the right Function also rewarded for keeping white circles on the left Figure 3.2 Machine learning function to identify a group of similar items (reprinted from chapter 1) 56 CHAPTER 3 Should you call a customer because they are at risk of churning? ice. Instead of trying to push the puck straight ahead, they use small corrections to guide the puck in the right direction. Gradient boosting follows a similar approach. The Extreme part of the name is in recognition that XGBoost has a number of other characteristics that makes the model particularly accurate. For example, the model automatically handles regularization of the data so you\'92re not inadvertently thrown off by a dataset with big differences in the values you look at. And finally, for the next level of depth, the sidebar gives Richard\'92s more detailed explanation. Richie\'92s explanation of XGBoost XGBoost is an incredibly powerful machine learning model. To begin with, it supports multiple forms of regularization. This is important because gradient boosting algorithms are known to have potential problems with overfitting. An overfit model is one that is very strongly tied to the unique features of the training data and does not generalize well to unseen data. As we add more rounds to XGBoost, we can see this when our validation accuracy starts deteriorating. Apart from restricting the number of rounds with early stopping, XGBoost also controls overfitting with column and row subsampling and the parameters eta, gamma, lambda, and alpha. This penalizes specific aspects of the model that tend to make it fit the training data too tightly. Another feature is that XGBoost builds each tree in parallel on all available cores. Although each step of gradient boosting needs to be carried out serially, XGBoost\'92s use of all available cores for building each tree gives it a big advantage over other algorithms, particularly when solving more complex problems. XGBoost also supports out-of-core computation. When data does not fit into memory, it divides that data into blocks and stores those on disk in a compressed form. It even supports sharding of these blocks across multiple disks. These blocks are then decompressed on the fly by an independent thread while loading into memory. XGBoost has been extended to support massively parallel processing big data frameworks such as Spark, Flink, and Hadoop. This means it can be used for building extremely large and complex models with potentially billions of rows and millions of features that run at high speed. XGBoost is sparsity aware, meaning that it handles missing values without any requirement for imputation. We have taken this for granted, but many machine learning algorithms require values for all attributes of all samples; in which case, we would have had to impute an appropriate value. This is not always easy to do without skewing the results of the model in some way. Furthermore, XGBoost handles missing values in a very efficient way: its performance is proportional to the number of present values and is independent of the number of missing values. Finally, XGBoost implements a highly efficient algorithm for optimizing the objective known as Newton boosting. Unfortunately, an explanation of this algorithm is beyond the scope of this book. XGBoost primer 57 You can read more about XGBoost on Amazon\'92s site: https://docs.aws.amazon.com/ sagemaker/latest/dg/xgboost.html. 3.4.2 How the machine learning model determines whether the function is getting better or getting worse AUC XGBoost is good at learning. But what does it mean to learn? It simply means that the model gets punished less and rewarded more. And how does the machine learning model know whether it should be punished or rewarded? The area under the curve (AUC) is a metric that is commonly used in machine learning as the basis for rewarding or punishing the function. The curve is the \'93When the function gets a greater area under the curve, it is rewarded\'94 guideline. When the function gets a reduced AUC, it is punished. To get a feel for how AUC works, imagine you are a celebrity in a fancy resort. You are used to being pampered, and you expect that to happen. One of the staff attending to your every whim is the shade umbrella adjuster. Let\'92s call him Function. When Function fails to adjust the umbrella so that you are covered by its shade, you berate him. When he consistently keeps you in the shade, you give him a tip. That is how a machine learning model works: it rewards Function when he increases the AUC and punishes him when he reduces the AUC. Now over to Richie for a more technical explanation. Richie\'92s explanation of the area under the curve (AUC) When we tell XGBoost that our objective is binary:logistic, what we are asking it for is actually not a prediction of a positive or negative label. We are instead asking for the probability of a positive label. As a result, we get a continuous value between 0 and 1. It is then up to us to decide what probability will produce a positive prediction. It might make sense to choose 0.5 (50%) as our cutoff, but at other times, we might want to be really certain of our prediction before predicting a positive. Typically, we would do this when the cost of the decision associated with a positive label is quite high. In other cases, the cost of missing a positive can be more important and justify choosing a cutoff much less than 0.5. The plot in this sidebar\'92s figure shows true positives on the y-axis as a fraction between 0 and 1, and false positives on the x-axis as a fraction between 0 and 1: 
\f2 \uc0\u61601 
\f0  The true positive rate is the portion of all positives that are actually identified as positive by our model. 
\f2 \uc0\u61601 
\f0  The false positive rate is the portion of incorrect positive predictions as a percentage of all negative numbers. This plot is known as an ROC curve. a When we use AUC as our evaluation metric, we are telling XGBoost to optimize our model by maximizing the area under the ROC curve to give us the best possible results when averaged across all cutoff probabilities between 0 and 1. 58 CHAPTER 3 Should you call a customer because they are at risk of churning? 3.5 Getting ready to build the model Now that you have a deeper understanding of how XGBoost works, you can set up another notebook on SageMaker and make some decisions. As you did in chapter 2, you are going to do the following: 
\f2 \uc0\u61601 
\f0  Upload a dataset to S3. 
\f2 \uc0\u61601 
\f0  Set up a notebook on SageMaker. 
\f2 \uc0\u61601 
\f0  Upload the starting notebook. 
\f2 \uc0\u61601 
\f0  Run it against the data. (continued) Whichever value you choose for your cutoff produces both TP (true positive) and corresponding FP (false positive) values. If you choose a probability that allows you to capture most or all of the true positives by picking a low cutoff (such as 0.1), you will also accidentally predict more negatives as positives. Whichever value you pick will be a trade-off between these two competing measures of model accuracy. When the curve is well above the diagonal (as in this figure), you get an AUC value close to 1. A model that simply matches the TP and FP rates for each cutoff will have an AUC of 0.5 and will directly match the diagonal dotted line in the figure. a ROC stands for Receiver Operator Characteristic. It was first developed by engineers during World War II for detecting enemy objects in battle, and the name has stuck. 1.0 Area Under Curve reaches .0 with the dataset used in this chapter 1 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 False Positive Rate True Positive Rate 0.8 1.0 Area Under Curve = 0.97 The area under the curve (AUC) showing true and false positive values Getting ready to build the model 59 Along the way, we\'92ll go into some details we glossed over in chapter 2. TIP If you\'92re jumping into the book at this chapter, you might want to visit the appendixes, which show you how to do the following: 
\f2 \uc0\u61601 
\f0  Appendix A: sign up for AWS, Amazon\'92s web service 
\f2 \uc0\u61601 
\f0  Appendix B: set up S3, AWS\'92s file storage service 
\f2 \uc0\u61601 
\f0  Appendix C: set up SageMaker 3.5.1 Uploading a dataset to S3 To set up the dataset for this chapter, you\'92ll follow the same steps as you did in appendix B. You don\'92t need to set up another bucket though. You can just go to the same bucket you created earlier. In our example, we called the bucket mlforbusiness, but your bucket will be called something different. When you go to your S3 account, you will see something like that shown in figure 3.3. Click this bucket to see the ch02 folder you created in the previous chapter. For this chapter, you\'92ll create a new folder called ch03. You do this by clicking Create Folder and following the prompts to create a new folder. Once you\'92ve created the folder, you are returned to the folder list inside your bucket. There you will see you now have a folder called ch03. Now that you have the ch03 folder set up in your bucket, you can upload your data file and start setting up the decision-making model in SageMaker. To do so, click the folder and download the data file at this link: https://s3.amazonaws.com/mlforbusiness/ch03/churn_data.csv. Then upload the CSV file into your ch03 folder by clicking the Upload button. Now you\'92re ready to set up the notebook instance. Click the bucket where you are storing your datasets. Figure 3.3 Viewing the list of buckets in S3 60 CHAPTER 3 Should you call a customer because they are at risk of churning? 3.5.2 Setting up a notebook on SageMaker Like you did in chapter 2, you\'92ll set up a notebook on SageMaker. This process is much faster for this chapter because, unlike in chapter 2, you now have a notebook instance set up and ready to run. You just need to run it and upload the Jupyter notebook we prepared for this chapter. (If you skipped chapter 2, follow the instructions in appendix C on how to set up SageMaker.) When you go to SageMaker, you\'92ll see your notebook instances. The notebook instance you created for chapter 2 (or that you\'92ve just created by following the instructions in appendix C) will either say Open or Start. If it says Start, click the Start link and wait a couple of minutes for SageMaker to start. Once the screen displays Open Jupyter, select that link to open your notebook list. Once it opens, create a new folder for chapter 3 by clicking New and selecting Folder at the bottom of the dropdown list (figure 3.4). This creates a new folder called Untitled Folder. When you tick the checkbox next to Untitled Folder, you will see the Rename button appear. Click it, and change the folder name to ch03 (figure 3.5). Click the ch03 folder, and you will see an empty notebook list. Just as we already prepared the CSV data you uploaded to S3 (churn_data.csv), we\'92ve already prepared the Jupyter Notebook you\'92ll now use. You can download it to your computer by navigating to this URL: https://s3.amazonaws.com/mlforbusiness/ch03/customer_churn.ipynb. 2. Select Folder. 1. Select New. Figure 3.4 Creating a new folder in SageMaker Building the model 61 Click Upload to upload the customer-churn.ipynb notebook to the ch03 folder (figure 3.6). After uploading the file, you\'92ll see the notebook in your list. Click it to open it. Now, just like in chapter 2, you are a few keystrokes away from being able to run your machine learning model. 3.6 Building the model As in chapter 2, you will go through the code in six parts: 
\f2 \uc0\u61601 
\f0  Load and examine the data. 
\f2 \uc0\u61601 
\f0  Get the data into the right shape. 1. Select checkbox. 2. Select Rename. Figure 3.5 Renaming a folder in SageMaker Click Upload to upload notebook. Figure 3.6 Uploading a notebook to SageMaker 62 CHAPTER 3 Should you call a customer because they are at risk of churning? 
\f2 \uc0\u61601 
\f0  Create training, validation, and test datasets. 
\f2 \uc0\u61601 
\f0  Train the machine learning model. 
\f2 \uc0\u61601 
\f0  Host the machine learning model. 
\f2 \uc0\u61601 
\f0  Test the model and use it to make decisions. 3.6.1 Part 1: Loading and examining the data First, you need to tell SageMaker where your data is. Update the code in the first cell of the notebook to point to your S3 bucket and folders (listing 3.1). If you called your S3 folder ch03 and did not rename the churn_data.csv file, then you just need to update the name of the data bucket to the name of the S3 bucket you uploaded the data to. Once you have done that, you can actually run the entire notebook. As you did in chapter 2, to run the notebook, click Cell in the toolbar at the top of the Jupyter Notebook, then click Run All. data_bucket = 'mlforbusiness' subfolder = 'ch03' dataset = 'churn_data.csv' When you run the notebook, SageMaker loads the data, trains the model, sets up the endpoint, and generates decisions from the test data. SageMaker takes about 10 minutes to complete these actions, so you have time to get yourself a cup of coffee or tea while this is happening. When you return with your hot beverage, if you scroll to the bottom of your notebook, you should see the decisions that were made on the test data. But before we get into that, let\'92s work through the notebook. Back at the top of the notebook, you\'92ll see the cell that imports the Python libraries and modules you\'92ll use in this notebook. You\'92ll hear more about these in a subsequent chapter. For now, let\'92s move to the next cell. If you didn\'92t click Run All in the notebook, click the cell and press C+R to run the code in this cell, as shown in listing 3.1. Moving on to the next cell, you will now import all of the Python libraries and modules that SageMaker uses to prepare the data, train the machine learning model, and set up the endpoint. As you learned in chapter 2, pandas is one of the most commonly used Python libraries in data science. In the code cell shown in listing 3.2, you\'92ll import pandas as pd. When you see pd in the cell, it means you are using a pandas function. Other items that you import include these: 
\f2 \uc0\u61601 
\f0  boto3\'97Amazon\'92s Python library that helps you work with AWS services in Python. 
\f2 \uc0\u61601 
\f0  sagemaker\'97Amazon\'92s Python module to work with SageMaker. Listing 3.1 Setting up the notebook and storing the data S3 bucket where the data is stored Subfolder of the S3 bucket where the data is stored Dataset that\'92s used to train and test the model Building the model 63 
\f2 \uc0\u61601 
\f0  s3fs\'97A module that makes it easier to use boto3 to manage files on S3. 
\f2 \uc0\u61601 
\f0  sklearn.metrics\'97A new import (it wasn\'92t used in chapter 2). This module lets you generate summary reports on the output of the machine learning model. import pandas as pd import boto3 import sagemaker import s3fs from sklearn.model_selection \\ import train_test_split import sklearn.metrics as metrics role = sagemaker.get_execution_role() s3 = s3fs.S3FileSystem(anon=False) In the cell in listing 3.3, we are using the pandas read_csv function to read our data and the head function to display the top five rows. This is one of the first things you\'92ll do in each of the chapters so you can see the data and understand its shape. To load and view the data, click the cell with your mouse to select it, and then press C+R to run the code. df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/\{dataset\}') df.head() You can see that the data has a single customer per row and that it reflects the format of the data in table 3.3. The first column in table 3.4 indicates whether the customer churned or did not churn. If the customer churned, the first column contains a 1. If they remain a customer, it should show a 0. Note that these data rows are provided by way of example, and the rows of data you see might be different. You can see from the first five customers that none of them have churned. This is what you would expect because Carlos doesn\'92t lose that many customers. To see how many rows are in the dataset, you run the pandas shape function as shown in listing 3.4. To see how many customers in the dataset have churned, you run the pandas value_counts function. Listing 3.2 Importing the modules Listing 3.3 Loading and viewing the data Imports the pandas Python library Imports the boto3 AWS library Imports SageMaker Imports the s3fs module to make working with S3 files easier Imports only the train_test_split module from the sklearn library Imports the metrics module from the sklearn library Creates a role in SageMaker Establishes the connection with S3 Reads the S3 churn_data.csv dataset in listing 3.1 Displays the top five rows of the DataFrame 64 CHAPTER 3 Should you call a customer because they are at risk of churning? print(f'Number of rows in dataset: \{df.shape[0]\}') print(df['churned'].value_counts()) You can see from this data that out of 2,999 rows of data, 166 customers have churned. This represents a churn rate of about 5% per week, which is higher than the rate that Carlos experiences. Carlos\'92s true churn rate is about 0.5% per week (or about 15 customers per week). We did something a little sneaky with the data in this instance to bring the churn rate up to this level. The dataset actually contains the churned customers from the past three months and a random selection of non-churned customers over that same period to bring the total number of customers up to 2,999 (the actual number of customers that Carlos has). We did this because we are going to cover how to handle extremely rare events in a subsequent chapter and, for this chapter, we wanted to use a similar toolset to that which we used in chapter 2. There are risks in the approach we took with the data in this chapter. If there are differences in the ordering patterns of churned customers over the past three months, then our results might be invalid. In discussions with Carlos, he believes that the pattern of churning and normal customers remains steady over time, so we felt confident we can use this approach. The other point to note is that this approach might not be well received if we were writing an academic paper. One of the lessons you\'92ll learn as you work with your own company\'92s data is that you rarely get everything you want. You have to constantly assess whether you can make good decisions based on the data you have. Table 3.4 Dataset for Carlos\'92s customers displayed in Excel churned id customer_ code co_name total_ spend week_ minus_4 week_ minus_3 week_ minus_2 last_ week 4-3_ delta 3-2_ delta 2-1_ delta 0 1 1826 Hoffman, Martinez, and Chandler 68567.34 0.81 0.02 0.74 1.45 0.79 -0.72 -0.71 0 2 772 Lee Martin and Escobar 74335.27 1.87 1.02 1.29 1.19 0.85 -0.27 0.10 Listing 3.4 Number of churned customers in the dataset Displays the total number of rows Displays the number of rows where a customer churned and the number of rows where the customer did not Building the model 65 3.6.2 Part 2: Getting the data into the right shape Now that you can see your dataset in the notebook, you can start working with it. XGBoost can only work with numbers, so we need to either remove our categorical data or encode it. Encoding data means that you set each distinct value in the dataset as a column and then put a 1 in the rows that contain the value of the column and a zero in the other rows in that column. This worked well for your products in Karen\'92s dataset, but it will not help you out with Carlos\'92s dataset. That\'92s because the categorical data (customer_name, customer_code, and id) are unique\'97these occur only once in the dataset. Turning these into columns would not improve the model either. Your best approach in this case is also the simplest approach: just remove the categorical data. To remove the data, use the pandas drop function, and display the first five rows of the dataset again by using the head function. You use axis=1 to indicate that you want to remove columns rather than rows in the pandas DataFrame. encoded_data = df.drop( ['id', 'customer_code', 'co_name'], axis=1) encoded_data.head() Removing the columns shows the dataset without the categorical information (table 3.5). 3.6.3 Part 3: Creating training, validation, and test datasets Now that you have your data in a format that XGBoost can work with, you can split the data into test, validation, and training datasets as you did in chapter 2. One important difference with the approach we are taking going forward is that we are using the stratify parameter during the split. The stratify parameter is particularly useful for datasets where the target variable you are predicting is relatively rare. The parameter works by shuffling the deck as it\'92s building the machine learning model and making sure that the train, validate, and test datasets contain similar ratios of target variables. This ensures that the model does not get thrown off course by an unrepresentative selection of customers in any of the datasets. Listing 3.5 Removing the categorical data Table 3.5 The transformed dataset without categorical information churned total_ spend week_ minus_4 week_ minus_3 week_ minus_2 last_ week 4-3_delta 3-2_delta 2-1_delta 0 68567.34 0.81 0.02 0.74 1.45 0.79 -0.72 -0.71 0 74335.27 1.87 1.02 1.29 1.19 0.85 -0.27 0.10 Removes categorical columns by calling the drop function on the df DataFrame Displays the first five rows of the DataFrame 66 CHAPTER 3 Should you call a customer because they are at risk of churning? We glossed over this code in chapter 2. We\'92ll go into more depth here and show you how to use stratify (listing 3.6). You create training and testing samples from a dataset, with 70% allocated to the training data and 30% allocated to the testing and validation samples. The stratify argument tells the function to use y to stratify the data so that a random sample is balanced proportionally according to the distinct values in y. You might notice that the code to split the dataset is slightly different than the code used in chapter 2. Because you are using stratify, you have to explicitly declare your target column (churned in this example). The stratify function returns a couple of additional values that you don\'92t care about. The underscores in the y = test_and_val_data line (those beginning with val_df) are simply placeholders for variables. Don\'92t worry if this seems a bit arcane. You don\'92t need to understand this part of the code in order to train, validate, and test the model. You then split the testing and validation data, with two thirds allocated to validation and one third to testing (listing 3.6). Over the entire dataset, 70% of the data is allocated to training, 20% to validation, and 10% to testing. y = encoded_data['churned'] train_df, test_and_val_data, _, _ = train_test_split( encoded_data, y, test_size=0.3, stratify=y, random_state=0) y = test_and_val_data['churned'] val_df, test_df, _, _ = train_test_split( testing_data, y, test_size=0.333, stratify=y, random_state=0) print(train_df.shape, val_df.shape, test_df.shape) print() print('Train') print(train_df['churned'].value_counts()) print() print('Validate') print(val_df['churned'].value_counts()) print() print('Test') print(test_df['churned'].value_counts()) Just as you did in chapter 2, you convert the three datasets to CSV and save the data to S3. The following listing creates the datasets that you\'92ll save to the same S3 folder as your original churn_data.csv file. Listing 3.6 Creating the training, validation, and test datasets Sets the target variable for use in splitting the data Creates training and testing samples from dataset df. random_state and ensures that repeating the command splits the data in the same way Splits testing and validation data into a validation dataset and a testing dataset The value_counts function shows the number of customers who did not churn (denoted by a 0) and the number of churned customers (denoted by a 1) in the train, validate, and test datasets. Building the model 67 train_data = train_df.to_csv(None, header=False, index=False).encode() val_data = val_df.to_csv(None, header=False, index=False).encode() test_data = test_df.to_csv(None, header=True, index=False).encode() with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/train.csv', 'wb') as f: f.write(train_data) with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/val.csv', 'wb') as f: f.write(val_data) with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/test.csv', 'wb') as f: f.write(test_data) train_input = sagemaker.s3_input( s3_data=f's3://\{data_bucket\}/\{subfolder\}/processed/train.csv', content_type='csv') val_input = sagemaker.s3_input( s3_data=f's3://\{data_bucket\}/\{subfolder\}/processed/val.csv', content_type='csv') Figure 3.7 shows the datasets you now have in S3. 3.6.4 Part 4: Training the model Now you train the model. In chapter 2, we didn\'92t go into much detail about what is happening with the training. Now that you have a better understanding of XGBoost, we\'92ll explain the process a bit more. Listing 3.7 Converting the datasets to CSV and saving to S3 Writes the train.csv file to S3 Writes the val.csv file to S3 Writes the test.csv file to S3 The three files you just created in S3 Figure 3.7 CSV file listing for the S3 folder 68 CHAPTER 3 Should you call a customer because they are at risk of churning? The interesting parts of the following listing (listing 3.8) are the estimator hyperparameters. We\'92ll discuss max_depth and subsample in a later chapter. For now, the hyperparameters of interest to us are 
\f2 \uc0\u61601 
\f0  objective\'97As in chapter 2, you set this hyperparameter to binary:logistic. You use this setting when your target variable is 1 or 0. If your target variable is a multiclass variable or a continuous variable, then you use other settings, as we\'92ll discuss in later chapters. 
\f2 \uc0\u61601 
\f0  eval_metric\'97The evaluation metric you are optimizing for. The metric argument auc stands for area under the curve, as discussed by Richie earlier in the chapter. 
\f2 \uc0\u61601 
\f0  num_round\'97How many times you want to let the machine learning model run through the training data (the number of rounds). With each loop through the data, the function gets better at separating the dark circles from the light circles, for example (to refer back to the explanation of machine learning in chapter 1). After a while though, the model gets too good; it begins to find patterns in the test data that are not reflected in the real world. This is called overfitting. The larger the number of rounds, the more likely you are to be overfitting. To avoid this, you set early stopping rounds. 
\f2 \uc0\u61601 
\f0  early_stopping_rounds\'97The number of rounds where the algorithm fails to improve. 
\f2 \uc0\u61601 
\f0  scale_pos_weight\'97The scale positive weight is used with imbalanced datasets to make sure the model puts enough emphasis on correctly predicting rare classes during training. In the current dataset, about 1 in 17 customers will churn. So we set scale_pos_weight to 17 to accommodate for this imbalance. This tells XGBoost to focus more on customers who actually churn rather than on happy customers who are still happy. NOTE If you have the time and interest, try retraining your model without setting scale_pos_weight and see what effect this has on your results. sess = sagemaker.Session() container = sagemaker.amazon.amazon_estimator.get_image_uri( boto3.Session().region_name, 'xgboost', 'latest') estimator = sagemaker.estimator.Estimator( container, role, train_instance_count=1, train_instance_type='ml.m5.large', output_path= \\ Listing 3.8 Training the model Sets the type of server that SageMaker uses to run your model Building the model 69 f's3://\{data_bucket\}/\{subfolder\}/output', sagemaker_session=sess) estimator.set_hyperparameters( max_depth=3, subsample=0.7, objective='binary:logistic', eval_metric='auc', num_round=100, early_stopping_rounds=10, scale_pos_weight=17) estimator.fit(\{'train': train_input, 'validation': val_input\}) When you ran this cell in the current chapter (and in chapter 2), you saw a number of rows of red notifications pop up in the notebook. We passed over this without comment, but these actually contain some interesting information. In particular, you can see if the model is overfitting by looking at this data. When you run the training, the model does a couple of things in each round of training. First, it trains, and second, it validates. The red notifications that you see are the result of that validation process. As you read through the notifications, you can see that the validation score improves for the first 48 rounds and then starts getting worse. What you are seeing is overfitting. The algorithm is improving at building a function that separates the dark circles from the light circles in the training set (as in chapter 1), but it is getting worse at doing it in the validation dataset. This means the model is starting to pick up patterns in the test data that do not exist in the real world (or at least in our validation dataset). One of the great features in XGBoost is that it deftly handles overfitting for you. The early_stopping_rounds hyperparameter in listing 3.8 stops the training when there\'92s no improvement in the past 10 rounds. The output shown in listing 3.9 is taken from the output of the Train the Model cell in the notebook. You can see that round 15 had an AUC of 0.976057 and that round 16 had an AUC of 0.975683, and that neither of these is better than the previous best of 0.980493 from round 6. Because we set early_stopping_rounds=10, the training stops at round 16, which is 10 rounds past the best result in round 6. Richie\'92s explanation of overfitting We touched on overfitting earlier in the XGBoost explanation. Overfitting is the process of building a model that maps too closely or exactly to the provided training data and fails to predict unseen data as accurately or reliably. This is also sometimes known as a model that does not generalize well. Unseen data includes test data, validation data, and data that can be provided to our endpoint in production. Stores the output of the model at this location in S3 Binary logistic objective hyperparameter Area under the curve (AUC) evaluation metric hyperparameter Number of rounds hyperparameter Early stopping rounds hyperparameter Scale positive weight hyperparameter 70 CHAPTER 3 Should you call a customer because they are at risk of churning? [15]#011train-auc:0.98571#011validation-auc:0.976057 [16]#011train-auc:0.986562#011validation-auc:0.975683 Stopping. Best iteration: [6]#011train-auc:0.97752#011validation-auc:0.980493 3.6.5 Part 5: Hosting the model Now that you have a trained model, you can host it on SageMaker so it is ready to make decisions (listing 3.10). We\'92ve covered a lot of ground in this chapter, so we\'92ll delve into how the hosting works in a subsequent chapter. For now, just know that it is setting up a server that receives data and returns decisions. endpoint_name = 'customer-churn' try: sess.delete_endpoint( sagemaker.predictor.RealTimePredictor( endpoint=endpoint_name).endpoint) print( 'Warning: Existing endpoint deleted to make way for new endpoint.') except: pass predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium', endpoint_name=endpoint_name) from sagemaker.predictor import csv_serializer, json_serializer predictor.content_type = 'text/csv' predictor.serializer = csv_serializer predictor.deserializer = None 3.6.6 Part 6: Testing the model Now that the endpoint is set up and hosted, you can start making decisions. Start by running your test data through the system to see how the model works on data it hasn\'92t seen before. The first three lines in listing 3.11 create a function that returns 1 if the customer is more likely to churn and 0 if they are less likely to churn. The next two lines open the test CSV file you created in listing 3.7. And the last two lines apply the get_prediction function to every row in the test dataset to display the data. def get_prediction(row): prob = float(predictor.predict(row[1:]).decode('utf-8')) return 1 if prob > 0.5 else 0 Listing 3.9 Training rounds output Listing 3.10 Hosting the model Listing 3.11 Making predictions using the test data Indicates the server type (in this case, an ml.t2.medium server) Returns a value between 0 and 1 Building the model 71 with s3.open(f'\{data_bucket\}/\{subfolder\}/processed/test.csv') as f: test_data = pd.read_csv(f) test_data['decison'] = test_data.apply(get_prediction, axis=1) test_data.set_index('decision', inplace=True) test_data[:10] In your results, you want to only show a 1 or 0. If the prediction is greater than 0.5 (if prob > 0.5), get_prediction sets the prediction as 1. Otherwise, it sets the prediction as 0. The results look pretty good (table 3.6). Every row that has a 1 in the churn column also has a 1 in the decision column. There are some rows with a 1 in the decision column and a 0 in the churn column, which means Carlos is going to call them even though they are not at risk of churning. But this is acceptable to Carlos. Far better to call a customer that\'92s not going to churn than to not call a customer that will churn. To see how the model performs overall, you can look at how many customers churned in the test dataset compared to how many Carlos would have called. To do this, you use the value_counts function as shown in the next listing. print(test_data['churned'].value_counts()) print(test_data['prediction'].value_counts()) print( metrics.accuracy_score( test_data['churned'], test_data['prediction'])) The value_counts function shows that Carlos would have called 33 customers and that, if he did nothing, 17 would have churned. But this isn\'92t very helpful for two reasons: 
\f2 \uc0\u61601 
\f0  What this tells us is that 94.67% of our predictions are correct, but that\'92s not as good as it sounds because only about 6% of Carlos\'92s customers churned. If we were to guess that none of our customers churned, we would be 94% accurate. 
\f2 \uc0\u61601 
\f0  It doesn\'92t tell us how many of those he called would have churned. Table 3.6 Results of the test decision churned total_ spend week_ minus_4 week_ minus_3 week_ minus_2 last_ week 4-3_ delta 3-2_ delta 2-1_ delta 0 0 17175.67 1.47 0.61 1.86 1.53 0.86 \'961.25 0.33 0 0 68881.33 0.82 2.26 1.59 1.72 \'961.44 0.67 -0.13 \'85\'85\'85 \'85\'85\'85\'85\'85\'85\'85 1 1 71528.99 2.48 1.36 0.09 1.24 1.12 1.27 \'961.15 Listing 3.12 Checking the predictions made using the test data Counts the number of customers who churned Counts the number of customers who did not churn Calculates the accuracy of the prediction 72 CHAPTER 3 Should you call a customer because they are at risk of churning? For this, you need to create a confusion matrix: 0 283 1 17 Name: churned, dtype: int64 0 267 1 33 Name: prediction, dtype: int64 0.94.67 A confusion matrix is one of the most confusingly named terms in machine learning. But, because it is also one of the most helpful tools in understanding the performance of a model, we\'92ll cover it here. Although the term is confusing, creating a confusion matrix is easy. You use an sklearn function as shown in the next listing. print( metrics.confusion_matrix( test_data['churned'], test_data['prediction'])) A confusion matrix is a table containing an equal number of rows and columns. The number of rows and columns corresponds to the number of possible values (classes) for the target variable. In Carlos\'92s dataset, the target variable could be a 0 or a 1, so the confusion matrix has two rows and two columns. In a more general sense though, the rows of the matrix represent the actual class, and the columns represent the predicted class. (Note: Wikipedia currently has rows and columns reversed to this explanation; however, our description is the way the sklearn.confusion_matrix function works.) In the following output, the first row represents happy customers (0) and the second row represents churns (1). The left column shows predicted happy customers, and the right column displays predicted churns. For Carlos, the right column also shows how many customers he called. You can see that Carlos called 16 customers who did not churn and 17 customers who did churn. [[267 16] [ 0 17]] Importantly, the 0 at the bottom left shows how many customers who churned were predicted not to churn and that he did not call. To Carlos\'92s great satisfaction, that number is 0. Listing 3.13 Creating the confusion matrix Creates the confusion matrix Deleting the endpoint and shutting down your notebook instance 73 model_path = f'\{estimator.output_path\}/\\ \{estimator._current_job_name\}/output/model.tar.gz' s3.get(model_path, 'xgb_tar.gz') with tarfile.open('xgb_tar.gz') as tar: with tar.extractfile('xgboost-model') as m:S xgb_model = pickle.load(m) xgb_scores = xgb_model.get_score() print(xgb_scores) Note that we do not include this code in the notebook as it is beyond the scope of what we want to cover here. But for those of you who want to dive deeper, you can do so using this code, or for more details, see the XGBoost documentation at https://xgboost.readthedocs.io/en/latest/python/python_api.html. 3.7 Deleting the endpoint and shutting down your notebook instance It is important that you shut down your notebook instance and delete your endpoint. We don\'92t want you to get charged for SageMaker services that you\'92re not using. 3.7.1 Deleting the endpoint Appendix D describes how to shut down your notebook instance and delete your endpoint using the SageMaker console, or you can do that with the code in the next listing. Richie\'92s note on interpretable machine learning Throughout this book, we focus on providing examples of business problems that can be solved by machine learning using one of several algorithms. We also attempt to explain in high-level terms how these algorithms work. Generally, we use fairly simple metrics, such as accuracy, to indicate whether a model is working or not. But what if you were asked to explain why your model worked? Which of your features really matter most in determining whether the model works, and why? For example, is the model biased in ways that can harm minority groups in your customer base or workforce? Questions like this are becoming increasingly prevalent, particularly due to the widespread use of neural networks, which are particularly opaque. One advantage of XGBoost over neural networks (that we have not previously touched on) is that XGBoost supports the examination of feature importances to help address the explainability issue. At the time of writing, Amazon does not support this directly in the SageMaker XGBoost API; however, the model is stored on S3 as an archive named model.tar.gz. By accessing this file, we can view feature importances. The following listing provides sample code on how to do this. Listing 3.14 Sample code used to access SageMaker\'92s XGBoost model.tar.gz 74 CHAPTER 3 Should you call a customer because they are at risk of churning? # Remove the endpoint (optional) # Comment out this cell if you want the endpoint to persist after Run All sess.delete_endpoint(text_classifier.endpoint) To delete the endpoint, uncomment the code in the listing, then click C+R to run the code in the cell. 3.7.2 Shutting down the notebook instance To shut down the notebook, go back to your browser tab where you have SageMaker open. Click the Notebook Instances menu item to view all of your notebook instances. Select the radio button next to the notebook instance name, as shown in figure 3.8, then click Stop on the Actions menu. It will take a couple of minutes to shut down. 3.8 Checking to make sure the endpoint is deleted If you didn\'92t delete the endpoint using the notebook (or if you just want to make sure it\'92s deleted), you can do this from the SageMaker console. To delete the endpoint, click the radio button to the left of the endpoint name, then click the Actions menu item and click Delete in the menu that appears. When you have successfully deleted the endpoint, you will no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you see the text \'93There are currently no resources\'94 displayed at the bottom of the Endpoints page (figure 3.9). Listing 3.15 Deleting the notebook 1. Select the radio button. 2. Select Stop. Figure 3.8 Shutting down the notebook Summary 75 Summary 
\f2 \uc0\u61601 
\f0  You created a machine learning model to determine which customers to call because they are at risk of taking their business to a competitor. 
\f2 \uc0\u61601 
\f0  XGBoost is a gradient-boosting, machine learning model that uses an ensemble of different approaches to improve the effectiveness of its learning. 
\f2 \uc0\u61601 
\f0  Stratify is one technique to help you handle imbalanced datasets. It shuffles the deck as it builds the machine learning model, making sure that the train, validate, and test datasets contain similar ratios of target variables. 
\f2 \uc0\u61601 
\f0  A confusion matrix is one of the most confusingly named terms in machine learning, but it is also one of the most helpful tools in understanding the performance of a model. Endpoint successfully deleted Figure 3.9 Verifying that you have successfully deleted the endpoint 76 Should an incident be escalated to your support team? Naomi heads up an IT team that handles customer support tickets for a number of companies. A customer sends a tweet to a Twitter account, and Naomi\'92s team replies with a resolution or a request for further information. A large percentage of the tweets can be handled by sending links to information that helps customers resolve their issues. But about a quarter of the responses are to people who need more help than that. They need to feel they\'92ve been heard and tend to get very cranky if they don\'92t. These are the customers that, with the right intervention, become the strongest advocates; with the wrong intervention, they become the loudest detractors. Naomi wants to know who these customers are as early as possible so that her support team can intervene in the right way. This chapter covers 
\f2 \uc0\u61601 
\f0  An overview of natural language processing (NLP) 
\f2 \uc0\u61601 
\f0  How to approach an NLP machine learning scenario 
\f2 \uc0\u61601 
\f0  How to prepare data for an NLP scenario 
\f2 \uc0\u61601 
\f0  SageMaker\'92s text analytics engine, BlazingText 
\f2 \uc0\u61601 
\f0  How to interpret BlazingText results The process flow 77 She and her team have spent the past few years automating responses to the most common queries and manually escalating the queries that must be handled by a person. Naomi wants to build a triage system that reviews each request as it comes in to determine whether the response should be automatic or should be handed off to a person. Fortunately for Naomi, she has a couple of years of historical tweets that her team has reviewed and decided whether they can be handled automatically or should be handled by a person. In this chapter, you\'92ll take Naomi\'92s historical data and use it to decide whether new tweets should be handled automatically or escalated to one of Naomi\'92s team members. 4.1 What are you making decisions about? As always, the first thing you want to look at is what you\'92re making decisions about. In this chapter, the decision Naomi is making is, should this tweet be escalated to a person? The approach that Naomi\'92s team has taken over the past few years is to escalate tweets in which the customer appears frustrated. Her team did not apply any hard and fast rules when making this decision. They just had a feeling that the customer was frustrated so they escalated the tweet. In this chapter, you are going to build a machine learning model that learns how to identify frustration based on the tweets that Naomi\'92s team has previously escalated. 4.2 The process flow The process flow for this decision is shown in figure 4.1. It starts when a customer sends a tweet to the company\'92s support account. Naomi\'92s team reviews the tweet to Support person reviews tweet to determine whether tweet should be handled by automated system or a person Tweet handled by automated response Tweet handled by member of the support staff Tweeter Support triage Automated response 1 2 3 Support person responds Figure 4.1 Tweet response workflow for Naomi\'92s customer support tickets 78 CHAPTER 4 Should an incident be escalated to your support team? determine whether they need to respond personally or whether it can be handled by their bot. The final step is a tweet response from either the bot or Naomi\'92s team. Naomi wants to replace the determination at step 1 of figure 4.1 with a machine learning application that can make a decision based on how frustrated the incoming tweet seems. This chapter shows you how to prepare this application. 4.3 Preparing the dataset In the previous two chapters, you prepared a synthetic dataset from scratch. For this chapter, you are going to take a dataset of tweets sent to tech companies in 2017. The dataset is published by a company called Kaggle, which runs machine learning competitions. To determine what data is required to solve a particular problem, you need to focus on the objective you are pursuing and, in this case, think about the minimum data Naomi needs to achieve her objective. Once you have that, you can decide whether you can achieve her objective using only that data supplied, or you need to expand the data to allow Naomi to better achieve her objective. As a reminder, Naomi\'92s objective is to identify the tweets that should be handled by a person, based on her team\'92s past history of escalating tweets. So Naomi\'92s dataset should contain an incoming tweet and a flag indicating whether it was escalated or not. The dataset we use in this chapter is based on a dataset uploaded to Kaggle by Stuart Axelbrooke from Thought Vector. (The original dataset can be viewed at https:// www.kaggle.com/thoughtvector/customer-support-on-twitter/.) This dataset contains over 3 million tweets sent to customer support departments for several companies ranging from Apple and Amazon to British Airways and Southwest Air. Like every dataset you\'92ll find in your company, you can\'92t just use this data as is. It needs to be formatted in a way that allows your machine learning algorithm do its thing. The original dataset on Kaggle contains both the original tweet and the response. In the scenario in this chapter, only the original tweet is relevant. To prepare the data for this chapter, we removed all the tweets except the original tweet and used the responses to label the original tweet as escalated or not escalated. The resulting dataset contains a tweet with that label and these columns: Kaggle, competition, and public datasets Kaggle is a fascinating company. Founded in 2010, Kaggle gamifies machine learning by pitting teams of data scientists against each other to solve machine learning problems for prize money. In mid-2017, shortly before being acquired by Google, Kaggle announced it had reached a milestone of one million registered competitors. Even if you have no intention of competing in data science competitions, Kaggle is a good resource to become familiar with because it has public datasets that you can use in your machine learning training and work. NLP (natural language processing) 79 
\f2 \uc0\u61601 
\f0  tweet_id\'97Uniquely identifies the tweet 
\f2 \uc0\u61601 
\f0  author_id\'97Uniquely identifies the author 
\f2 \uc0\u61601 
\f0  created_at\'97Shows the time of the tweet 
\f2 \uc0\u61601 
\f0  in_reply_to\'97Shows which company is being contacted 
\f2 \uc0\u61601 
\f0  text\'97Contains the text in the tweet 
\f2 \uc0\u61601 
\f0  escalate\'97Indicates whether the tweet was escalated or not Table 4.1 shows the first three tweets in the dataset. Each of the tweets is to Sprint Care, the support team for the US phone company, Sprint. You can see that the first tweet (\'93and how do you propose we do that\'94) was not escalated by Naomi\'92s team. But the second tweet (\'93I have sent several private messages and no one is responding as usual\'94) was escalated. Naomi\'92s team let their automated response system handle the first tweet but escalated the second tweet to a member of her team for a personal response. In this chapter, you\'92ll build a machine learning application to handle the task of whether to escalate the tweet. But this application will be a little different than the machine learning applications you built in previous chapters. In order to decide which tweets should be escalated, the machine learning application needs to know something about language and meaning, which you might think is pretty difficult to do. Fortunately, some very smart people have been working on this problem for a while. They call it natural language processing, or NLP. 4.4 NLP (natural language processing) The goal of NLP is to be able to use computers to work with language as effectively as computers can work with numbers or variables. This is a hard problem because of the richness of language. (The previous sentence is a good example of the difficulty of this problem.) The term rich means something slightly different when referring to language than it does when referring to a person. And the sentence \'93Well, that\'92s rich!\'94 can mean the opposite of how rich is used in other contexts. Scientists have worked on NLP since the advent of computing, but it has only been recently that they have made significant strides in this area. NLP originally focused on Table 4.1 Tweet dataset tweet_id author_id created_at in_reply_to text escalate 2 115712 Tue Oct 31 22:11 2017 sprintcare @sprintcare and how do you propose we do that False 3 115712 Tue Oct 31 22:08 2017 sprintcare @sprintcare I have sent several private messages and no one is responding as usual True 5 115712 Tue Oct 31 21:49 2017 sprintcare @sprintcare I did. False 80 CHAPTER 4 Should an incident be escalated to your support team? getting computers to understand the structure of each language. In English, a typical sentence has a subject, verb, and an object, such as this sentence: \'93Sam throws the ball\'94; whereas in Japanese, a sentence typically follows a subject, object, verb pattern. But the success of this approach was hampered by the mind-boggling number and variety of exceptions and slowed by the necessity to individually describe each different language. The same code you use for English NLP won\'92t work for Japanese NLP. The big breakthrough in NLP occurred in 2013 when NIPS published a paper on word vectors.1 With this approach, you don\'92t look at parts of the language at all! You just apply a mathematical algorithm to a bunch of text and work with the output of the algorithm. This has two advantages: 
\f2 \uc0\u61601 
\f0  It naturally handles exceptions and inconsistencies in a language. 
\f2 \uc0\u61601 
\f0  It is language-agnostic and can work with Japanese text as easily as it can work with English text. In SageMaker, working with word vectors is as easy as working with the data you worked with in chapters 2 and 3. But there are a few decisions you need to make when configuring SageMaker that require you to have some appreciation of what is happening under the hood. 4.4.1 Creating word vectors Just as you used the pandas function get_dummies in chapter 2 to convert categorical data (such as desk, keyboard, and mouse) to a wide dataset, the first step in creating a word vector is to convert all the words in your text into wide datasets. As an example, the word queen is represented by the dataset 0,1,0,0,0, as shown in figure 4.2. The word queen has 1 under it, while every other word in the row has a 0. This can be described as a single dimensional vector. Using a single dimensional vector, you test for equality and nothing else. That is, you can determine whether the vector is equal to the word queen, and in figure 4.2, you can see that it is. 1 See \'93Distributed Representations of Words and Phrases and their Compositionality\'94 by Tomas Mikolov et al. at https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-theircompositionality.pdf. King Queen Man Woman Princess Figure 4.2 Single dimensional vector testing for equality NLP (natural language processing) 81 Mikolov\'92s breakthrough was the realization that meaning can be captured by a multidimensional vector with the representation of each word distributed across each dimension. Figure 4.3 shows conceptually how dimensions look in a vector. Each dimension can be thought of as a group of related words. In Mikolov\'92s algorithm, these groups of related words don\'92t have labels, but to show how meaning can emerge from multidimensional vectors, we have provided four labels on the left side of the figure: Royalty, Masculinity, Femininity, and Elderliness. Looking at the first dimension in figure 4.3, Royalty, you can see that the values in the King, Queen, and Princess columns are higher than the values in the Man and Woman columns; whereas, for Masculinity, the values in the King and Man columns are higher than in the others. From this you start to get the picture that a King is Masculine Royalty whereas a Queen is non-masculine Royalty. If you can imagine working your way through hundreds of vectors, you can see how meaning emerges. Back to Naomi\'92s problem, as each tweet comes in, the application breaks the tweet down into multidimensional vectors and compares it to the tweets labeled by Naomi\'92s team. The application identifies which tweets in the training dataset have similar vectors. It then looks at the label of the trained tweets and assigns that label to the incoming tweet. For example, if an incoming tweet has the phrase \'93no one is responding as usual,\'94 the tweets in the training data with similar vectors would likely have been escalated, and so the incoming tweet would be escalated as well. The magic of the mathematics behind word vectors is that it groups the words being defined. Each of these groups is a dimension in the vector. For example, in the tweet where the tweeter says \'93no one is responding as usual,\'94 the words as usual might King Queen Man Woman Princess Royalty Masculinity Femininity Elderliness Figure 4.3 Multidimensional vectors capture meaning in languages. 82 CHAPTER 4 Should an incident be escalated to your support team? be grouped into a dimension with other pairs of words such as of course, yeah obviously, and a doy, which indicate frustration. The King/Queen, Man/Woman example is used regularly in the explanation of word vectors. Adrian Colyer\'92s excellent blog, \'93the morning paper,\'94 discusses word vectors in more detail at https://blog.acolyer.org/?s=the+amazing+power+of+word+vectors. Figures 4.2 and 4.3 are based on figures from the first part of this article. If you are interested in exploring this topic further, the rest of Adrian\'92s article is a good place to start. 4.4.2 Deciding how many words to include in each group In order to work with vectors in SageMaker, the only decision you need to make is whether SageMaker should use single words, pairs of words, or word triplets when creating the groups. For example, if SageMaker uses the word pair as usual, it can get better results than if it uses the single word as and the single word usual, because the word pair expresses a different concept than do the individual words. In our work, we normally use word pairs but have occasionally gotten better results from triplets. In one project where we were extracting and categorizing marketing terms, using triplets resulted in much higher accuracy, probably because marketing fluff is often expressed in word triplets such as world class results, high powered engine, and fat burning machine. NLP uses the terms unigram, bigram, and trigram for single-, double-, and tripleword groups. Figures 4.4, 4.5, and 4.6 show examples of single-word (unigram), doubleword (bigram), and triple-word (trigram) word groups, respectively. As figure 4.4 shows, unigrams are single words. Unigrams work well when word order is not important. For example, if you were creating word vectors for medical research, unigrams do a good job of identifying similar concepts. As figure 4.5 shows, bigrams are pairs of words. Bigrams work well when word order is important, such as in sentiment analysis. The bigram as usual conveys frustration, but the unigrams as and usual do not. As figure 4.6 shows, trigrams are groups of three words. In practice, we don\'92t see much improvement in moving from bigrams to trigrams, but on occasion there can be. One project we worked on to identify marketing terms delivered significantly better results using trigrams, probably because the trigrams better captured the common Unigrams 1. 2. 3. 4. 5. 6. Figure 4.4 NLP defines single words as unigrams. What is BlazingText and how does it work? 83 pattern hyperbole noun noun (as in greatest coffee maker) and the pattern hyperbole adjective noun (as in fastest diesel car). In our case study, the machine learning application will use an algorithm called BlazingText. This predicts whether a tweet should be escalated. 4.5 What is BlazingText and how does it work? BlazingText is a version of an algorithm, called fastText, developed by researchers at Facebook in 2017. And fastText is a version of the algorithm developed by Google\'92s own Mikolov and others. Figure 4.7 shows the workflow after BlazingText is put into use. In step 1, a tweet is sent by a person requiring support. In step 2, BlazingText decides whether the tweet should be escalated to a person for a response. In step 3, the tweet is escalated to a person (step 3a) or handled by a bot (step 3b). In order for BlazingText to decide whether a tweet should be escalated, it needs to determine whether the person sending the tweet is feeling frustrated or not. To do this, BlazingText doesn\'92t actually need to know whether the person is feeling frustrated 1. Bigrams 2. 3. 4. 5. Figure 4.5 Pairs of words are known as bigrams. 1. 2. 3. 4. Trigrams Figure 4.6 Words grouped into threes are trigrams. 84 CHAPTER 4 Should an incident be escalated to your support team? or even understand what the tweet was about. It just needs to determine how similar the tweet is to other tweets that have been labeled as frustrated or not frustrated. With that as background, you are ready to start building the model. If you like, you can read more about BlazingText on Amazon\'92s site at https://docs.aws.amazon.com/sagemaker/ latest/dg/blazingtext.html. 4.6 Getting ready to build the model Now that you have a deeper understanding of how BlazingText works, you\'92ll set up another notebook in SageMaker and make some decisions. You are going to do the following (as you did in chapters 2 and 3): 1 Upload a dataset to S3 2 Set up a notebook on SageMaker Refresher on how SageMaker is structured Now that you\'92re getting comfortable with using Jupyter Notebook, it is a good time to review how SageMaker is structured. When you first set up SageMaker, you created a notebook instance, which is a server that AWS configures to run your notebooks. In appendix C, we instructed you to select a medium-sized server instance because it has enough grunt to do anything we cover in this book. As you work with larger datasets in your own work, you might need to use a larger server. When you run your notebook for the case studies in this book, AWS creates two additional servers. The first is a temporary server that is used to train the machine learning model. The second server AWS creates is the endpoint server. This server stays up until you remove the endpoint. To delete the endpoint in SageMaker, click the radio button to the left of the endpoint name, then click the Actions menu item, and click Delete in the menu that appears. 1. Tweet sent by customer requiring support 3a. Escalated tweet sent to person to respond 3b. Tweet that does not require escalation is sent to a bot to respond 2. BlazingText decides whether tweet should be escalated or not 3a. Escalated tweet sent to person to respond 3b. Tweet that does not require escalation is sent to a bot to respond Figure 4.7 The BlazingText workflow to determine whether a tweet should be escalated Getting ready to build the model 85 3 Upload the starting notebook 4 Run it against the data TIP If you\'92re jumping into the book at this chapter, you might want to visit the appendixes, which show you how to do the following: 
\f2 \uc0\u61601 
\f0  Appendix A: sign up for AWS, Amazon\'92s web service 
\f2 \uc0\u61601 
\f0  Appendix B: set up S3, AWS\'92s file storage service 
\f2 \uc0\u61601 
\f0  Appendix C: set up SageMaker 4.6.1 Uploading a dataset to S3 To set up your dataset for this chapter, you\'92ll follow the same steps as you did in appendix B. You don\'92t need to set up another bucket though. You can just go to the same bucket you created earlier. In our example, we called the bucket mlforbusiness, but your bucket will be called something different. When you go to your S3 account, you will see something like that shown in figure 4.8. Click this bucket to see the ch02 and ch03 folders you created in the previous chapters. For this chapter, you\'92ll create a new folder called ch04. You do this by clicking Create Folder and following the prompts to create a new folder. Once you\'92ve created the folder, you are returned to the folder list inside your bucket. There you\'92ll see you now have a folder called ch04. Now that you have the ch04 folder set up in your bucket, you can upload your data file and start setting up the decision-making model in SageMaker. To do so, click the folder and download the data file at this link: https://s3.amazonaws.com/mlforbusiness/ch04/inbound.csv. Then upload the CSV file into the ch04 folder by clicking Upload. Now you\'92re ready to set up the notebook instance. Click the bucket where you are storing your datasets. Figure 4.8 Viewing the list of buckets 86 CHAPTER 4 Should an incident be escalated to your support team? 4.6.2 Setting up a notebook on SageMaker Like you did for chapters 2 and 3, you\'92ll set up a notebook on SageMaker. If you skipped chapters 2 and 3, follow the instructions in appendix C on how to set up SageMaker. When you go to SageMaker, you\'92ll see your notebook instances. The notebook instance you created for chapters 2 and 3 (or that you\'92ve just created by following the instructions in appendix C) will either say Open or Start. If it says Start, click the Start link and wait a couple of minutes for SageMaker to start. Once it displays Open Jupyter, select that link to open your notebook list. Once it opens, create a new folder for chapter 4 by clicking New and selecting Folder at the bottom of the dropdown list. This creates a new folder called Untitled Folder. To rename the folder, tick the checkbox next to Untitled Folder, and you will see the Rename button appear. Click Rename and change the name to ch04. Click the ch04 folder, and you will see an empty notebook list. Just as we already prepared the CSV data you uploaded to S3, we\'92ve already prepared the Jupyter notebook you\'92ll now use. You can download it to your computer by navigating to this URL: https://s3.amazonaws.com/mlforbusiness/ch04/customer_support.ipynb. Click Upload to upload the customer_support.ipynb notebook to the folder. After uploading the file, you\'92ll see the notebook in your list. Click it to open it. Now, just like in chapters 2 and 3, you are a few keystrokes away from being able to run your machine learning model. 4.7 Building the model As in chapters 2 and 3, you will go through the code in six parts: 
\f2 \uc0\u61601 
\f0  Load and examine the data. 
\f2 \uc0\u61601 
\f0  Get the data into the right shape. 
\f2 \uc0\u61601 
\f0  Create training and validation datasets (there\'92s no need for a test dataset in this example). 
\f2 \uc0\u61601 
\f0  Train the machine learning model. 
\f2 \uc0\u61601 
\f0  Host the machine learning model. 
\f2 \uc0\u61601 
\f0  Test the model and use it to make decisions Refresher on running code in Jupyter notebooks SageMaker uses Jupyter Notebook as its interface. Jupyter Notebook is an opensource data science application that allows you to mix code with text. As shown in the figure, the code sections of a Jupyter notebook have a grey background, and the text sections have a white background. Building the model 87 4.7.1 Part 1: Loading and examining the data As in the previous two chapters, the first step is to say where you are storing the data. To do that, you need to change 'mlforbusiness' to the name of the bucket you created when you uploaded the data and rename its subfolder to the name of the subfolder on S3 where you store the data (listing 4.1). If you named the S3 folder ch04, then you don\'92t need to change the name of the folder. If you kept the name of the CSV file that you uploaded earlier in the chapter, then you don\'92t need to change the inbound.csv line of code. If you changed the name of the CSV file, then update inbound.csv to the name you changed it to. And, as always, to run the code in the notebook cell, click the cell and press C+R. data_bucket = 'mlforbusiness' subfolder = 'ch04' dataset = 'inbound.csv' To run the code in the notebook, click into a code cell and press C+R. Alternatively, you can select Run All from the Cell menu item at the top of the notebook. Listing 4.1 Say where you are storing the data Text area (double-click to edit text) Code cell (click into the cell and press Ctrl-Enter to run the code) mlforbusiness' Sample notebook showing text and code cells S3 bucket where the data is stored Subfolder of the S3 bucket where the data is stored Dataset that\'92s used to train and test the model 88 CHAPTER 4 Should an incident be escalated to your support team? The Python modules and libraries imported in listing 4.2 are the same as the import code in chapters 2 and 3, with the exception of lines 6, 7, and 8. These lines import Python\'92s json module. This module is used to work with data structured in JSON format (a structured mark-up language for describing data). Lines 6 and 7 import Python\'92s json and csv modules. These two formats define the data. The next new library you import is NLTK (https://www.nltk.org/). This is a commonly used library for getting text ready to use in a machine learning model. In this chapter, you will use NLTK to tokenize words. Tokenizing text involves splitting the text and stripping out those things that make it harder for the machine learning model to do what it needs to do. In this chapter, you use the standard word_tokenize function that splits text into words in a way that consistently handles abbreviations and other anomalies. BlazingText often works better when you don\'92t spend a lot of time preprocessing the text, so this is all you\'92ll do to prepare each tweet (in addition to applying the labeling, of course, which you\'92ll do in listing 4.8). To run the code, click in the notebook cell and press C+R. import pandas as pd import boto3 import sagemaker import s3fs from sklearn.model_selection \\ import train_test_split import json import csv import nltk role = sagemaker.get_execution_role() s3 = s3fs.S3FileSystem(anon=False) You\'92ve worked with CSV files throughout the book. JSON is a type of structured markup language similar to XML but simpler to work with. The following listing shows an example of an invoice described in JSON format. \{ "Invoice": \{ "Header": \{ "Invoice Number": "INV1234833", "Invoice Date": "2018-11-01" \}, Listing 4.2 Importing the modules Listing 4.3 Sample JSON format Imports the pandas Python library Imports the boto3 AWS library Imports SageMaker Imports the s3fs module to make working with S3 files easier Imports only the sklearn train_test_split module Imports Python\'92s json module for working with JSON files Imports the csv module to work with CSV files Imports NLTK to tokenize tweets Creates a role in SageMaker Establishes the connection with S3 Building the model 89 "Lines": [ \{ "Description": "Punnet of strawberries", "Qty": 6, "Unit Price": 3 \}, \{ "Description": "Punnet of blueberries", "Qty": 6, "Unit Price": 4 \} ] \} \} Next, you\'92ll load and view the data. The dataset you are loading has a half-million rows but loads in only a few seconds, even on the medium-sized server we are using in our SageMaker instance. To time and display how long the code in a cell takes to run, you can include the line %%time in the cell, as shown in the following listing. %%time df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/\{dataset\}') display(df.head()) Table 4.2 shows the output of running display(df.head()). Note that using the .head() function on your DataFrame displays only the top five rows. Listing 4.4 Loading and viewing the data Table 4.2 The top five rows in the tweet dataset row_id tweet_id author_id created_at in_reply_to text escalate 0 2 115712 Tue Oct 31 22:11 2017 sprintcare @sprintcare and how do you propose we do that False 1 3 115712 Tue Oct 31 22:08 2017 sprintcare @sprintcare I have sent several private messag\'85 True 2 5 115712 Tue Oct 31 21:49 2017 sprintcare @sprintcare I did. False 3 16 115713 Tue Oct 31 20:00:43 +0000 2017 sprintcare @sprintcare Since I signed up with you\'85.Sinc\'85 False 4 22 115716 Tue Oct 31 22:16:48 +0000 2017 Ask_Spectrum @Ask_Spectrum Would you like me to email you a\'85 False Displays how long it takes to run the code in the cell Reads the S3 inbound.csv dataset in listing 4.1 Displays the top five rows of the DataFrame 90 CHAPTER 4 Should an incident be escalated to your support team? You can see from the first five tweets that only one was escalated. At this point, we don\'92t know if that is expected or unexpected. Listing 4.5 shows how many rows are in the dataset and how many were escalated or not. To get this information, you run the pandas shape and value_counts functions. print(f'Number of rows in dataset: \{df.shape[0]\}') print(df['escalated'].value_counts()) The next listing shows the output from the code in listing 4.5. Number of rows in dataset: 520793 False 417800 True 102993 Name: escalate, dtype: int64 Out of the dataset of more than 500,000 tweets, just over 100,000 were manually escalated. If Naomi can have a machine learning algorithm read and escalate tweets, then her team will only have to read 20% of the tweets they currently review. 4.7.2 Part 2: Getting the data into the right shape Now that you can see your dataset in the notebook, you can start working with it. First, you create train and validation data for the machine learning model. As in the previous two chapters, you use sklearn\'92s train_test_split function to create the datasets. With BlazingText, you can see the accuracy of the model in the logs as it is validating the model, so there is no need to create a test dataset. train_df, val_df, _, _ = train_test_split( df, df['escalate'], test_size=0.2, random_state=0) print(f'\{train_df.shape[0]\} rows in training data') print(f'\{val_df.shape[0]\} rows in validation data') Unlike the XGBoost algorithm that we worked with in previous chapters, BlazingText cannot work directly with CSV data. It needs the data in a different format, which you will do in listings 4.8 through 4.10. Listing 4.5 Showing the number of escalated tweets in the dataset Listing 4.6 Total number of tweets and the number of escalated tweets Listing 4.7 Creating train and validate datasets Displays the number of rows in the dataset Displays the number of rows where a tweet was escalated and the number of rows where the tweet was not escalated Creates the train and validation datasets Displays the number of rows in the training data Displays the number of rows in the validation data Building the model 91 Listing 4.8 defines two functions. The first function, preprocess, takes a DataFrame containing either the validation or training datasets you created in listing 4.7, turns it into a list, and then for each row in the list, calls the second function transform_ instance to convert the row to the format __label__0 or __label__1, followed by the text of the tweet. To run the preprocess function on the validation data, you call the function on the val_df DataFrame you created in listing 4.8. You\'92ll run this code first on the validation dataset and then on the training dataset. The validation dataset has 100,000 rows, and this cell will take about 30 seconds to run on that data. The training dataset has 400,000 rows and will take about 2 minutes to run. Most of the time is spent converting the dataset to a DataFrame and back again. This is fine for a dataset of a half-million rows. If you are working with a dataset with millions of rows, you\'92ll need to start working directly with the csv module rather than using pandas. To learn more about the cvs module, visit https://docs.python.org/3/ library/csv.html. def preprocess(df): all_rows = df.values.tolist() transformed_rows = list( map(transform_instance, all_rows)) transformed_df = pd.DataFrame(transformed_rows) return transformed_df def transform_instance(row): cur_row = [] label = '__label__1' if row[5] == True \\ else '__label__0' cur_row.append(label) cur_row.extend( nltk.word_tokenize(row[4].lower())) return cur_row transformed_validation_rows = preprocess(val_df) display(transformed_validation_rows.head()) Formatting data for BlazingText BlazingText requires a label in the format __label__0 for a tweet that was not escalated and __label__1 for a tweet that was escalated. The label is then followed by the tokenized text of the tweet. Tokenizing is the process of taking text and breaking it into parts that are linguistically meaningful. This is a difficult task to perform but, fortunately for you, the hard work is handled by the NLTK library. Listing 4.8 Transforming each row to the format used by BlazingText Turns the DataFrame into a list Applies transform_instance to every row in the list Turns it back into a DataFrame Returns the DataFrame Creates an empty list that holds the label followed by each of the words in the tweet Creates a label with the value of 1 if escalated or 0 if not Sets the first element of the cur_row list as the label Sets each of the words as a separate element in the list Returns the row Runs the preprocess function Displays the first five rows of data 92 CHAPTER 4 Should an incident be escalated to your support team? The data shown in table 4.3 shows the first few rows of data in the format BlazingText requires. You can see that the first two tweets were labeled 1 (escalate), and the third row is labeled 0 (don\'92t escalate). Now that you have the text in the format BlazingText can work with, and that text is sitting in a DataFrame, you can use the pandas to_csv to store the data on S3 so you can load it into the BlazingText algorithm. The code in the following listing writes out the validation data to S3. s3_validation_data = f's3://\{data_bucket\}/\\ \{subfolder\}/processed/validation.csv' data = transformed_validation_rows.to_csv( header=False, index=False, quoting=csv.QUOTE_NONE, sep='|', escapechar='^').encode() with s3.open(s3_validation_data, 'wb') as f: f.write(data) Next, you\'92ll preprocess the training data by calling the preprocess function on the train_df DataFrame you created in listing 4.7. %%time transformed_train_rows = preprocess(train_df) display(transformed_train_rows.head()) s3_train_data = f's3://\{data_bucket\}/\{subfolder\}/processed/train.csv' data = transformed_train_rows.to_csv( header=False, index=False, quoting=csv.QUOTE_NONE, sep='|', Table 4.3 Validation data for Naomi\'92s tweets Labeled preprocessed data __label__1 @ 115990 no joke \'85 this is one of the worst customer experiences i have had verizon . maybe time for @ 115714 @ 115911 @ att ? https://t.co/vqmlkvvwxe __label__1 @ amazonhelp neither man seems to know how to deliver a package . that is their entire job ! both should lose their jobs immediately. __label__0 @ xboxsupport yes i see nothing about resolutions or what size videos is exported only quality i have a 34 '' ultrawide monitor 21:9 2560x1080 what i need https://t.co/apvwd1dlq8 Listing 4.9 Transforming the data for BlazingText Listing 4.10 Preprocessing and writing training data Building the model 93 escapechar='^').encode() with s3.open(s3_train_data, 'wb') as f: f.write(data) With that, the train and test datasets are saved to S3 in a format ready for use in the model. The next section takes you though the process of getting the data into SageMaker so it\'92s ready to kick off the training process. 4.7.3 Part 3: Creating training and validation datasets Now that you have your data in a format that BlazingText can work with, you can create the training and validation datasets. %%time train_data = sagemaker.session.s3_input( s3_train_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix') validation_data = sagemaker.session.s3_input( s3_validation_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix') With that, the data is in a SageMaker session, and you are ready to start training the model. 4.7.4 Part 4: Training the model Now that you have prepared the data, you can start training the model. This involves three steps: 
\f2 \uc0\u61601 
\f0  Setting up a container 
\f2 \uc0\u61601 
\f0  Setting the hyperparameters for the model 
\f2 \uc0\u61601 
\f0  Fitting the model The hyperparameters are the interesting part of this code: 
\f2 \uc0\u61601 
\f0  epochs\'97Similar to the num_round parameter for XGBoost in chapters 2 and 3, it specifies how many passes BlazingText performs over the training data. We chose the value of 10 after trying lower values and seeing that more epochs were required. Depending on how the results converge or begin overfitting, you might need to shift this value up or down. 
\f2 \uc0\u61601 
\f0  vector_dim\'97Specifies the dimension of the word vectors that the algorithm learns; default is 100. We set this to 10 because experience has shown that a value as low as 10 is usually still effective and consumes less server time. Listing 4.11 Creating the training, validation, and test datasets Creates the train_data dataset Creates the validation_data dataset 94 CHAPTER 4 Should an incident be escalated to your support team? 
\f2 \uc0\u61601 
\f0  early_stopping\'97Similar to early stopping in XGBoost. The number of epochs can be set to a high value, and early stopping ensures that the training finishes when it stops, improving against the validation dataset. 
\f2 \uc0\u61601 
\f0  patience\'97Sets how many epochs should pass without improvement before early stopping kicks in. 
\f2 \uc0\u61601 
\f0  min_epochs\'97Sets a minimum number of epochs that will be performed even if there is no improvement and the patience threshold is reached. 
\f2 \uc0\u61601 
\f0  word_ngrams\'97N-grams were discussed in figures 4.4, 4.5, and 4.6 earlier in this chapter. Briefly, unigrams are single words, bigrams are pairs of words, and trigrams are groups of three words. In listing 4.12, the first line sets up a container to run the model. A container is just the server that runs the model. The next group of lines configures the server. The set_hyperparameters function sets the hyperparameters for the model. The final line in the next listing kicks off the training of the model. s3_output_location = f's3://\{data_bucket\}/\\ \{subfolder\}/output' sess = sagemaker.Session() container = sagemaker.amazon.amazon_estimator.get_image_uri( boto3.Session().region_name, "blazingtext", "latest") estimator = sagemaker.estimator.Estimator( container, role, train_instance_count=1, train_instance_type='ml.c4.4xlarge', train_max_run = 600, output_path=s3_output_location, sagemaker_session=sess) estimator.set_hyperparameters( mode="supervised", epochs=10, vector_dim=10, early_stopping=True, patience=4, min_epochs=5, word_ngrams=2) estimator.fit(inputs=data_channels, logs=True) Listing 4.12 Training the model Sets the location of the trained model Names this training session Sets up the container (server) Configures the server Assigns the role set up in listing 4.1 Sets the number of servers used to train the model Sets the size of the server Maximum number of minutes the server will run before being terminated Specifies the location of the completed model Names this training session Specifies the mode for BlazingText (supervised or unsupervised) Sets the number of epochs (or rounds) of training Sets the number of vectors Enables early stopping Early stops after 4 epochs when no improvement is observed Performs a minimum of 5 epochs even if there\'92s no improvement after epoch 1 Uses bigrams Building the model 95 NOTE BlazingText can run in supervised or unsupervised mode. Because this chapter uses labeled text, we operate in supervised mode. When you ran this cell in the current chapter (and in chapters 2 and 3), you saw a number of rows with red notifications pop up in the notebook. The red notifications that appear when you run this cell look very different from the XGBoost notifications. Each type of machine learning model provides information that is relevant to understanding how the algorithm is progressing. For the purposes of this book, the most important information comes at the end of the notifications: the training and validation accuracy scores, which display when the training finishes. The model in the following listing shows a training accuracy of over 98.88% and a validation accuracy of 92.28%. Each epoch is described by the validation accuracy. ... -------------- End of epoch: 9 Using 16 threads for prediction! Validation accuracy: 0.922196 Validation accuracy improved! Storing best weights... ##### Alpha: 0.0005 Progress: 98.95% Million Words/sec: 26.89 ##### -------------- End of epoch: 10 Using 16 threads for prediction! Validation accuracy: 0.922455 Validation accuracy improved! Storing best weights... ##### Alpha: 0.0000 Progress: 100.00% Million Words/sec: 25.78 ##### Training finished. Average throughput in Million words/sec: 26.64 Total training time in seconds: 3.40 #train_accuracy: 0.9888 Number of train examples: 416634 #validation_accuracy: 0.9228 Number of validation examples: 104159 2018-10-07 06:56:20 Uploading - Uploading generated training model 2018-10-07 06:56:35 Completed - Training job completed Billable seconds: 49 4.7.5 Part 5: Hosting the model Now that you have a trained model, you can host it on SageMaker so it is ready to make decisions (listings 4.14 and 4.15). We\'92ve covered a lot of ground in this chapter, so we\'92ll delve into how the hosting works in a subsequent chapter. For now, just know that it is setting up a server that receives data and returns decisions. Listing 4.13 Training rounds output Training accuracy Validation accuracy 96 CHAPTER 4 Should an incident be escalated to your support team? endpoint_name = 'customer-support' try: sess.delete_endpoint( sagemaker.predictor.RealTimePredictor( endpoint=endpoint_name).endpoint) print( 'Warning: Existing endpoint deleted to make way for new endpoint.') except: pass Next, in listing 4.15, you create and deploy the endpoint. SageMaker is highly scalable and can handle very large datasets. For the datasets we use in this book, you only need a t2.medium machine to host your endpoint. print('Deploying new endpoint...') text_classifier = estimator.deploy( initial_instance_count = 1, instance_type = 'ml.t2.medium', endpoint_name=endpoint_name) 4.7.6 Part 6: Testing the model Now that the endpoint is set up and hosted, you can start making decisions. In listing 4.16, you set a sample tweet, tokenize it, and then make a prediction. Try changing the text in the first line and clicking C+R to test different tweets. For example, changing the text disappointed to happy or ambivalent changes the label from 1 to 0. This means that the tweet \'93Help me I\'92m very disappointed\'94 will be escalated, but the tweets \'93Help me I\'92m very happy\'94 and \'93Help me I\'92m very ambivalent\'94 will not be escalated. tweet = "Help me I'm very disappointed!" tokenized_tweet = \\ [' '.join(nltk.word_tokenize(tweet))] payload = \{"instances" : tokenized_tweet\} response = \\ text_classifier.predict(json.dumps(payload)) escalate = pd.read_json(response) escalate Listing 4.14 Hosting the model Listing 4.15 Creating a new endpoint to host the model Listing 4.16 Making predictions using the test data In order to not create duplicate endpoints, name your endpoint. Deletes existing endpoint of that name Creates a new endpoint Sample tweet Tokenizes the tweets Creates a payload in a format that the text_classifier you created in listing 4.15 can interpret Gets the response Converts the response to a pandas DataFrame Displays the decision Checking to make sure the endpoint is deleted 97 4.8 Deleting the endpoint and shutting down your notebook instance It is important that you shut down your notebook instance and delete your endpoint. We don\'92t want you to get charged for SageMaker services that you\'92re not using. 4.8.1 Deleting the endpoint Appendix D describes how to shut down your notebook instance and delete your endpoint using the SageMaker console, or you can do that with the code in the next listing. # Remove the endpoint (optional) # Comment out this cell if you want the endpoint to persist after Run All sess.delete_endpoint(text_classifier.endpoint) To delete the endpoint, uncomment the code in the listing, then click C+R to run the code in the cell. 4.8.2 Shutting down the notebook instance To shut down the notebook, go back to your browser tab where you have SageMaker open. Click the Notebook Instances menu item to view all of your notebook instances. Select the radio button next to the notebook instance name as shown in figure 4.9, then click Stop on the Actions menu. It takes a couple of minutes to shut down. 4.9 Checking to make sure the endpoint is deleted If you didn\'92t delete the endpoint using the notebook (or if you just want to make sure it is deleted), you can do this from the SageMaker console. To delete the endpoint, click the radio button to the left of the endpoint name, then click the Actions menu item and click Delete in the menu that appears. When you have successfully deleted the endpoint, you will no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you Listing 4.17 Deleting the notebook 1. Select the radio button. 2. Select Stop. Figure 4.9 Shutting down the notebook 98 CHAPTER 4 Should an incident be escalated to your support team? see the text \'93There are currently no resources\'94 displayed at the bottom of the Endpoints page (figure 4.10). Naomi is very pleased with your results. She can now run all the tweets received by her team through your machine learning application to determine whether they should be escalated. And it identifies frustration in about the same way her team members used to identify discontented tweets (because the machine learning algorithm was trained using her team\'92s past decisions about whether to escalate a tweet). It\'92s pretty amazing. Imagine how hard it would have been to try to establish rules to identify frustrated tweeters. Summary 
\f2 \uc0\u61601 
\f0  You determine which tweets to escalate using natural language processing (NLP) that captures meaning by a multidimensional word vector. 
\f2 \uc0\u61601 
\f0  In order to work with vectors in SageMaker, the only decision you need to make is whether SageMaker should use single words, pairs of words, or word triplets when creating the groups. To indicate this, NLP uses the terms unigram, bigram, and trigram, respectively. 
\f2 \uc0\u61601 
\f0  BlazingText is an algorithm that allows you to classify labeled text to set up your data for an NLP scenario. 
\f2 \uc0\u61601 
\f0  NLTK is a commonly used library for getting text ready to use in a machine learning model by tokenizing text. 
\f2 \uc0\u61601 
\f0  Tokenizing text involves splitting the text and stripping out those things that make it harder for the machine learning model to do what it needs to do. Endpoint successfully deleted Figure 4.10 Verifying that you have successfully deleted the endpoint 99 Should you question an invoice sent by a supplier? Brett works as a lawyer for a large bank. He is responsible for checking that the law firms hired by the bank bill the bank correctly. How tough can this be, you ask? Pretty tough is the answer. Last year, Brett\'92s bank used hundreds of different firms across thousands of different legal matters, and each invoice submitted by a firm contains dozens or hundreds of lines. Tracking this using spreadsheets is a nightmare. In this chapter, you\'92ll use SageMaker and the Random Cut Forest algorithm to create a model that highlights the invoice lines that Brett should query with a law firm. Brett can then apply this process to every invoice to keep the lawyers working for his bank on their toes, saving the bank hundreds of thousands of dollars per year. Off we go! This chapter covers 
\f2 \uc0\u61601 
\f0  What\'92s the real question you\'92re trying to answer? 
\f2 \uc0\u61601 
\f0  A machine learning scenario without trained data 
\f2 \uc0\u61601 
\f0  The difference between supervised and unsupervised machine learning 
\f2 \uc0\u61601 
\f0  Taking a deep dive into anomaly detection 
\f2 \uc0\u61601 
\f0  Using the Random Cut Forest algorithm 100 CHAPTER 5 Should you question an invoice sent by a supplier? 5.1 What are you making decisions about? As always, the first thing we want to look at is what we\'92re making decisions about. In this chapter, at first glance, it appears the question Brett must decide is, should this invoice line be looked at more closely to determine if the law firm is billing us correctly? But, if you build a machine learning algorithm to definitively answer that question correctly 100% of the time, you\'92ll almost certainly fail. Fortunately for you and Brett, that is not the real question you are trying to answer. To understand the true value that Brett brings to the bank, let\'92s look at his process. Prior to Brett and his team performing their functions, the bank found that law firm costs were spiraling out of control. The approach that Brett\'92s team has taken over the past few years is to manually review each invoice and use gut instinct to determine whether they should query costs with the law firm. When Brett reads an invoice, he usually gets a pretty good feel for whether the costs are in line with the type of case the law firm is working on. He can tell pretty accurately whether the firm has billed an unusually high number of hours from partners rather than junior lawyers on the case, or whether a firm seems to be padding the number of hours their paralegals are spending on a matter. When Brett comes across an apparent anomaly, an invoice that he feels has incorrect charges, he contacts the law firm and requests that they provide further information on their fees. The law firm responds in one of two ways: 
\f2 \uc0\u61601 
\f0  They provide additional information to justify their fees. 
\f2 \uc0\u61601 
\f0  They reduce their fees to an amount that is more in line with a typical matter of this type. It is important to note that Brett really doesn\'92t have a lot of clout in this relationship. If his bank instructs a law firm to work on a case, and they say that a particular piece of research took 5 hours of paralegal time, there is little Brett can do to dispute that. Brett can say that it seems like a lot of time. But the law firm can respond with, \'93Well, that\'92s how long it took,\'94 and Brett has to accept that. But this way of looking at Brett\'92s job is too restrictive. The interesting thing about Brett\'92s job is that Brett is effective not because he can identify 100% of the invoice lines that should be queried, but because the law firms know that Brett is pretty good at picking up anomalies. So, if the law firms charge more than they would normally charge for a particular type of service, they know they need to be prepared to justify it. Lawyers really dislike justifying their costs, not because they can\'92t, but because it takes time that they\'92d rather spend billing other clients. Consequently, when lawyers prepare their timesheets, if they know that there is a good chance that a line has more time on it than can easily be justified, they will weigh whether they should adjust their time downward or not. This decision, multiplied over the thousands of lines billed to the bank each year, results in hundreds of thousands of dollars in savings for the bank. The real question you are trying to answer in this scenario is, what invoice lines does Brett need to query to encourage the law firms to bill the bank correctly? The process flow 101 And this question is fundamentally different from the original question about how to accurately determine which line is an anomaly. If you are trying to correctly identify anomalies, your success is determined by your accuracy. However, in this case, if you are simply trying to identify enough anomalies to encourage the law firms to bill the bank correctly, then your success is determined by how efficiently you can hit the threshold of enough anomalies. 5.2 The process flow The process flow for this decision is shown in figure 5.1. It starts when a lawyer creates an invoice and sends it to Brett (1). On receiving the invoice, Brett or a member of his What percentage of anomalies is enough anomalies? A great deal of time and effort can be expended answering this question accurately. If a lawyer knew that one out every thousand anomalous lines would be queried, their behavior might not change at all. But if they knew that 9 out of 10 anomalous lines would be queried, then they would probably prepare their timesheets with a little more consideration. In an academic paper, you want to clearly identify this threshold. In the business world, you need to weigh the benefits of accuracy against the cost of not being able to work on another project because you are spending time identifying a threshold. In Brett\'92s case, it is probably sufficient to compare the results of the algorithm against how well a member of Brett\'92s team can perform the task. If it comes out about the same, then you\'92ve hit the threshold. Lawyer creates invoice 1 2 3 4 Brett reviews invoice Invoice paid Invoice queried Figure 5.1 Current workflow showing Brett\'92s process for reviewing invoices received from lawyers 102 CHAPTER 5 Should you question an invoice sent by a supplier? team reviews the invoice (2) and then does one of two things, depending on whether the fees listed in the invoice seem reasonable: 
\f2 \uc0\u61601 
\f0  The invoice is passed on to Accounts Payable for payment (3). 
\f2 \uc0\u61601 
\f0  The invoice is sent back to the lawyer with a request for clarification of some of the charges (4). With thousands of invoices to review annually, this is a full-time job for Brett and his two staff members. Figure 5.2 shows the new workflow after you implement the machine learning application you\'92ll build in this chapter. When the lawyer sends the invoice (1), instead of Brett or his team reviewing the invoice, it is passed through a machine learning model that determines whether the invoice contains any anomalies (2). If there are no anomalies, the invoice is passed through to Accounts Payable without further review by Brett\'92s team (3). If an anomaly is detected, the application sends the invoice back to the lawyer and requests further information on the fees charged (4). The role Brett plays in this process is to review a certain number of these transactions to ensure the system is functioning as designed (5). Now that Brett is not required to review invoices he is able to spend more time on other aspects of his role such as maintaining and improving relationships with suppliers. 5 Brett periodically reviews to confirm process is working Lawyer creates invoice 1 2 3 4 Invoice run through machine learning model Invoice paid Invoice queried Figure 5.2 New workflow after implementing machine learning app to catch anomalies in invoices Preparing the dataset 103 5.3 Preparing the dataset The dataset you are using in this chapter is a synthetic dataset created by Richie. It contains 100,000 rows of invoice line data from law firms retained by Brett\'92s bank. Law firm invoices are usually quite detailed and show how many minutes the firm spent performing each task. Law firms typically have a stratified fee structure, where junior lawyers and paralegals (staff who perform work that doesn\'92t need to be performed by a qualified lawyer) are billed at a lower cost than senior lawyers and law firm partners. The important information on law firm invoices is the type of material worked on (antitrust, for example), the resource that performed the work (paralegal, junior lawyer, partner, and so on), how many minutes were spent on the activity, and how much it cost. The dataset you\'92ll use in this chapter contains the following columns: 
\f2 \uc0\u61601 
\f0  Matter Number\'97An identifier for each invoice. If two lines have the same matter number, it means that they are on the same invoice. 
\f2 \uc0\u61601 
\f0  Firm Name\'97The name of the law firm. 
\f2 \uc0\u61601 
\f0  Matter Type\'97The type of matter the invoice relates to. 
\f2 \uc0\u61601 
\f0  Resource\'97The type of resource that performs the activity. 
\f2 \uc0\u61601 
\f0  Activity\'97The type of activity performed by the resource. 
\f2 \uc0\u61601 
\f0  Minutes\'97How many minutes it took to perform the activity. 
\f2 \uc0\u61601 
\f0  Fee\'97The hourly rate for the resource. 
\f2 \uc0\u61601 
\f0  Total\'97The total fee. 
\f2 \uc0\u61601 
\f0  Error\'97A column indicating whether the invoice line contains an error. Note that this column exists in this dataset to allow you to determine how successful the model was at picking the lines with errors. In a real-life dataset, you wouldn\'92t have this field. Synthetic data vs. real data Synthetic data is data created by you, the analyst, as opposed to data found in the real world. When you are working with data from your own company, your data will be real data rather than synthetic data. A good set of real data is more fun to work with than synthetic data because it is typically more nuanced than synthetic data. With real data, there are interesting patterns you can find in the data that you weren\'92t expecting to see. Synthetic data, on the other hand, is great in that it shows exactly the concept you want to show, but it lacks the element of surprise and the joy of discovery that working with real data provides. In chapters 2 and 3, you worked with synthetic data (purchase order data and customer churn data). In chapter 4, you worked with real data (tweets to customer support teams). In chapter 6, you\'92ll be back to working with real data (electricity usage data). 104 CHAPTER 5 Should you question an invoice sent by a supplier? Table 5.1 shows three invoice lines in the dataset. In this chapter, you\'92ll build a machine learning application to pick the lines that contain errors. In machine learning lingo, you are identifying anomalies in the data. 5.4 What are anomalies Anomalies are the data points that have something unusual about them. Defining unusual is not always easy. For example, the image in figure 5.3 contains an anomaly that is pretty easy to spot. All the characters in the image are capital S's with the exception of the single number 5. But what about the image shown in figure 5.4? The anomaly is less easy to spot. There are actually two anomalies in this dataset. The first anomaly is similar to the anomaly in figure 5.3. The number 5 in the bottom right of the image is the only number. Every other character is a letter. The last anomaly is difficult: the only characters that appear in pairs are vowels. Admittedly, the last anomaly would be almost impossible for a human to identify but, given enough data, a machine learning algorithm could find it. Table 5.1 Dataset invoice lines for the lawyers submitting invoices to the bank Matter Number Firm Name Matter Type Resource Activity Minutes Fee Total Error 0 Cox Group Antitrust Paralegal Attend Court 110 50 91.67 False 0 Cox Group Antitrust Junior Attend Court 505 150 1262.50 True 0 Cox Group Antitrust Paralegal Attend Meeting 60 50 50.00 False Figure 5.3 A simple anomaly. It\'92s easy to spot the anomaly in this dataset. Supervised vs. unsupervised machine learning 105 Just like the images in figures 5.3 and 5.4, Brett\'92s job is to identify anomalies in the invoices sent to his bank by law firms. Some invoices have anomalies that are easy to find. The invoice might contain a high fee for the resource, such as a law firm charging $500 per hour for a paralegal or junior lawyer, or the invoice might contain a high number of hours for a particular activity, such as a meeting being billed for 360 minutes. But other anomalies are more difficult to find. For example, antitrust matters might typically involve longer court sessions than insolvency matters. If so, a 500-minute court session for an insolvency matter might be an anomaly, but the same court session for an antitrust matter might not. One of the challenges you might have noticed in identifying anomalies in figures 5.3 and 5.4 is that you did not know what type of anomaly you were looking for. This is not dissimilar to identifying anomalies in real-world data. If you had been told that the anomaly had to do with numbers versus letters, you would have easily identified the 5 in figures 5.3 and 5.4. Brett, who is a trained lawyer and has been reviewing legal invoices for years, can pick out anomalies quickly and easily, but he might not consciously know why he feels that a particular line is an anomaly. In this chapter, you will not define any rules to help the model determine what lines contain anomalies. In fact, you won\'92t even tell the model which lines contain anomalies. The model will figure it out for itself. This is called unsupervised machine learning. 5.5 Supervised vs. unsupervised machine learning In the example you are working through in this chapter, you could have had Brett label the invoice lines he would normally query and use that to train an XGBoost model in a manner similar to the XGBoost models you trained in chapters 2 and 3. But what if you didn\'92t have Brett working for you? Could you still use machine learning to tackle this problem? It turns out you can. The machine learning application in this chapter uses an unsupervised algorithm called Random Cut Forest to determine whether an invoice should be queried. The Figure 5.4 A complex anomaly. It\'92s far more difficult to spot the second anomaly in this dataset. 106 CHAPTER 5 Should you question an invoice sent by a supplier? difference between a supervised algorithm and an unsupervised algorithm is that with an unsupervised algorithm, you don\'92t provide any labeled data. You just provide the data and the algorithm decides how to interpret it. In chapters 2, 3, and 4, the machine learning algorithms you used were supervised. In this chapter, the algorithm you will use is unsupervised. In chapter 2, your dataset had a column called tech_approval_required that the model used to learn whether technical approval was required. In chapter 3, your dataset had a column called churned that the model used to learn whether a customer churned or not. In chapter 4, your dataset had a column called escalate to learn whether a particular tweet should be escalated. In this chapter, you are not going to tell the model which invoices should be queried. Instead, you are going to let the algorithm figure out which invoices contain anomalies, and you will query the invoices that have anomalies over a certain threshold. This is unsupervised machine learning. 5.6 What is Random Cut Forest and how does it work? The machine learning algorithm you\'92ll use in this chapter, Random Cut Forest, is a wonderfully descriptive name because the algorithm takes random data points (Random), cuts them to the same number of points and creates trees (Cut). It then looks at all of the trees together (Forest) to determine whether a particular data point is an anomaly\'97hence, Random Cut Forest. A tree is an ordered way of storing numerical data. The simplest type of tree is called a binary tree. It\'92s a great way to store data because it\'92s easy and fast for a computer to work with. To create a tree, you randomly subdivide the data points until you have isolated the point you are testing to determine whether it is an anomaly. Each time you subdivide the data points, it creates a new level of the tree. The fewer times you need to subdivide the data points before you isolate the target data point, the more likely the data point is to be an anomaly for that sample of data. In the two sections that follow, you\'92ll look at two examples of trees with a target data point injected. In the first sample, the target data point will appear to be an anomaly. In the second sample, the target data point will not be an anomaly. When you look at the samples together as a forest, you\'92ll see that the latter point is not likely to be an anomaly. 5.6.1 Sample 1 Figure 5.5 shows six dark dots that represent six data points that have been pulled at random from the dataset. The white dot represents the target data point that you are testing to determine whether it is an anomaly. Visually, you can see that this white dot sits somewhat apart from the other values in this sample of data, so it might be an anomaly. But how do you determine this algorithmically? This is where the tree representation comes in. What is Random Cut Forest and how does it work? 107 Figure 5.6 shows the top level of the tree. The top level is a single node that represents all of the data points in the sample (including the target data point you are testing). If the node contains any data points other than the target point you are testing for, the color of the node is shown as dark. (The top-level node is always dark because it represents all of the data points in the sample.) Figure 5.7 shows the data points after the first subdivision. The dividing line is inserted at random through the data points. Each side of the subdivision represents a node in the tree. Figure 5.8 shows the next level of the tree. The left side of figure 5.7 becomes Node B on the left of the tree. The right side of figure 5.7 becomes Node C on the right of the tree. Both nodes in the tree are shown as dark because both sides of the subdivided diagram in figure 5.7 contain at least one dark dot. Node A Target point you are testing for Random sample of data points Figure 5.5 Sample 1: The white dot represents an anomaly. Level 1 Node A A Figure 5.6 Sample 1: Level-1 tree represents a node with all of the data points in one group. 108 CHAPTER 5 Should you question an invoice sent by a supplier? The next step is to further subdivide the part of the diagram that contains the target data point. This is shown in figure 5.9. You can see that Node C on the right is untouched, whereas the left side is subdivided into Nodes D and E. Node E contains only the target data point, so no further subdivision is required. Figure 5.10 shows the final tree. Node E is shown in white because it contains the target data point. The tree has three levels. The smaller the tree, the greater the likelihood that the point is an anomaly. A three-level tree is a pretty small tree, indicating that the target data point might be an anomaly. Now, let\'92s take a look at another sample of six data points that are clustered more closely around the target data point. Node B Node C Figure 5.7 Sample 1: Level-2 data points divided between two nodes after the first subdivision. Level 1 B C Level 2 Figure 5.8 Sample 1: Level-2 tree represents the data points split into two groups, where both nodes are shown as dark. What is Random Cut Forest and how does it work? 109 5.6.2 Sample 2 In the second data sample, the randomly selected data points are clustered more closely around the target data point. It is important to note that our target data point is the same data point that was used in sample 1. The only difference is that a different sample of data points was drawn from the dataset. You can see in figure 5.11 that the data points in the sample (dark dots) are more closely clustered around the target data point than they were in sample 1. Node D Node C Node E Figure 5.9 Sample 1: Level-3 data points separate the target data point from the values in the dataset. Level 1 C Level 2 Level 3 Target data point isolated in Node E D E Figure 5.10 Sample 1: Level-3 tree represents one of the level-2 groups split again to isolate the target data point. 110 CHAPTER 5 Should you question an invoice sent by a supplier? NOTE In figure 5.11 and the following figures in this section, the tree is displayed below the diagram of the data points. Just as in sample 1, figure 5.12 splits the diagram into two sections, which we have labeled B and C. Because both sections contain dark dots, level 2 of the tree diagram is shown as dark. Next, the section containing the target data point is split again. Figure 5.13 shows that section B has been split into two sections labeled D and E, and a new level has been added to the tree. Both of these sections contain one or more dark dots, so level 3 of the tree diagram is shown as dark. Node A Node A A Level 1 Figure 5.11 Sample 2: Level-1 data points and tree represent all of the data points in a single group. What is Random Cut Forest and how does it work? 111 Node B Node C Level 1 B C Level 2 Figure 5.12 Sample 2: Level-2 data points and tree represent the level-1 groups split into two groups. Node D Node E Node C Level 1 C Level 2 D E Level 3 Figure 5.13 Sample 2: Level-3 data points and tree represent one of the level-2 groups split into two groups. 112 CHAPTER 5 Should you question an invoice sent by a supplier? The target data point is in section E, so that section is split into two sections labeled F and G as shown in figure 5.14. The target data point is in section F, so that section is split into two sections labeled H and J as shown in figure 5.15. Section J contains only the target data point, so it is shown as white. No further splitting is required. The resulting diagram has 5 levels, which indicates that the target data point is not likely to be an anomaly. The final step performed by the Random Cut Forest algorithm is to combine the trees into a forest. If lots of the samples have very small trees, then the target data point is likely to be an anomaly. If only a few of the samples have small trees, then it is likely to not be an anomaly. You can read more about Random Cut Forest on the AWS site at https://docs.aws .amazon.com/sagemaker/latest/dg/randomcutforest.html. Node D Node F Node G Node C Level 1 C Level 2 Level 3 Level 4 D F G Figure 5.14 Sample 2: Level-4 data points and tree represent one of the level-3 groups split into two groups. What is Random Cut Forest and how does it work? 113 Richie\'92s explanation of the forest part of Random Cut Forest Random Cut Forest partitions the dataset into the number of trees in the forest (specified by the num_trees hyperparameter). During training, a total of num_trees \'d7 num_samples_per_tree individual data points get sampled from the full dataset without replacement. For a small dataset, this can be equal to the total number of observations, but for large datasets, it need not be. During inference, however, a brand new data point gets assigned an anomaly score by cycling through all the trees in the forest and determining what anomaly score to give it from each tree. This score then gets averaged to determine if this point should actually be considered an anomaly or not. Node D Node H Node J Node G Node C Level 1 C Level 2 Level 3 Level 4 Level 5 D G H J Figure 5.15 Sample 2: Level-5 data points and tree represent one of the level-4 groups split into two groups, isolating the target data point. 114 CHAPTER 5 Should you question an invoice sent by a supplier? 5.7 Getting ready to build the model Now that you have a deeper understanding of how Random Cut Forest works, you can set up another notebook on SageMaker and make some decisions. As you did in chapters 2, 3, and 4, you are going to do the following: 1 Upload a dataset to S3 2 Set up a notebook on SageMaker 3 Upload the starting notebook 4 Run it against the data TIP If you\'92re jumping into the book at this chapter, you might want to visit the appendixes, which show you how to do the following: 
\f2 \uc0\u61601 
\f0  Appendix A: sign up for AWS, Amazon\'92s web service 
\f2 \uc0\u61601 
\f0  Appendix B: set up S3, AWS\'92s file storage service 
\f2 \uc0\u61601 
\f0  Appendix C: set up SageMaker 5.7.1 Uploading a dataset to S3 To set up the dataset for this chapter, you\'92ll follow the same steps as you did in appendix B. You don\'92t need to set up another bucket though. You can go to the same bucket you created earlier. In our example, we called it mlforbusiness, but your bucket will be called something different. When you go to your S3 account, you will see the bucket you created to hold the data files for previous chapters. Click this bucket to see the ch02, ch03, and ch04 folders you created in previous chapters. For this chapter, you\'92ll create a new folder called ch05. You do this by clicking Create Folder and following the prompts to create a new folder. Once you\'92ve created the folder, you are returned to the folder list inside your bucket. There you\'92ll see you now have a folder called ch05. Now that you have the ch05 folder set up in your bucket, you can upload your data file and start setting up the decision-making model in SageMaker. To do so, click the folder and download the data file at this link: https://s3.amazonaws.com/mlforbusiness/ch05/activities.csv. Then upload the CSV file into your ch05 folder by clicking Upload. Now you\'92re ready to set up the notebook instance. Building the model 115 5.7.2 Setting up a notebook on SageMaker Like you did in chapters 2, 3, and 4, you\'92ll set up a notebook on SageMaker. If you skipped the earlier chapters, follow the instructions in appendix C on how to set up SageMaker. When you go to SageMaker, you\'92ll see your notebook instances. The notebook instance you created for earlier chapters (or that you\'92ve just created by following the instructions in appendix C) will either say Open or Start. If it says Start, click the Start link and wait a couple of minutes for SageMaker to start. Once the screen displays Open Jupyter, click the Open Jupyter link to open up your notebook list. Once it opens, create a new folder for chapter 5 by clicking New and selecting Folder at the bottom of the dropdown list. This creates a new folder called Untitled Folder. When you tick the checkbox next to Untitled Folder, you will see the Rename button appear. Click it and change the folder name to ch05. Click the ch05 folder, and you will see an empty notebook list. Just as we already prepared the CSV data you uploaded to S3 (activities.csv), we\'92ve already prepared the Jupyter notebook you\'92ll now use. You can download it to your computer by navigating to this URL: https://s3.amazonaws.com/mlforbusiness/ch05/detect_suspicious_lines.ipynb. Click Upload to upload the detect_suspicious_lines.ipynb notebook to the ch05 folder. After uploading the file, you\'92ll see the notebook in your list. Click it to open it. Now, just like in the previous chapters, you are a few keystrokes away from being able to run your machine learning model. 5.8 Building the model As in the previous chapters, you will go through the code in six parts: 1 Load and examine the data. 2 Get the data into the right shape. 3 Create training and validation datasets (there\'92s no need for a test dataset in this example). 4 Train the machine learning model. 5 Host the machine learning model. 6 Test the model and use it to make decisions. Refresher on running code in Jupyter notebooks SageMaker uses Jupyter Notebook as its interface. Jupyter Notebook is an opensource data science application that allows you to mix code with text. As shown in the figure, the code sections of a Jupyter notebook have a gray background, and the text sections have a white background. 116 CHAPTER 5 Should you question an invoice sent by a supplier? 5.8.1 Part 1: Loading and examining the data As in the previous three chapters, the first step is to say where you are storing the data. In listing 5.1, you need to change 'mlforbusiness' to the name of the bucket you created when you uploaded the data, then change the subfolder to the name of the subfolder on S3 where you want to store the data. If you named the S3 folder ch05, then you don\'92t need to change the name of the folder. If you kept the name of the CSV file you uploaded earlier in the chapter, then you don\'92t need to change the activities.csv line of code either. If you renamed the CSV file, then you need to update the filename with the name you changed it to. To run the code in the notebook cell, click the cell and press C+R. data_bucket = 'mlforbusiness' subfolder = 'ch05' dataset = 'activities.csv' (continued) To run the code in the notebook, click a code cell and press C+R. To run the notebook, you can select Run All from the Cell menu item at the top of the notebook. When you run the notebook, SageMaker loads the data, trains the model, sets up the endpoint, and generates decisions from the test data. Listing 5.1 Say where you are storing the data Text area (double-click to edit text) Code cell (click into the cell and press Ctrl-Enter to run the code) mlforbusiness' Sample Jupyter notebook showing text and code cells S3 bucket where the data is stored Subfolder of the S3 bucket where the data is stored Dataset that\'92s used to train and test the model Building the model 117 Next you\'92ll import all of the Python libraries and modules that SageMaker uses to prepare the data, train the machine learning model, and set up the endpoint. The Python modules and libraries imported in listing 5.2 are the same as the imports you used in previous chapters. import pandas as pd import boto3 import s3fs import sagemaker from sklearn.model_selection \\ import train_test_split import json import csv role = sagemaker.get_execution_role() s3 = s3fs.S3FileSystem(anon=False) The dataset contains invoice lines from all matters handled by your panel of lawyers over the past 3 months. The dataset has about 100,000 lines covering 2,000 invoices (50 lines per invoice). It contains the following columns: 
\f2 \uc0\u61601 
\f0  Matter Number\'97An identifier for each invoice. If two lines have the same number, it means that these are on the same invoice. 
\f2 \uc0\u61601 
\f0  Firm Name\'97The name of the law firm. 
\f2 \uc0\u61601 
\f0  Matter Type\'97The type of activity the invoice relates to. 
\f2 \uc0\u61601 
\f0  Resource\'97The resource that performs the activity. 
\f2 \uc0\u61601 
\f0  Activity\'97The activity performed by the resource. 
\f2 \uc0\u61601 
\f0  Minutes\'97How many minutes it took to perform the activity. 
\f2 \uc0\u61601 
\f0  Fee\'97The hourly rate for the resource. 
\f2 \uc0\u61601 
\f0  Total\'97The total fee. 
\f2 \uc0\u61601 
\f0  Error\'97Indicates whether the invoice line contains an error. NOTE The Error column is not used during training because, in our scenario, this information is not known until you contact the law firm and determine whether the line was in error. This field is included here to allow you to determine how well your model is working. Next, you\'92ll load and view the data. In listing 5.3, you read the top 20 rows of the CSV data in activities.csv to display those in a pandas DataFrame. In this listing, you use a different way of displaying rows in the pandas DataFrame. Previously, you used the Listing 5.2 Importing the modules Imports the pandas Python library Imports the boto3 AWS library Imports the s3fs module to make working with S3 files easier Imports SageMaker Imports only the train_test_split module from the sklearn library Imports the json module to work with JSON files Imports the csv module to work with comma separated files Creates a role on SageMaker Establishes the connection with S3 118 CHAPTER 5 Should you question an invoice sent by a supplier? head() function to display the top 5 rows. In this listing, you use explicit numbers to display specific rows. df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/\{dataset\}') display(df[5:8]) In this example, the top 5 rows all show no errors. You can tell if a row shows an error by looking at the rightmost column, Error. Rows 5, 6, and 7 are displayed because they show two rows with Error = False and one row with Error = True. Table 5.2 shows the output of running display(df[5:8]). In listing 5.4, you use the pandas value_counts function to determine the error rate. You can see that out of 100,000 rows, about 2,000 have errors, which gives a 2% error rate. Note that in a real-life scenario, you won\'92t know the error rate, so you would have to run a small project to determine your error rate by sampling lines from invoices. [id="esc ---- df['Error'].value_counts() ---- The following listing shows the output from the code in listing 5.4. False 103935 True 2030 Name: escalate, dtype: int64 The next listing shows the types of matters, resources, and activities. Listing 5.3 Loading and viewing the data Table 5.2 Dataset invoice lines display the three rows returned from running display(df[5:8]). Row number Matter Number Firm Name Matter Type Resource Activity Minutes Fee Total Error 5 0 Cox Group Antitrust Paralegal Attend Court 110 50 91.67 False 6 0 Cox Group Antitrust Junior Attend Court 505 150 1262.50 True 7 0 Cox Group Antitrust Paralegal Attend Meeting 60 50 50.00 False Listing 5.4 Displaying the error rate Listing 5.5 Total number of tweets and the number of escalated tweets Reads the S3 dataset set in listing 5.1 Displays 3 rows of the DataFrame (rows 5, 6, and 7) Displays the error rate: False is no error; True is an error. Building the model 119 print(f'Number of rows in dataset: \{df.shape[0]\}') print() print('Matter types:') print(df['Matter Type'].value_counts()) print() print('Resources:') print(df['Resource'].value_counts()) print() print('Activities:') print(df['Activity'].value_counts()) The results of the code in listing 5.6 are shown in listing 5.7. You can see that there are 10 different matter types, ranging from Antitrust to Securities litigation; four different types of resources, ranging from Paralegal to Partner; and four different activity types, such as Phone Call, Attend Meeting, and Attend Court. Number of rows in dataset: 105965 Matter types: Antitrust 23922 Insolvency 16499 IPO 14236 Commercial arbitration 12927 Project finance 11776 M&A 6460 Structured finance 5498 Asset recovery 4913 Tax planning 4871 Securities litigation 4863 Name: Matter Type, dtype: int64 Resources: Partner 26587 Junior 26543 Paralegal 26519 Senior 26316 Name: Resource, dtype: int64 * Activities: Prepare Opinion 26605 Phone Call 26586 Attend Court 26405 Attend Meeting 26369 Name: Activity, dtype: int64 The machine learning model uses these features to determine which invoice lines are potentially erroneous. In the next section, you\'92ll work with these features to get them into the right shape for use in the machine learning model. Listing 5.6 Describing the data Listing 5.7 Viewing the data description 120 CHAPTER 5 Should you question an invoice sent by a supplier? 5.8.2 Part 2: Getting the data into the right shape Now that you\'92ve loaded the data, you need to get the data into the right shape. This involves several steps: 
\f2 \uc0\u61601 
\f0  Changing the categorical data to numerical data 
\f2 \uc0\u61601 
\f0  Splitting the dataset into training data and validation data 
\f2 \uc0\u61601 
\f0  Removing unnecessary columns The machine learning algorithm you\'92ll use in this notebook is the Random Cut Forest algorithm. Just like the XGBoost algorithm you used in chapters 2 and 3, Random Cut Forest can\'92t handle text values\'97everything needs to be a number. And, as you did in chapters 2 and 3, you\'92ll use the pandas get_dummies function to convert each of the different text values in the Matter Type, Resource, and Activity columns and place a 0 or a 1 as the value in the column. For example, the rows shown in the three-column table 5.3 would be converted to a four column table. The converted table (table 5.4) has four columns because an additional column gets created for each unique value in any of the columns. Given that there are two different values in the Resource column in table 5.3, that column is split into two columns: one for each type of resource. In listing 5.8, you create a pandas DataFrame called encoded_df by calling the get_dummies() function on the original pandas df DataFrame. Calling the head() function here returns the first three rows of the DataFrame. Note that this can create very wide datasets, as every unique value becomes a column. The DataFrame you work with in this chapter increases from a 9-column table to a 24-column table. To determine how wide your table will be, you need to subtract the number of columns you are applying the get_dummies function to and add the number of unique elements in each column. So, your original 9-column table becomes a 6- column table once you subtract the 3 columns you apply the get_dummies function Table 5.3 Data before applying the get_dummies function Matter Number Matter Type Resource 0 Antitrust Paralegal 0 Antitrust Partner Table 5.4 Data after applying the get_dummies function Matter Number Matter_Type_Antitrust Resource_Paralegal Resource_Partner 01 1 0 01 0 1 Building the model 121 to. Then it expands to a 24-column table once you add 10 columns for each unique element in the Matter Type column and four columns each for the unique elements in the Resource and Activity columns. encoded_df = pd.get_dummies( df, columns=['Matter Type','Resource','Activity']) encoded_df.head(3) 5.8.3 Part 3: Creating training and validation datasets You now split the dataset into train and validation data. Note that with this notebook, you don\'92t have any test data. In a real-world situation, the best way to test the data is often to compare your success at identifying errors before using the machine learning model with your success after you use the machine learning algorithm. A test size of 0.2 instructs the function to place 80% of the data into a train DataFrame and 20% into a validation DataFrame. If you are splitting a dataset into training and validation data, you typically will place 70% of your data into a training dataset, 20% into test, and 10% into validation. For the dataset in this chapter, you are just splitting the data into training and test datasets as, in Brett\'92s data, there will be no validation data. train_df, val_df, _, _ = train_test_split( encoded_df, encoded_df['Error'], test_size=0.2, random_state=0) print( f'\{train_df.shape[0]\} rows in training data') With that, the data is in a SageMaker session, and you are ready to start training the model. 5.8.4 Part 4: Training the model In listing 5.10, you import the RandomCutForest function, set up the training parameters, and store the result in a variable called rcf. This all looks very similar to how you set up the training jobs in previous chapters, with the exception of the final two parameters in the RandomCutForest function. The parameter num_samples_per_tree sets how many samples you include in each tree. Graphically, you can think of it as the number of dark dots per tree. If you have lots of samples per tree, your trees will get very large before the function creates a slice Listing 5.8 Creating the train and validate data Listing 5.9 Creating training and validation datasets Converts three columns into a column for each unique value Displays the top three rows of the DataFrame Creates the training and validation datasets Displays the number of rows in the training data 122 CHAPTER 5 Should you question an invoice sent by a supplier? that contains only the target point. Large trees take longer to calculate than small trees. AWS recommends you start with 100 samples per tree, as that provides a good middle ground between speed and size. The parameter num_trees is the number of trees (groups of dark dots). This parameter should be set to approximate the fraction of errors expected. In your dataset, about 2% (or 1/50) are errors, so you\'92ll set the number of trees to 50. The final line of code in the following listing runs the training job and creates the model. from sagemaker import RandomCutForest session = sagemaker.Session() rcf = RandomCutForest(role=role, train_instance_count=1, train_instance_type='ml.m4.xlarge', data_location=f's3://\{data_bucket\}/\{subfolder\}/', output_path=f's3://\{data_bucket\}/\{subfolder\}/output', num_samples_per_tree=100, num_trees=50) rcf.fit(rcf.record_set(train_df_no_result.values)) 5.8.5 Part 5: Hosting the model Now that you have a trained model, you can host it on SageMaker so it is ready to make decisions. If you have run this notebook already, you might already have an endpoint. To handle this, in the next listing, you delete any existing endpoints you have so you don\'92t end up paying for a bunch of endpoints you aren\'92t using. endpoint_name = 'suspicious-lines' try: sess.delete_endpoint( sagemaker.predictor.RealTimePredictor( endpoint=endpoint_name).endpoint) print( 'Warning: Existing endpoint deleted to make way for new endpoint.') except: pass Next, in listing 5.12, you create and deploy the endpoint. SageMaker is highly scalable and can handle very large datasets. For the datasets we use in this book, you only need a t2.medium machine to host your endpoint. Listing 5.10 Training the model Listing 5.11 Hosting the model: deleting existing endpoints Number of samples per tree Number of trees So you don\'92t create duplicate endpoints, name your endpoint. Deletes existing endpoint with that name Building the model 123 rcf_endpoint = rcf.deploy( initial_instance_count=1, instance_type='ml.t2.medium' ) You now need to set up the code that takes the results from the endpoint and puts them in a format you can easily work with. from sagemaker.predictor import csv_serializer, json_deserializer rcf_endpoint.content_type = 'text/csv' rcf_endpoint.serializer = csv_serializer rcf_endpoint.accept = 'application/json' rcf_endpoint.deserializer = json_deserializer 5.8.6 Part 6: Testing the model You can now compute anomalies on the validation data as shown in listing 5.14. Here you use the val_df_no_result dataset because it does not contain the Error column (just as the training data did not contain the Error column). You then create a DataFrame called scores_df to hold the results from the numerical values returned from the rcf_endpoint.predict function. Then you\'92ll combine the scores_df DataFrame with the val_df DataFrame so you can see the score from the Random Cut Forest algorithm associated with each row in the training data. results = rcf_endpoint.predict( val_df_no_result.values) scores_df = pd.DataFrame(results['scores']) val_df = val_df.reset_index(drop=True) results_df = pd.concat( [val_df, scores_df], axis=1) results_df['Error'].value_counts() To combine the data, we used the pandas concat function in listing 5.14. This function combines two DataFrames, using the index of the DataFrames. If the axis parameter is 0, you will concatenate rows. If it is 1, you will concatenate columns. Because we have just created the scores_df DataFrame, the index for the rows starts at 0 and goes up to 21,192 (as there are 21,193 rows in the val_df and scores_df DataFrames). We then reset the index of the val_df DataFrame so that it also starts at 0. That way when we concatenate the DataFrames, the scores line up with the correct rows in the val_df DataFrame. Listing 5.12 Hosting the model: setting machine size Listing 5.13 Hosting the model: converting to a workable format Listing 5.14 Adding scores to validation data Number of machines to host your endpoint Size of the machine Gets the results from the val_df_no_result DataFrame Creates a new DataFrame with the results Resets the index of the val_df DataFrame so it starts at zero Concatenates the columns in the val_df and the scores_df DataFrames Shows how many errors there are in the val_df DataFrame 124 CHAPTER 5 Should you question an invoice sent by a supplier? You can see from the following listing that there are 20,791 correct lines in the validation dataset (val_df) and 402 errors (based on the Errors column in the val_df DataFrame). False 20791 True 402 Name: Error, dtype: int64 Brett believes that he and his team catch about half the errors made by law firms and that this is sufficient to generate the behavior the bank wants from their lawyers: to bill accurately because they know that if they don\'92t, they will be asked to provide additional supporting information for their invoices. To identify the errors with scores in the top half of the results, you use the pandas median function to identify the median score of the errors and then create a DataFrame called results_above_cutoff to hold the results (listing 5.16). To confirm that you have the median, you can look at the value counts of the Errors column in the DataFrame to determine that there are 201 rows in the DataFrame (half the total number of errors in the val_df DataFrame). The next listing calculates the number of rows where the score is greater than the median score. score_cutoff = results_df[ results_df['Error'] == True]['score'].median() print(f'Score cutoff: \{score_cutoff\}') results_above_cutoff = results_df[ results_df['score'] > score_cutoff] results_above_cutoff['Error'].value_counts() And the next listing shows the number of true errors above the median score and the number of false positives. Score cutoff: 1.58626156755 True 201 False 67 Listing 5.15 Reviewing erroneous lines Listing 5.16 Calculating errors greater than 1.5 (the median score) Listing 5.17 Viewing false positives Rows that do not contain an error Rows that do contain an error Gets the median score in the results_df DataFrame Creates a new DataFrame called results_above_cutoff that contains rows where the score is greater than the median True score Displays the number of rows in the results_above_cutoff DataFrame Queries only the invoices that have a score greater than 1.586 Returns 201 invoice lines over the threshold that are errors Returns 67 invoice lines over the threshold that are identified as errors but are not errors Building the model 125 Because you are looking at the value_counts of the Errors column, you can also see that for the 67 rows that did not contain errors, you will query the law firm. Brett tells you that this is a better hit rate than his team typically gets. With this information, you are able to prepare the two key ratios that allow you to describe how your model is performing. These two key ratios are recall and precision: 
\f2 \uc0\u61601 
\f0  Recall is the proportion of correctly identified errors over the total number of invoice lines with errors. 
\f2 \uc0\u61601 
\f0  Precision is the proportion of correctly identified errors over the total number of invoice lines predicted to be errors. These concepts are easier to understand with examples. The key numbers in this analysis that allow you to calculate recall and precision are the following: 
\f2 \uc0\u61601 
\f0  There are 402 errors in the validation dataset. 
\f2 \uc0\u61601 
\f0  You set a cutoff to identify half the erroneous lines submitted by the law firms (201 lines). 
\f2 \uc0\u61601 
\f0  When you set the cutoff at this point, you misidentify 67 correct invoice lines as being erroneous. Recall is the number of identified errors divided by the total number of errors. Because we decided to use the median score to determine the cutoff, the recall will always be 50%. Precision is the number of correctly identified errors divided by the total number of errors predicted. The total number of errors predicted is 268 (201 + 67). The precision is 201 / 268, or 75%. Now that you have defined the cutoff, you can set a column in the results_df DataFrame that sets a value of True for rows with scores that exceed the cutoff and False for rows with scores that are less than the cutoff, as shown in the following listing. results_df['Prediction'] = \\ results_df['score'] > score_cutoff results_df.head() The dataset now shows the results for each invoice line in the validation dataset. Listing 5.18 Displaying the results in a pandas DataFrame Exercise: 1 What is the score for row 356 of the val_df dataset? 2 How would you submit this single row to the prediction function to return the score for only that row? Sets the values in the Prediction column to True where the score is greater than the cutoff Displays the results 126 CHAPTER 5 Should you question an invoice sent by a supplier? 5.9 Deleting the endpoint and shutting down your notebook instance It is important that you shut down your notebook instance and delete your endpoint. We don\'92t want you to get charged for SageMaker services that you\'92re not using. 5.9.1 Deleting the endpoint Appendix D describes how to shut down your notebook instance and delete your endpoint using the SageMaker console, or you can do that with the code in this listing. # Remove the endpoint (optional) # Comment out this cell if you want the endpoint to persist after Run All sagemaker.Session().delete_endpoint(rcf_endpoint.endpoint) To delete the endpoint, uncomment the code in the listing, then click C+R to run the code in the cell. 5.9.2 Shutting down the notebook instance To shut down the notebook, go back to your browser tab where you have SageMaker open. Click the Notebook Instances menu item to view all of your notebook instances. Select the radio button next to the notebook instance name as shown in figure 5.16, then click Stop on the Actions menu. It takes a couple of minutes to shut down. 5.10 Checking to make sure the endpoint is deleted If you didn\'92t delete the endpoint using the notebook (or if you just want to make sure it is deleted), you can do this from the SageMaker console. To delete the endpoint, click the radio button to the left of the endpoint name, then click the Actions menu item and click Delete in the menu that appears. When you have successfully deleted the endpoint, you will no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you Listing 5.19 Deleting the notebook 1. Select the radio button. 2. Select Stop. Figure 5.16 Shutting down the notebook Summary 127 see the text \'93There are currently no resources\'94 displayed at the bottom of the Endpoints page (figure 5.17). Brett\'92s team can now run each of the invoices they receive from their lawyers and determine within seconds whether they should query the invoice or not. Now Brett\'92s team can focus on assessing the adequacy of the law firm\'92s responses to their query rather than on whether an invoice should be queried. This will allow Brett\'92s team to handle significantly more invoices with the same amount of effort. Summary 
\f2 \uc0\u61601 
\f0  Identify what your algorithm is trying to achieve. In Brett\'92s case in this chapter, the algorithm does not need to identify every erroneous line, it only needs to identify enough lines to drive the right behavior from the law firms. 
\f2 \uc0\u61601 
\f0  Synthetic data is data created by you, the analyst, as opposed to real data found in the real world. A good set of real data is more interesting to work with than synthetic data because it is typically more nuanced. 
\f2 \uc0\u61601 
\f0  Unsupervised machine learning can be used to solve problems where you don\'92t have any trained data. 
\f2 \uc0\u61601 
\f0  The difference between a supervised algorithm and an unsupervised algorithm is that with an unsupervised algorithm, you don\'92t provide any labeled data. You just provide the data, and the algorithm decides how to interpret it. 
\f2 \uc0\u61601 
\f0  Anomalies are data points that have something unusual about them. 
\f2 \uc0\u61601 
\f0  Random Cut Forest can be used to address the challenges inherent in identifying anomalies. 
\f2 \uc0\u61601 
\f0  Recall and precision are two of the key ratios you use to describe how your model is performing. Endpoint successfully deleted Figure 5.17 Verifying that you have successfully deleted the endpoint 128 Forecasting your company\'92s monthly power usage Kiara works for a retail chain that has 48 locations around the country. She is an engineer, and every month her boss asks her how much energy they will consume in the next month. Kiara follows the procedure taught to her by the previous engineer in her role: she looks at how much energy they consumed in the same month last year, weights it by the number of locations they have gained or lost, and provides that number to her boss. Her boss sends this estimate to the facilities management teams to help plan their activities and then to Finance to forecast expenditure. The problem is that Kiara\'92s estimates are always wrong\'97sometimes by a lot. As an engineer, she reckons there must be a better way to approach this problem. In this chapter, you\'92ll use SageMaker to help Kiara produce better estimates of her company\'92s upcoming power consumption. This chapter covers 
\f2 \uc0\u61601 
\f0  Preparing your data for time-series analysis 
\f2 \uc0\u61601 
\f0  Visualizing data in your Jupyter notebook 
\f2 \uc0\u61601 
\f0  Using a neural network to generate forecasts 
\f2 \uc0\u61601 
\f0  Using DeepAR to forecast power consumption What are you making decisions about? 129 6.1 What are you making decisions about? This chapter covers different material than you\'92ve seen in earlier chapters. In previous chapters, you used supervised and unsupervised machine learning algorithms to make decisions. You learned how each algorithm works and then you applied the algorithm to the data. In this chapter, you\'92ll use a neural network to predict how much power Kiara\'92s company will use next month. Neural networks are much more difficult to intuitively understand than the machine learning algorithms we\'92ve covered so far. Rather than attempt to give you a deep understanding of neural networks in this chapter, we\'92ll focus on how to explain the output from a neural network. Instead of a theoretical discussion of neural networks, you\'92ll come out of this chapter knowing how to use a neural network to forecast time-series events and how to explain the results of the forecast. Rather than learning in detail the why of neural networks, you\'92ll learn the how. Figure 6.1 shows the predicted versus actual power consumption for one of Kiara\'92s sites for a six-week period from mid-October 2018 to the end of November 2018. The site follows a weekly pattern with a higher usage on the weekdays and dropping very low on Sundays. The shaded area shows the range Kiara predicted with 80% accuracy. When Kiara calculates the average error for her prediction, she discovers it is 5.7%, which means that for any predicted amount, it is more likely to be within 5.7% of the predicted figure than not. Using SageMaker, you can do all of this without an indepth understanding of how neural networks actually function. And, in our view, that\'92s OK. To understand how neural networks can be used for time-series forecasting, you first need to understand why time-series forecasting is a thorny issue. Once you understand this, you\'92ll see what a neural network is and how a neural network can be applied to time-series forecasting. Then you\'92ll roll up your sleeves, fire up SageMaker, and see it in action on real data. NOTE The power consumption data you\'92ll use in this chapter is provided by BidEnergy (http://www.bidenergy.com), a company that specializes in powerusage forecasting and in minimizing power expenditure. The algorithms used Figure 6.1 Predicted versus actual power consumption for November 2018 for one of Kiara\'92s sites 130 CHAPTER 6 Forecasting your company\'92s monthly power usage by BidEnergy are more sophisticated than you\'92ll see in this chapter, but you\'92ll get a feel for how machine learning in general, and neural networks in particular, can be applied to forecasting problems. 6.1.1 Introduction to time-series data Time-series data consists of a number of observations at particular intervals. For example, if you created a time series of your weight, you could record your weight on the first of every month for a year. Your time series would have 12 observations with a numerical value for each observation. Table 6.1 shows what this might look like. It\'92s pretty boring to look at a table of data. It\'92s hard to get a real understanding of the data when it is presented in a table format. Line charts are the best way to view data. Figure 6.2 shows the same data presented as a chart. You can see from this time series that the date is on the left and my weight is on the right. If you wanted to record the time series of body weight for your entire family, for example, you would add a column for each of your family members. In table 6.2, you can see my weight and the weight of each of my family members over the course of a year. Table 6.1 Time-series data showing my (Doug's) weight in kilograms over the past year Date Weight (kg) 2018-01-01 75 2018-02-01 73 2018-03-01 72 2018-04-01 71 2018-05-01 72 2018-06-01 71 2018-07-01 70 2018-08-01 73 2018-09-01 70 2018-10-01 69 2018-11-01 72 2018-12-01 74 What are you making decisions about? 131 And, once you have that, you can visualize the data as four separate charts, as shown in figure 6.3. Table 6.2 Time-series data showing the weight in kilograms of family members over a year Date Me Spouse Child 1 Child 2 2018-01-01 75 52 38 67 2018-02-01 73 52 39 68 2018-03-01 72 53 40 65 2018-04-01 71 53 41 63 2018-05-01 72 54 42 64 2018-06-01 71 54 42 65 2018-07-01 70 55 42 65 2018-08-01 73 55 43 66 2018-09-01 70 56 44 65 2018-10-01 69 57 45 66 2018-11-01 72 57 46 66 2018-12-01 74 57 46 66 Figure 6.2 A line chart displays the same time-series data showing my weight in kilograms over the past year. 132 CHAPTER 6 Forecasting your company\'92s monthly power usage You\'92ll see the chart formatted in this way throughout this chapter and the next. It is a common format used to concisely display time-series data. 6.1.2 Kiara\'92s time-series data: Daily power consumption Power consumption data is displayed in a manner similar to our weight data. Kiara\'92s company has 48 different business sites (retail outlets and warehouses), so each site gets its own column when you compile the data. Each observation is a cell in that column. Table 6.3 shows a sample of the electricity data used in this chapter. This data looks similar to the data in table 6.2, which shows the weight of each family member each month. The difference is that instead of each column representing a family member, in Kiara\'92s data, each column represents a site (office or warehouse location) for her company. And instead of each row representing a person\'92s weight on the first day of the month, each row of Kiara\'92s data shows how much power each site used on that day. Now that you see how time-series data can be represented and visualized, you are ready to see how to use a Jupyter notebook to visualize this data. Table 6.3 Power usage data sample for Kiara\'92s company in 30-minute intervals Time Site_1 Site_2 Site_3 Site_4 Site_5 Site_6 2017-11-01 00:00:00 13.30 13.3 11.68 13.02 0.0 102.9 2017-11-01 00:30:00 11.75 11.9 12.63 13.36 0.0 122.1 2017-11-01 01:00:00 12.58 11.4 11.86 13.04 0.0 110.3 Figure 6.3 Line charts display the same time-series data showing the weight in kilograms of members of a family over the past year. Loading the Jupyter notebook for working with time-series data 133 6.2 Loading the Jupyter notebook for working with time-series data To help you understand how to display time-series data in SageMaker, for the first time in this book, you\'92ll work with a Jupyter notebook that does not contain a SageMaker machine learning model. Fortunately, because the SageMaker environment is simply a standard Jupyter Notebook server with access to SageMaker models, you can use SageMaker to run ordinary Jupyter notebooks as well. You\'92ll start by downloading and saving the Jupyter notebook at https://s3.amazonaws.com/mlforbusiness/ch06/time_series_practice.ipynb You\'92ll upload it to the same SageMaker environment you have used for previous chapters. Like you did for previous chapters, you\'92ll set up a notebook on SageMaker. If you skipped the earlier chapters, follow the instructions in appendix C on how to set up SageMaker. When you go to SageMaker, you\'92ll see your notebook instances. The notebook instances you created for the previous chapters (or the one that you\'92ve just created by following the instructions in appendix C) will either say Open or Start. If it says Start, click the Start link and wait a couple of minutes for SageMaker to start. Once it displays Open Jupyter, select that link to open your notebook list (figure 6.4). Create a new folder for chapter 6 by clicking New and selecting Folder at the bottom of the dropdown list (figure 6.5). This creates a new folder called Untitled Folder. To rename the folder, when you tick the checkbox next to Untitled Folder, you will see the Rename button appear. Click it and change the name to ch06. Click the ch06 folder and you will see an empty notebook list. Click Upload to upload the time_series _practice.ipynb notebook to the folder. After uploading the file, you\'92ll see the notebook in your list. Click it to open it. You are now ready to work with the time_series_practice notebook. But before we set up the time-series data for this notebook, let\'92s look at some of the theory and practices surrounding time-series analysis. Select Open Jupyter. Figure 6.4 Viewing the Notebook instances list 134 CHAPTER 6 Forecasting your company\'92s monthly power usage 6.3 Preparing the dataset: Charting time-series data Jupyter notebooks and pandas are excellent tools for working with time-series data. In the SageMaker neural network notebook you\'92ll create later in this chapter, you\'92ll use pandas and a data visualization library called Matplotlib to prepare the data for the neural network and to analyze the results. To help you understand how it works, you\'92ll get your hands dirty with a time-series notebook that visualizes the weight of four different people over the course of a year. To use a Jupyter notebook to visualize data, you\'92ll need to set up the notebook to do so. As shown in listing 6.1, you first need to tell Jupyter that you intend to display some charts in this notebook. You do this with the line %matplotlib inline, as shown in line 1. %matplotlib inline import pandas as pd import matplotlib.pyplot as plt Matplotlib is a Python charting library, but there are lots of Python charting libraries that you could use. We have selected Matplotlib because it is available in the Python standard library and, for simple things, is easy to use. The reason line 1 starts with a % symbol is that the line is an instruction to Jupyter, rather than a line in your code. It tells the Jupyter notebook that you\'92ll be displaying Listing 6.1 Displaying charts 2. Select Folder. 1. Select New. Figure 6.5 Creating a new folder in SageMaker Loads Matplotlib capability into Jupyter Imports the pandas library for working with data Imports the Matplotlib library for displaying charts Preparing the dataset: Charting time-series data 135 charts, so it should load the software to do this into the notebook. This is called a magic command. As you can see in listing 6.1, after loading the Matplotlib functionality into your Jupyter notebook, you then imported the libraries: pandas and Matplotlib. (Remember that in line 1 of the listing, where you referenced %matplotlib inline, you did not import the Matplotlib library; line 3 is where you imported that library.) After importing the relevant libraries, you then need to get some data. When working with SageMaker in the previous chapters, you loaded that data from S3. For this notebook, because you are just learning about visualization with pandas and Jupyter Notebook, you\'92ll just create some data and send it to a pandas DataFrame, as shown in the next listing. my_weight = [ \{'month': '2018-01-01', 'Me': 75\}, \{'month': '2018-02-01', 'Me': 73\}, \{'month': '2018-03-01', 'Me': 72\}, \{'month': '2018-04-01', 'Me': 71\}, \{'month': '2018-05-01', 'Me': 72\}, \{'month': '2018-06-01', 'Me': 71\}, \{'month': '2018-07-01', 'Me': 70\}, \{'month': '2018-08-01', 'Me': 73\}, \{'month': '2018-09-01', 'Me': 70\}, \{'month': '2018-10-01', 'Me': 69\}, \{'month': '2018-11-01', 'Me': 72\}, \{'month': '2018-12-01', 'Me': 74\} ] df = pd.DataFrame(my_weight).set_index('month') df.index = pd.to_datetime(df.index) df.head() Now, here\'92s where the real magic lies. To display a chart, all you need to do is type the following line into the Jupyter notebook cell: df.plot() The Matplotlib library recognizes that the data is time-series data from the index type you set in line 3 of listing 6.2, so it just works. Magic! The output of the df.plot() command is shown in figure 6.6. The magic command: Is it really magic? Actually, it is. When you see a command in a Jupyter notebook that starts with % or with %%, the command is known as a magic command. Magic commands provide additional features to the Jupyter notebook, such as the ability to display charts or run external scripts. You can read more about magic commands at https://ipython .readthedocs.io/en/stable/interactive/magics.html. Listing 6.2 Inputting the time-series data Creates the dataset for data in figure 6.1 Converts the dataset to a pandas DataFrame Sets the index of the DataFrame to a time series Displays the first five rows 136 CHAPTER 6 Forecasting your company\'92s monthly power usage To expand the data to include the weight of the entire family, you first need to set up the data. The following listing shows the dataset expanded to include data from every family member. family_weight = [ \{'month': '2018-01-01', 'Me': 75, 'spouse': 67, 'ch_1': 52, 'ch_2': 38\}, \{'month': '2018-02-01', 'Me': 73, 'spouse': 68, 'ch_1': 52, 'ch_2': 39\}, \{'month': '2018-03-01', 'Me': 72, 'spouse': 65, 'ch_1': 53, 'ch_2': 40\}, \{'month': '2018-04-01', 'Me': 71, 'spouse': 63, 'ch_1': 53, 'ch_2': 41\}, \{'month': '2018-05-01', 'Me': 72, 'spouse': 64, 'ch_1': 54, 'ch_2': 42\}, \{'month': '2018-06-01', 'Me': 71, 'spouse': 65, 'ch_1': 54, 'ch_2': 42\}, \{'month': '2018-07-01', 'Me': 70, 'spouse': 65, 'ch_1': 55, 'ch_2': 42\}, \{'month': '2018-08-01', 'Me': 73, 'spouse': 66, 'ch_1': 55, 'ch_2': 43\}, \{'month': '2018-09-01', 'Me': 70, 'spouse': 65, 'ch_1': 56, 'ch_2': 44\}, \{'month': '2018-10-01', 'Me': 69, 'spouse': 66, 'ch_1': 57, 'ch_2': 45\}, \{'month': '2018-11-01', 'Me': 72, 'spouse': 66, 'ch_1': 57, 'ch_2': 46\}, \{'month': '2018-12-01', 'Me': 74, 'spouse': 66, 'ch_1': 57, 'ch_2': 46\} Listing 6.3 Inputting time-series data for the whole family Figure 6.6 Time-series data returned by df.plot showing my weight in kilograms over the past year Creates the dataset for the month and each person\'92s weight Preparing the dataset: Charting time-series data 137 ] df2 = pd.DataFrame( family_weight).set_index('month') df2.index = pd.to_datetime(df2.index) df2.head() Displaying four charts in Matplotlib is a little more complex than displaying one chart. You need to first create an area to display the charts, and then you need to loop across the columns of data to display the data in each column. Because this is the first loop you\'92ve used in this book, we\'92ll go into some detail about it. 6.3.1 Displaying columns of data with a loop The loop is called a for loop, which means you give it a list (usually) and step through each item in the list. This is the most common type of loop you\'92ll use in data analysis and machine learning because most of the things you\'92ll loop through are lists of items or rows of data. The standard way to loop is shown in the next listing. Line 1 of this listing defines a list of three items: A, B, and C. Line 2 sets up the loop, and line 3 prints each item. my_list = ['A', 'B', 'C'] for item in my_list: print(item) Running this code prints A, B, and C, as shown next. A B C When creating charts with Matplotlib, in addition to looping, you need to keep track of how many times you have looped. Python has a nice way of doing this: it\'92s called enumerate. To enumerate through a list, you provide two variables to store the information from the loop and to wrap the list you are looping through. The enumerate function returns two such variables. The first variable is the count of how many times you\'92ve looped (starting with zero), and the second variable is the item retrieved from the list. Listing 6.6 shows listing 6.4 converted to an enumerated for loop. Listing 6.4 Standard way to loop through a list Listing 6.5 Standard output when you run the command to loop through a list Converts the dataset to a pandas DataFrame Sets the index of the DataFrame to a time series Displays the first five rows Creates a list called my_list Loops through my_list Prints each item in the list 138 CHAPTER 6 Forecasting your company\'92s monthly power usage my_list = ['A', 'B', 'C'] for i, item in enumerate(my_list): print(f'\{i\}. \{item\}') Running this code creates the same output as that shown in listing 6.5 but also allows you to display how many items you have looped through in the list. To run the code, click the cell and press C+R. The next listing shows the output from using the enumerate function in your loop. 0. A 1. B 2. C With that as background, you are ready to create multiple charts in Matplotlib. 6.3.2 Creating multiple charts In Matplotlib, you use the subplots functionality to create multiple charts. To fill the subplots with data, you loop through each of the columns in the table showing the weight of each family member (listing 6.3) and display the data from each column. start_date = "2018-01-01" end_date = "2018-12-31" fig, axs = plt.subplots( 2, 2, figsize=(12, 5), sharex=True) axx = axs.ravel() for i, column in enumerate(df2.columns): df2[df2.columns[i]].loc[start_date:end_date].plot( ax=axx[i]) axx[i].set_xlabel("month") axx[i].set_ylabel(column) In line 3 of listing 6.8, you state that you want to display a grid of 2 charts by 2 charts with a width of 12 inches and height of 5 inches. This creates four Matplotlib objects in a 2-by-2 grid that you can fill with data. You use the code in line 4 to turn the 2-by-2 grid into a list that you can loop through. The variable axx stores the list of Matplotlib subplots that you will fill with data. Listing 6.6 Standard way to enumerate through a loop Listing 6.7 Output when enumerating through the loop Listing 6.8 Charting time-series data for the whole family Creates a list called my_list Loops through my_list and stores the count in the i variable and the list item in the item variable Prints the loop count (starting with zero) and each item in the list Sets the start date Sets the end date Creates the matplotlib to hold the four charts Ensures the plots are stored as a series so you can loop through them Loops through each of the columns of data Sets the chart to display a particular column of data Sets the y-axis label Sets the x-axis label What is a neural network? 139 When you run the code in the cell by clicking into the cell and pressing C+R, you can see the chart, as shown in figure 6.7. So far in this chapter, you\'92ve looked at time-series data, learned how to loop through it, and how to visualize it. Now you\'92ll learn why neural networks are a good way to forecast time-series events. 6.4 What is a neural network? Neural networks (sometimes referred to as deep learning) approach machine learning in a different way than traditional machine learning models such as XGBoost. Although both XGBoost and neural networks are examples of supervised machine learning, each uses different tools to tackle the problem. XGBoost uses an ensemble of approaches to attempt to predict the target result, whereas a neural network uses just one approach. A neural network attempts to solve the problem by using layers of interconnected neurons. The neurons take inputs in one end and push outputs to the other side. The connections between the neurons have weights assigned to them. If a neuron receives enough weighted input, then it fires and pushes the signal to the next layer of neurons it\'92s connected to. Figure 6.7 Time-series data generated with code, showing the weight of family members over the past year. 140 CHAPTER 6 Forecasting your company\'92s monthly power usage DEFINITION A neuron is simply a mathematical function that receives two or more inputs, applies a weighting to the inputs, and passes the result to multiple outputs if the weighting is above a certain threshold. Imagine you\'92re a neuron in a neural network designed to filter gossip based on how salacious it is or how true it is. You have ten people (the interconnected neurons) who tell you gossip, and if it is salacious enough, true enough, or a combination of the two, you\'92ll pass it on to the ten people who you send gossip to. Otherwise, you\'92ll keep it to yourself. Also imagine that some of the people who send you gossip are not very trustworthy, whereas others are completely honest. (The trustworthiness of your sources changes when you get feedback on whether the gossip was true or not and how salacious it was perceived to be.) You might not pass on a piece of gossip from several of your least trustworthy sources, but you might pass on gossip that the most trusted people tell you, even if only one person tells you this gossip. Now, let\'92s look at a specific time-series neural network algorithm that you\'92ll use to forecast power consumption. 6.5 Getting ready to build the model Now that you have a deeper understanding of neural networks and of how DeepAR works, you can set up another notebook in SageMaker and make some decisions. As you did in the previous chapters, you are going to do the following: 1 Upload a dataset to S3 2 Set up a notebook on SageMaker 3 Upload the starting notebook 4 Run it against the data What is DeepAR? DeepAR is Amazon\'92s time-series neural network algorithm that takes as its input related types of time-series data and automatically combines the data into a global model of all time series in a dataset. It then uses this global model to predict future events. In this way, DeepAR is able to incorporate different types of time-series data (such as power consumption, temperature, wind speed, and so on) into a single model that is used in our example to predict power consumption. In this chapter, you are introduced to DeepAR, and you\'92ll build a model for Kiara that uses historical data from her 48 sites. In chapter 7, you will incorporate other features of the DeepAR algorithm (such as weather patterns) to enhance your prediction.a a You can read more about DeepAR on this AWS site: https://docs.aws.amazon.com/sagemaker/latest/dg/ deepar.html. Building the model 141 TIP If you\'92re jumping into the book at this chapter, you might want to visit the appendixes, which show you how to do the following: 
\f2 \uc0\u61601 
\f0  Appendix A: sign up to AWS, Amazon\'92s web service 
\f2 \uc0\u61601 
\f0  Appendix B: set up S3, AWS\'92s file storage service 
\f2 \uc0\u61601 
\f0  Appendix C: set up SageMaker 6.5.1 Uploading a dataset to S3 To set up the dataset for this chapter, you\'92ll follow the same steps as you did in appendix B. You don\'92t need to set up another bucket though. You can just go to the same bucket you created earlier. In our example, we called the bucket mlforbusiness, but your bucket will be called something different. When you go to your S3 account, you will see a list of your buckets. Clicking the bucket you created for this book, you might see the ch02, ch03, ch04, and ch05 folders if you created these in the previous chapters. For this chapter, you\'92ll create a new folder called ch06. You do this by clicking Create Folder and following the prompts to create a new folder. Once you\'92ve created the folder, you are returned to the folder list inside your bucket. There you will see you now have a folder called ch06. Now that you have the ch06 folder set up in your bucket, you can upload your data file and start setting up the decision-making model in SageMaker. To do so, click the folder and download the data file at this link: https://s3.amazonaws.com/mlforbusiness/ch06/meter_data.csv Then upload the CSV file into the ch06 folder by clicking Upload. Now you\'92re ready to set up the notebook instance. 6.5.2 Setting up a notebook on SageMaker Just as we prepared the CSV data you uploaded to S3 for this scenario, we\'92ve already prepared the Jupyter notebook you\'92ll use now. You can download it to your computer by navigating to this URL: https://s3.amazonaws.com/mlforbusiness/ch06/energy_usage.ipynb Once you have downloaded the notebook to your computer, you can upload it to the same SageMaker folder you used to work with the time_series_practice.ipynb notebook. (Click Upload to upload the notebook to the folder.) 6.6 Building the model As in the previous chapters, you will go through the code in six parts: 1 Load and examine the data. 2 Get the data into the right shape. 3 Create training and validation datasets. 142 CHAPTER 6 Forecasting your company\'92s monthly power usage 4 Train the machine learning model. 5 Host the machine learning model. 6 Test the model and use it to make decisions. 6.6.1 Part 1: Loading and examining the data As in the previous chapters, the first step is to say where you are storing the data. To do that, you need to change 'mlforbusiness' to the name of the bucket you created when you uploaded the data, and rename its subfolder to the name of the subfolder on S3 where you want to store the data (listing 6.9). If you named the S3 folder ch06, then you don\'92t need to change the name of the folder. If you kept the name of the CSV file that you uploaded earlier in the chapter, then you don\'92t need to change the meter_data.csv line of code. If you changed the name of the CSV file, then update meter_data.csv to the name you changed it to. To run the code in the notebook cell, click the cell and press C+R. data_bucket = 'mlforbusiness' subfolder = 'ch06' dataset = 'meter_data.csv' Many of the Python modules and libraries imported in listing 6.10 are the same as the imports you used in previous chapters, but you\'92ll also use Matplotlib in this chapter. %matplotlib inline import datetime import json import random from random import shuffle import boto3 import ipywidgets as widgets import matplotlib.pyplot as plt import numpy as np import pandas as pd from dateutil.parser import parse import s3fs import sagemaker role = sagemaker.get_execution_role() s3 = s3fs.S3FileSystem(anon=False) Listing 6.9 Say where you are storing the data Listing 6.10 Importing the modules S3 bucket where the data is stored Subfolder of S3 bucket where the data is stored Dataset that\'92s used to train and test the model Uses plotting in the Jupyter notebook Uses date and time functions Imports Python\'92s json module to work with JSON files Imports the random module to generate random numbers Imports the shuffle function Imports the to shuffle random numbers boto3 AWS library Imports interactive widgets in Jupyter notebooks Imports plotting functionality from Matplotlib Imports the numpy library to work with arrays of numbers Imports the pandas Python library Date parsing convenience functions Imports the s3fs module to make working with S3 Imports files easier SageMaker Creates a role on SageMaker Establishes the connection with S3 Building the model 143 Next, you\'92ll load and view the data. In listing 6.11, you read in the CSV data and display the top 5 rows in a pandas DataFrame. Each row of the dataset shows 30-minute energy usage data for a 13-month period from November 1, 2017, to mid-December 2018. Each column represents one of the 48 retail sites owned by Kiara\'92s company. s3_data_path = \\ f"s3://\{data_bucket\}/\{subfolder\}/data" s3_output_path = \\ f"s3://\{data_bucket\}/\{subfolder\}/output" df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/meter_data.csv', index_col=0) df.head() Table 6.4 shows the output of running display(df[5:8]). The table only shows the first 6 sites, but the dataset in the Jupyter notebook has all 48 sites. You can see that column 5 in table 6.4 shows zero consumption for the first hour and a half in November. We don\'92t know if this is an error in the data or if the stores didn\'92t consume any power during that period. We\'92ll discuss the implications of this as you work through the analysis. Let\'92s take a look at the size of your dataset. When you run the code in listing 6.12, you can see that the dataset has 48 columns (one column per site) and 19,632 rows of 30-minute data usage. print(f'Number of rows in dataset: \{df.shape[0]\}') print(f'Number of columns in dataset: \{df.shape[1]\}') Now that you\'92ve loaded the data, you need to get the data into the right shape so you can start working with it. Listing 6.11 Loading and viewing the data Table 6.4 Power usage data in half-hour intervals Index Site_1 Site_2 Site_3 Site_4 Site_5 Site_6 2017-11-01 00:00:00 13.30 13.3 11.68 13.02 0.0 102.9 2017-11-01 00:30:00 11.75 11.9 12.63 13.36 0.0 122.1 2017-11-01 01:00:00 12.58 11.4 11.86 13.04 0.0 110.3 Listing 6.12 Viewing the number of rows and columns Reads the S3 dataset Displays 3 rows of the DataFrame (rows 5, 6, and 7) Reads in the meter data Displays the top 5 rows 144 CHAPTER 6 Forecasting your company\'92s monthly power usage 6.6.2 Part 2: Getting the data into the right shape Getting the data into the right shape involves several steps: 
\f2 \uc0\u61601 
\f0  Converting the data to the right interval 
\f2 \uc0\u61601 
\f0  Determining if missing values are going to create any issues 
\f2 \uc0\u61601 
\f0  Fixing any missing values if you need to 
\f2 \uc0\u61601 
\f0  Saving the data to S3 First, you will convert the data to the right interval. Time-series data from the power meters at each site is recorded in 30-minute intervals. The fine-grained nature of this data is useful for certain work, such as quickly identifying power spikes or drops, but it is not the right interval for our analysis. For this chapter, because you are not combining this dataset with any other datasets, you could use the data with the 30-minute interval to run your model. However, in the next chapter, you will combine the historical consumption data with daily weather forecast predictions to better predict the consumption over the coming month. The weather data that you\'92ll use in chapter 7 reflects weather conditions set at daily intervals. Because you\'92ll be combining the power consumption data with daily weather data, it is best to work with the power consumption data using the same interval. CONVERTING THE DATASET FROM 30-MINUTE INTERVALS TO DAILY INTERVALS Listing 6.13 shows how to convert 30-minute data to daily data. The pandas library contains many helpful features for working with time-series data. Among the most helpful is the resample function, which easily converts time-series data in a particular interval (such as 30 minutes) into another interval (such as daily). In order to use the resample function, you need to ensure that your dataset uses the date column as the index and that the index is in date-time format. As you might expect from its name, the index is used to reference a row in the dataset. So, for example, a dataset with the index of 1:30 AM 1 November 2017 can be referenced by 1:30 AM 1 November 2017. The pandas library can take rows referenced by such indexes and convert them into other periods such as days, months, quarters, or years. df.index = pd.to_datetime(df.index) daily_df = df.resample('D').sum() daily_df.head() Table 6.5 shows the converted data in daily figures. Listing 6.13 Converting data to daily figures Sets the index column to a date-time format Resamples the dataset so that the data is in daily intervals rather than 30-minute intervals Displays the top 5 rows of the dataset Building the model 145 The following listing shows the number of rows and columns in the dataset as well as the earliest and latest dates. print(daily_df.shape) print(f'Time series starts at \{daily_df.index[0]\} \\ and ends at \{daily_df.index[-1]\}') In the output, you can now see that your dataset has changed from 19,000 rows of 30- minute data to 409 rows of daily data, and the number of columns remains the same: (409, 48) Time series starts at 2017-11-01 00:00:00 and ends at 2018-12-14 00:00:00 HANDLING ANY MISSING VALUES IF REQUIRED As you work with the pandas library, you\'92ll come across certain gems that allow you to handle a thorny problem in an elegant manner. The line of code shown in listing 6.15 is one such instance. Basically, the data you are using in this chapter, with the exception of the first 30 days, is in good shape. It is missing a few observations, however, representing a handful of missing data points. This doesn\'92t impact the training of the data (DeepAR handles missing values well) but you can?t make predictions using data with missing values. To use this dataset to make predictions, you need to ensure there are no missing values. This section shows you how to do that. NOTE You need to make sure that the data you are using for your predictions is complete and has no missing values. The pandas fillna function has the ability to forward fill missing data. This means that you can tell fillna to fill any missing value with the preceding value. But Kiara knows that most of their locations follow a weekly cycle. If one of the warehouse sites (that are closed on weekends) is missing data for one Saturday, and you forward fill the day from Friday, your data will not be very accurate. Instead, the one-liner in the next listing replaces a missing value with the value from 7 days prior. daily_df = daily_df.fillna(daily_df.shift(7)) Table 6.5 Power usage data in daily intervals Index Site_1 Site_2 Site_3 Site_4 Site_5 Site_6 2017-11-01 1184.23 1039.1 985.95 1205.07 None 6684.4 2017-11-02 1210.9 1084.7 1013.91 1252.44 None 6894.3 2017-11-03 1247.6 1004.2 963.95 1222.4 None 6841 Listing 6.14 Viewing the data in daily intervals Listing 6.15 Replacing missing values Prints the number of rows and columns in the dataset Displays the earliest date and latest date in the dataset 146 CHAPTER 6 Forecasting your company\'92s monthly power usage With that single line of code, you have replaced missing values with values from 7 days earlier. VIEWING THE DATA Time-series data is best understood visually. To help you understand the data better, you can create charts showing the power consumption at each of the sites. The code in listing 6.16 is similar to the code you worked with in the practice notebook earlier in the chapter. The primary difference is that, instead of looping through every site, you set up a list of sites in a variable called indicies and loop through that. If you remember, in listing 6.16, you imported matplotlib.pyplot as plt. Now you can use all of the functions in plt. In line 2 of listing 6.16, you create a Matplotlib figure that contains a 2-by-5 grid. Line 3 of the listing tells Matplotlib that when you give it data to work with, it should turn the data into a single data series rather than an array. In line 4, the indices are the column numbers of the sites in the dataset. Remember that Python is zero-based, so 0 would be site 1. These 10 sites display in the 2-by-5 grid you set up in line 2. To view other sites, just change the numbers and run the cell again. Line 5 is a loop that goes through each of the indices you defined in line 4. For each item in an index, the loop adds data to the Matplotlib figure you created in line 2. Your figure contains a grid 2 charts wide by 5 charts long so it has room for 10 charts, which is the same as the number of indices. Line 6 is where you put all the data into the chart. daily_df is the dataset that holds your daily power consumption data for each of the sites. The first part of the line selects the data that you\'92ll display in the chart. Line 7 inserts the data into the plot. Lines 8 and 9 set the labels on the charts. print('Number of time series:',daily_df.shape[1]) fig, axs = plt.subplots( 5, 2, figsize=(20, 20), sharex=True) axx = axs.ravel() indices = [0,1,2,3,4,5,40,41,42,43] for i in indices: plot_num = indices.index(i) daily_df[daily_df.columns[i]].loc[ "2017-11-01":"2018-01-31"].plot( ax=axx[plot_num]) axx[plot_num].set_xlabel("date") axx[plot_num].set_ylabel("kW consumption") Listing 6.16 Creating charts to show each site over a period of months Displays the number of columns in the daily_df dataset. This is 48, the total number of sites. Creates the Matplotlib figure to hold the 10 charts (5 rows of 2 charts) Ensures that the data will be stored as a series Identifies the 10 sites you want to chart Loops through each of the sites Gets each element, one by one, from the list indices Sets the data in the plot to the site referenced by the variable indicies Sets the label for the x-axis Sets the label for the y-axis Building the model 147 Now that you can see what your data looks like, you can create your training and test datasets. 6.6.3 Part 3: Creating training and testing datasets DeepAR requires the data to be in JSON format. JSON is a very common data format that can be read by people and by machines. A hierarchical structure that you commonly use is the folder system on your computer. When you store documents relating to different projects you are working on, you might create a folder for each project and put the documents relating to each project in that folder. That is a hierarchical structure. In this chapter, you will create a JSON file with a simple structure. Instead of project folders holding project documents (like the previous folder example), each element in your JSON file will hold daily power consumption data for one site. Also, each element will hold two additional elements, as shown in listing 6.17. The first element is start, which contains the date, and the second is target, which contains each day\'92s power consumption data for the site. Because your dataset covers 409 days, there are 409 elements in the target element. \{ "start": "2017-11-01 00:00:00", "target": [ 1184.23, 1210.9000000000003, 1042.9000000000003, ... 1144.2500000000002, 1225.1299999999999 ] \} To create the JSON file, you need to take the data through a few transformations: 
\f2 \uc0\u61601 
\f0  Convert the data from a DataFrame to a list of series 
\f2 \uc0\u61601 
\f0  Withhold 30 days of data from the training dataset so you don\'92t train the model on data you are testing against 
\f2 \uc0\u61601 
\f0  Create the JSON files The first transformation is converting the data from a DataFrame to a list of data series, with each series containing the power consumption data for a single site. The following listing shows how to do this. daily_power_consumption_per_site = [] for column in daily_df.columns: Listing 6.17 Sample JSON file Listing 6.18 Converting a DataFrame to a list of series Creates an empty list to hold the columns in the DataFrame Loops through the columns in the DataFrame 148 CHAPTER 6 Forecasting your company\'92s monthly power usage site_consumption = site_consumption.fillna(0) daily_power_consumption_per_site.append( site_consumption) print(f'Time series covers \\ \{len(daily_power_consumption_per_site[0])\} days.') print(f'Time series starts at \\ \{daily_power_consumption_per_site[0].index[0]\}') print(f'Time series ends at \\ \{daily_power_consumption_per_site[0].index[-1]\}') In line 1 in the listing, you create a list to hold each of your sites. Each element of this list holds one column of the dataset. Line 2 creates a loop to iterate through the columns. Line 3 appends each column of data to the daily_power_consumption_per _site list you created in line 1. Lines 4 and 5 print the results so you can confirm that the conversion still has the same number of days and covers the same period as the data in the DataFrame. Next, you set a couple of variables that will help you keep the time periods and intervals consistent throughout the notebook. The first variable is freq, which you set to D. D stands for day, and it means that you are working with daily data. If you were working with hourly data, you\'92d use H, and monthly data is M. You also set your prediction period. This is the number of days out that you want to predict. For example, in this notebook, the training dataset goes from November 1, 2017, to October 31, 2018, and you are predicting power consumption for November 2018. November has 30 days, so you set the prediction_length to 30. Once you have set the variables in lines 1 and 2 of listing 6.19, you then define the start and end dates in a timestamp format. Timestamp is a data format that stores dates, times, and frequencies as a single object. This allows for easy transformation from one frequency to another (such as daily to monthly) and easy addition and subtraction of dates and times. In line 3, you set the start_date of the dataset to November 1, 2017, and the end date of the training dataset to the end of October 2018. The end date of the test dataset is 364 days later, and the end date of the training dataset is 30 days after that. Notice that you can simply add days to the original timestamp, and the dates are automatically calculated. freq = 'D' prediction_length = 30 start_date = pd.Timestamp( "2017-11-01 00:00:00", freq=freq) Listing 6.19 Setting the length of the prediction period Replaces any missing values with zeros Appends the column to the list Prints the number of days Prints the start date of the first site Prints the end date of the first site Sets frequency of the time series to day Sets prediction length to 30 days Sets start_date as November 1, 2017 Building the model 149 end_training = start_date + 364 end_testing = end_training + prediction_length print(f'End training: \{end_training\}, End testing: \{end_testing\}') The DeepAR JSON input format represents each time series as a JSON object. In the simplest case (which you will use in this chapter), each time series consists of a start timestamp (start) and a list of values (target). The JSON input format is a JSON file that shows the daily power consumption for each of the 48 sites that Kiara is reporting on. The DeepAR model requires two JSON files: the first is the training data and the second is the test data. Creating JSON files is a two-step process. First, you create a Python dictionary with a structure identical to the JSON file, and then you convert the Python dictionary to JSON and save the file. To create the Python dictionary format, you loop through each of the daily_power _consumption_per_site lists you created in listing 6.18 and set the start variable and target list. Listing 6.20 uses a type of Python loop called a list comprehension. The code between the open and close curly brackets (line 2 and 5 of listing 6.20) marks the start and end of each element in the JSON file shown in listing 6.17. The code in lines 3 and 4 inserts the start date and a list of days from the training dataset. Lines 1 and 7 mark the beginning and end of the list comprehension. The loop is described in line 6. The code states that the list ts will be used to hold each site as it loops through the daily_power_consumption_per_site list. That is why, in line 4, you see the variable ts[start_date:end_training]. The code ts[start_date:end _training] is a list that contains one site and all of the days in the range start_date to end_training that you set in listing 6.19. training_data = [ \{ "start": str(start_date), "target": ts[ start_date:end_training].tolist() \} for ts in timeseries ] test_data = [ \{ "start": str(start_date), "target": ts[ start_date:end_testing].tolist() \} for ts in timeseries ] Listing 6.20 Creating a Python dictionary in same structure as a JSON file Sets the end of the training dataset to October 31, 2018 Sets the end of the test dataset to November 30, 2018 Creates a list of dictionary objects to hold the training data Sets the start of each Sets the dictionary object start date Creates a list of power consumption training data for one site Sets the end of each dictionary object List comprehension loop Sets the end of the training data Creates a list of power consumption test data for one site 150 CHAPTER 6 Forecasting your company\'92s monthly power usage Now that you have created two Python dictionaries called test_data and training_data, you need to save these as JSON files on S3 for DeepAR to work with. To do this, create a helper function that converts a Python dictionary to JSON and then apply that function to the test_data and training_data dictionaries, as shown in the following listing. def write_dicts_to_s3(path, data): with s3.open(path, 'wb') as f: for d in data: f.write(json.dumps(d).encode("utf-8")) f.write("\\n".encode('utf-8')) write_dicts_to_s3( f'\{s3_data_path\}/train/train.json', training_data) write_dicts_to_s3( f'\{s3_data_path\}/test/test.json', test_data) Your training and test data are now stored on S3 in JSON format. With that, the data is in a SageMaker session and you are ready to start training the model. 6.6.4 Part 4: Training the model Now that you have saved the data on S3 in JSON format, you can start training the model. As shown in the following listing, the first step is to set up some variables that you will hand to the estimator function that will build the model. s3_output_path = \\ f's3://\{data_bucket\}/\{subfolder\}/output' sess = sagemaker.Session() image_name = sagemaker.amazon.amazon_estimator.get_image_uri( sess.boto_region_name, "forecasting-deepar", "latest") Next, you hand the variables to the estimator (listing 6.23). This sets up the type of machine that SageMaker will fire up to create the model. You will use a single instance of a c5.2xlarge machine. SageMaker creates this machine, starts it, builds the model, and shuts it down automatically. The cost of this machine is about US$0.47 per hour. It will take about 3 minutes to create the model, which means it will cost only a few cents. Listing 6.21 Saving the JSON files to S3 Listing 6.22 Setting up a server to train the model Creates a function that writes the dictionary data to S3 Opens an S3 file object Loops through the data Writes the dictionary object in JSON format Writes a newline character so that each dictionary object starts on a new line Applies the function to the training data Applies the function to the test data Sets the path to save the machine learning model Creates a variable to hold the SageMaker session Tells AWS to use the forecasting-deepar image to create the model Building the model 151 estimator = sagemaker.estimator.Estimator( sagemaker_session=sess, image_name=image_name, role=role, train_instance_count=1, train_instance_type='ml.c5.2xlarge', base_job_name='ch6-energy-usage', output_path=s3_output_path ) Listing 6.23 Setting up an estimator to hold training parameters Richie\'92s note on SageMaker instance types Throughout this book, you will notice that we have chosen to use the instance type ml.m4.xlarge for all training and inference instances. The reason behind this decision was simply that usage of these instance types was included in Amazon\'92s free tier at the time of writing. (For details on Amazon\'92s current inclusions in the free tier, see https://aws.amazon.com/free.) For all the examples provided in this book, this instance is more than adequate. But what should you be using in your workplace if your problem is more complex and/or your dataset is much larger than the ones we have presented? There are no hard and fast rules, but here are a few guidelines: 
\f2 \uc0\u61601 
\f0  See the SageMaker examples on the Amazon website for the algorithm you are using. Start with the Amazon example as your default. 
\f2 \uc0\u61601 
\f0  Make sure you calculate how much your chosen instance type is actually costing you for training and inference (https://aws.amazon.com/sagemaker/ pricing). 
\f2 \uc0\u61601 
\f0  If you have a problem with training or inference cost or time, don\'92t be afraid to experiment with different instance sizes. 
\f2 \uc0\u61601 
\f0  Be aware that quite often a very large and expensive instance can actually cost less to train a model than a smaller one, as well as run in much less time. 
\f2 \uc0\u61601 
\f0  XGBoost runs in parallel when training on a compute instance but does not benefit at all from a GPU instance, so don\'92t waste time on a GPU-based instance (p3 or accelerated computing) for training or inference. However, feel free to try an m5.24xlarge or m4.16xlarge in training. It might actually be cheaper! 
\f2 \uc0\u61601 
\f0  Neural-net-based models will benefit from GPU instances in training, but these usually should not be required for inference as these are exceedingly expensive. 
\f2 \uc0\u61601 
\f0  Your notebook instance is most likely to be memory constrained if you use it a lot, so consider an instance with more memory if this becomes a problem for you. Just be aware that you are paying for every hour the instance is running even if you are not using it! Creates the estimator variable for the model Applies the SageMaker Sets the session image Gives the image a role that allows it to run Sets up a single instance for the training machine Sets the size of the machine the model will be trained on Nominates a name Saves the model to the output for the job location you set up on S3 152 CHAPTER 6 Forecasting your company\'92s monthly power usage Once you set up the estimator, you then need to set its parameters. SageMaker exposes several parameters for you. The only two that you need to change are the last two parameters shown in lines 7 and 8 of listing 6.24: context_length and prediction_length. The context length is the minimum period of time that will be used to make a prediction. By setting this to 90, you are saying that you want DeepAR to use 90 days of data as a minimum to make its predictions. In business settings, this is typically a good value as it allows for the capture of quarterly trends. The prediction length is the period of time you are predicting. For this notebook, you are predicting November data, so you use the prediction_length of 30 days. estimator.set_hyperparameters( time_freq=freq, epochs="400", early_stopping_patience="40", mini_batch_size="64", learning_rate="5E-4", context_length="90", prediction_length=str(prediction_length) ) Now you train the model. This can take 5 to 10 minutes. %%time data_channels = \{ "train": "\{\}/train/".format(s3_data_path), "test": "\{\}/test/".format(s3_data_path) \} estimator.fit(inputs=data_channels, wait=True) After this code runs, the model is trained, so now you can host it on SageMaker so it is ready to make decisions. 6.6.5 Part 5: Hosting the model Hosting the model involves several steps. First, in listing 6.26, you delete any existing endpoints you have so you don\'92t end up paying for a bunch of endpoints you aren\'92t using. Listing 6.24 Inserting parameters for the estimator Listing 6.25 Training the model Sets the hyperparameters Sets the frequency to daily Sets the epochs to 400 (leave this value as is) Sets the early stopping to 40 (leave this value as is) Sets the batch size to 64 (leave this value as is) Sets the learning rate to 0.0005 (the decimal conversion of the exponential value 5E-4) Sets the context length to 90 days Sets the prediction length to 30 days Pulls in the training and test data to create the model Training data Test data Runs the estimator function that creates the model Building the model 153 endpoint_name = 'energy-usage' try: sess.delete_endpoint( sagemaker.predictor.RealTimePredictor( endpoint=endpoint_name).endpoint, delete_endpoint_config=True) print( 'Warning: Existing endpoint deleted to make way for new endpoint.') from time import sleep sleep(10) except: pass Next is a code cell that you don\'92t really need to know anything about. This is a helper class prepared by Amazon to allow you to review the results of the DeepAR model as a pandas DataFrame rather than as JSON objects. It is a good bet that in the future they will make this code part of the M library. For now, just run it by clicking into the cell and pressing C+R. You are now at the stage where you set up your endpoint to make predictions (listing 6.27). You will use an m5.large machine as it represents a good balance of power and price. As of March 2019, AWS charges 13.4 US cents per hour for the machine. So if you keep the endpoint up for a day, the total cost will be US$3.22. %%time predictor = estimator.deploy( initial_instance_count=1, instance_type='ml.m5.large', predictor_cls=DeepARPredictor, endpoint_name=endpoint_name) You are ready to start making predictions. 6.6.6 Part 6: Making predictions and plotting results In the remainder of the notebook, you will do three things: 
\f2 \uc0\u61601 
\f0  Run a prediction for a month that shows the 50th percentile (most likely) prediction and also displays the range of prediction between two other percentiles. For example, if you want to show an 80% confidence range, the prediction will also show you the lower and upper range that falls within an 80% confidence level. Listing 6.26 Deleting existing endpoints Listing 6.27 Setting up the predictor class Deploys the estimator to a variable named predictor Sets it up on a single machine Uses an m5.large machine Uses the DeepARPredictor class to return the results as a pandas DataFrame Names the endpoint energy-usage 154 CHAPTER 6 Forecasting your company\'92s monthly power usage 
\f2 \uc0\u61601 
\f0  Graph the results so that you can easily describe the results. 
\f2 \uc0\u61601 
\f0  Run a prediction across all the results for the data in November 2018. This data was not used to train the DeepAR model, so it will demonstrate how accurate the model is. PREDICTING POWER CONSUMPTION FOR A SINGLE SITE To predict power consumption for a single site, you just pass the site details to the predictor function. In the following listing, you are running the predictor against data from site 1. predictor.predict(ts=daily_power_consumption_per_site[0] [start_date+30:end_training], quantiles=[0.1, 0.5, 0.9]).head() Table 6.6 shows the result of running the prediction against site 1. The first column shows the day, and the second column shows the prediction from the 10th percentile of results. The third column shows the 50th percentile prediction, and the last column shows the 90th percentile prediction. Now that you have a way to generate predictions, you can chart the predictions. CHARTING THE PREDICTED POWER CONSUMPTION FOR A SINGLE SITE The code in listing 6.29 shows a function that allows you to set up charting. It is similar to the code you worked with in the practice notebook but has some additional complexities that allow you to display graphically the range of results. def plot( predictor, target_ts, end_training=end_training, plot_weeks=12, confidence=80 ): print(f"Calling served model to generate predictions starting from \\ \{end_training\} to \{end_training+prediction_length\}") Listing 6.28 Setting up the predictor class Table 6.6 Predicting power usage data for site 1 of Kiara\'92s companies Day 0.1 0.5 0.9 2018-11-01 1158.509766 1226.118042 1292.315430 2018-11-02 1154.938232 1225.540405 1280.479126 2018-11-03 1119.561646 1186.360962 1278.330200 Listing 6.29 Setting up a function that allows charting Runs the predictor function against the first site Sets the argument for the plot function Building the model 155 low_quantile = 0.5 - confidence * 0.005 up_quantile = confidence * 0.005 + 0.5 plot_history = plot_weeks * 7 fig = plt.figure(figsize=(20, 3)) ax = plt.subplot(1,1,1) prediction = predictor.predict( ts=target_ts[:end_training], quantiles=[ low_quantile, 0.5, up_quantile]) target_section = target_ts[ end_training-plot_history:\\ end_training+prediction_length] target_section.plot( color="black", label='target') ax.fill_between( prediction[str(low_quantile)].index, prediction[str(low_quantile)].values, prediction[str(up_quantile)].values, color="b", alpha=0.3, label=f'\{confidence\}% confidence interval' ) prediction["0.5"].plot( color="b", label='P50') ax.legend(loc=2) ax.set_ylim( target_section.min() * 0.5, target_section.max() * 1.5) Line 1 of the code creates a function called plot that lets you create a chart of the data for each site. The plot function takes three arguments: 
\f2 \uc0\u61601 
\f0  predictor\'97The predictor that you ran in listing 6.28, which generates predictions for the site 
\f2 \uc0\u61601 
\f0  plot_weeks\'97The number of weeks you want to display in your chart 
\f2 \uc0\u61601 
\f0  confidence\'97The confidence level for the range that is displayed in the chart In lines 2 and 3 in listing 6.29, you calculate the confidence range you want to display from the confidence value you entered as an argument in line 1. Line 4 calculates the number of days based on the plot_weeks argument. Lines 5 and 6 set the size of the plot and the subplot. (You are only displaying a single plot.) Line 7 runs the prediction function on the site. Lines 8 and 9 set the date range for the chart and the color of the actual line. In line 10, you set the prediction range that will display in the chart, Calculates the lower range Calculates the upper range Calculates the number of days based on the plot_weeks argument Sets the size of the chart Sets a single chart to display The prediction function Sets the actual values Sets the color of the line for the actual values Sets the color of the range Sets the color of the prediction Creates the legend Sets the scale 156 CHAPTER 6 Forecasting your company\'92s monthly power usage and in line 11 you define the prediction line. Finally, lines 12 and 13 set the legend and the scale of the chart. NOTE In this function to set up charting, we use global variables that were set earlier in the notebook. This is not ideal but keeps the function a little simpler for the purposes of this book. Listing 6.30 runs the function. The chart shows the actual and prediction data for a single site. This listing uses site 34 as the charted site, shows a period of 8 weeks before the 30-day prediction period, and defines a confidence level of 80%. site_id = 34 plot_weeks = 8 confidence = 80 plot( predictor, target_ts=daily_power_consumption_per_site[ site_id][start_date+30:], plot_weeks=plot_weeks, confidence=confidence ) Figure 6.8 shows the chart you have produced. You can use this chart to show the predicted usage patterns for each of Kiara\'92s sites. Listing 6.30 Running the function that creates the chart Sets the site for charting Sets the number of weeks to include in the chart Sets the confidence level at 80% Runs the plot function Figure 6.8 Chart showing predicted versus actual power consumption for November 2018, for one of Kiara\'92s sites Building the model 157 CALCULATING THE ACCURACY OF THE PREDICTION ACROSS ALL SITES Now that you can see one of the sites, it\'92s time to calculate the error across all the sites. To express this, you use a Mean Absolute Percentage Error (MAPE). This function takes the difference between the actual value and the predicted value and divides it by the actual value. For example, if an actual value is 50 and the predicted value is 45, you\'92d subtract 45 from 50 to get 5, and then divide by 50 to get 0.1. Typically, this is expressed as a percentage, so you multiply that number by 100 to get 10%. So the MAPE of an actual value of 50 and a predicted value of 45 is 10%. The first step in calculating the MAPE is to run the predictor across all the data for November 2018 and get the actual values (usages) for that month. Listing 6.31 shows how to do this. In line 5, you see a function that we haven\'92t used yet in the book: the zip function. This is a very useful piece of code that allows you to loop through two lists concurrently and do interesting things with the paired items from each list. In this listing, the interesting thing you\'92ll do is to print the actual value compared to the prediction. predictions= [] for i, ts in enumerate( daily_power_consumption_per_site): print(i, ts[0]) predictions.append( predictor.predict( ts=ts[start_date+30:end_training] )['0.5'].sum()) usages = [ts[end_training+1:end_training+30].sum() \\ for ts in daily_power_consumption_per_site] for p,u in zip(predictions,usages): print(f'Predicted \{p\} kwh but usage was \{u\} kwh,') The next listing shows the code that calculates the MAPE. Once the function is defined, you will use it to calculate the MAPE. def mape(y_true, y_pred): y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 The following listing runs the MAPE function across all the usages and predictions for the 30 days in November 2018 and returns the mean MAPE across all the days. print(f'MAPE: \{round(mape(usages, predictions),1)\}%') Listing 6.31 Running the predictor Listing 6.32 Calculating the Mean Absolute Percentage Error (MAPE) Listing 6.33 Running the MAPE function Loops through daily site power consumption Runs predictions for the month of November Gets the usages Prints usages and predictions 158 CHAPTER 6 Forecasting your company\'92s monthly power usage The MAPE across all the days is 5.7%, which is pretty good, given that you have not yet added weather data. You\'92ll do that in chapter 7. Also in chapter 7, you\'92ll get to work with a longer period of data so the DeepAR algorithm can begin to detect annual trends. 6.7 Deleting the endpoint and shutting down your notebook instance It is important that you shut down your notebook instance and delete your endpoint. We don\'92t want you to get charged for SageMaker services that you\'92re not using. 6.7.1 Deleting the endpoint Appendix D describes how to shut down your notebook instance and delete your endpoint using the SageMaker console, or you can do that with the following code. # Remove the endpoint (optional) # Comment out this cell if you want the endpoint to persist after Run All sagemaker.Session().delete_endpoint(rcf_endpoint.endpoint) To delete the endpoint, uncomment the code in the listing, then click C+R to run the code in the cell. 6.7.2 Shutting down the notebook instance To shut down the notebook, go back to your browser tab where you have SageMaker open. Click the Notebook Instances menu item to view all of your notebook instances. Select the radio button next to the notebook instance name as shown in figure 6.9, then select Stop from the Actions menu. It takes a couple of minutes to shut down. Listing 6.34 Deleting the notebook 1. Select the radio button. 2. Select Stop. Figure 6.9 Shutting down the notebook Summary 159 6.8 Checking to make sure the endpoint is deleted If you didn\'92t delete the endpoint using the notebook (or if you just want to make sure it is deleted), you can do this from the SageMaker console. To delete the endpoint, click the radio button to the left of the endpoint name, then click the Actions menu item and click Delete in the menu that appears. When you have successfully deleted the endpoint, you will no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you see the text \'93There are currently no resources\'94 displayed on the Endpoints page (figure 6.10). Kiara can now predict power consumption for each site with a 5.7% MAPE and, as importantly, she can take her boss through the charts to show what she is predicting to occur in each site. Summary 
\f2 \uc0\u61601 
\f0  Time-series data consists of a number of observations at particular intervals. You can visualize time-series data as line charts. 
\f2 \uc0\u61601 
\f0  Jupyter notebooks and the pandas library are excellent tools for transforming time-series data and for creating line charts of the data. 
\f2 \uc0\u61601 
\f0  Matplotlib is a Python charting library. 
\f2 \uc0\u61601 
\f0  Instructions to Jupyter begin with a % symbol. When you see a command in a Jupyter notebook that starts with % or with %%, it\'92s known as a magic command. 
\f2 \uc0\u61601 
\f0  A for loop is the most common type of loop you\'92ll use in data analysis and machine learning. The enumerate function lets you keep track of how many times you have looped through a list. 
\f2 \uc0\u61601 
\f0  A neural network (sometimes referred to as deep learning) is an example of supervised machine learning. 
\f2 \uc0\u61601 
\f0  You can build a neural network using SageMaker\'92s DeepAR model. Endpoint successfully deleted Figure 6.10 Endpoint deleted 160 CHAPTER 6 Forecasting your company\'92s monthly power usage 
\f2 \uc0\u61601 
\f0  DeepAR is Amazon\'92s time-series neural network algorithm that takes as its input related types of time-series data and automatically combines the data into a global model of all time series in a dataset to predict future events. 
\f2 \uc0\u61601 
\f0  You can use DeepAR to predict power consumption. 161 Improving your company\'92s monthly power usage forecast In chapter 6, you worked with Kiara to develop an AWS SageMaker DeepAR model to predict power consumption across her company\'92s 48 sites. You had just a bit more than one year\'92s data for each of the sites, and you predicted the temperature for November 2018 with an average percentage error of less that 6%. Amazing! Let\'92s expand on this scenario by adding additional data for our analysis and filling in any missing values. First, let\'92s take a deeper look at DeepAR. 7.1 DeepAR\'92s ability to pick up periodic events The DeepAR algorithm was able to identify patterns such as weekly trends in our data from chapter 6. Figure 7.1 shows the predicted and actual usage for site 33 in November. This site follows a consistent weekly pattern. This chapter covers 
\f2 \uc0\u61601 
\f0  Adding additional data to your analysis 
\f2 \uc0\u61601 
\f0  Using pandas to fill in missing values 
\f2 \uc0\u61601 
\f0  Visualizing your time-series data 
\f2 \uc0\u61601 
\f0  Using a neural network to generate forecasts 
\f2 \uc0\u61601 
\f0  Using DeepAR to forecast power consumption 162 CHAPTER 7 Improving your company\'92s monthly power usage forecast You and Kiara are heroes. The company newsletter included a two-page spread showing you and Kiara with a large printout of your predictions for December. Unfortunately, when January came around, anyone looking at that photo would have noticed that your predictions weren\'92t that accurate for December. Fortunately for you and Kiara, not many people noticed because most staff take some holiday time over Christmas, and some of the sites had a mandatory shutdown period. \'93Wait a minute!\'94 You and Kiara said at the same time as you were discussing why your December predictions were less accurate. \'93With staff taking time off and mandatory shut-downs, it\'92s no wonder December was way off.\'94 When you have rare but still regularly occurring events like a Christmas shutdown in your time-series data, your predictions will still be accurate, provided you have enough historical data for the machine learning model to pick up the trend. You and Kiara would need several years of power consumption data for your model to pick up a Christmas shutdown trend. But you don\'92t have this option because the smart meters were only installed in November 2017. So what do you do? Fortunately for you (and Kiara), SageMaker DeepAR is a neural network that is particularly good at incorporating several different time-series datasets into its forecasting. And these can be used to account for events in your time-series forecasting that your time-series data can\'92t directly infer. To demonstrate how this works, figure 7.2 shows time-series data covering a typical month. The x-axis shows days per month. The y-axis is the amount of power consumed on each day. The shaded area is the predicted power consumption with an 80% Predicted power consumption (80% confidence) Actual power consumption Figure 7.1 Predicted versus actual consumption from site 33 using the DeepAR model you built in chapter 6 Figure 7.2 Actual versus predicted usage during a normal month DeepAR\'92s greatest strength: Incorporating related time series 163 confidence interval. An 80% confidence interval means that 4 out of every 5 days will fall within this range. The black line shows the actual power consumption for that day. In figure 7.2, you can see that the actual power consumption was within the confidence interval for every day of the month. Figure 7.3 shows a month with a shutdown from the 10th to the 12th day of the month. You can see that the actual power consumption dropped on these days, but the predicted power consumption did not anticipate this. There are three possible reasons why the power consumption data during this shutdown was not correctly predicted. First, the shutdown is a regularly occurring event, but there is not enough historical data for the DeepAR algorithm to pick up the recurring event. Second, the shutdown is not a recurring event (and so can\'92t be picked up in the historical data) but is an event that can be identified through other datasets. An example of this is a planned shutdown where Kiara\'92s company is closing a site for a few days in December. Although the historical dataset won\'92t show the event, the impact of the event on power consumption can be predicted if the model incorporated planned staff schedules as one of its time series. We\'92ll discuss this more in the next section. Finally, the shutdown is not planned, and there is no dataset that could be incorporated to show the shutdown. An example of this is a work stoppage due to an employee strike. Unless your model can predict labor activism, there is not much your machine learning model can do to predict power consumption during these periods. 7.2 DeepAR\'92s greatest strength: Incorporating related time series To help the DeepAR model predict trends, you need to provide it with additional data that shows trends. As an example, you know that during the shutdown periods, only a handful of staff are rostered. If you could feed this data into the DeepAR algorithm, then it could use this information to predict power consumption during shutdown periods.1 1 You can read more about DeepAR on the AWS site: https://docs.aws.amazon.com/sagemaker/latest/dg/ deepar.html. Shutdown period Figure 7.3 Actual versus predicted usage during a month with a shutdown 164 CHAPTER 7 Improving your company\'92s monthly power usage forecast Figure 7.4 shows the number of staff rostered in a month during a shutdown. You can see that for most days, there are between 10 and 15 staff members at work, but on the 10th, 11th, and 12th, there are only 4 to 6 staff members. If you could incorporate this time series into the DeepAR model, you would better predict upcoming power consumption. Figure 7.5 shows the prediction when you use both historical consumption and rostering data in your DeepAR model. In this chapter, you\'92ll learn how to incorporate additional datasets into your DeepAR model to improve the accuracy of the model in the face of known upcoming events that are either not periodic or are periodic, but you don\'92t have enough historical data for the model to incorporate into its predictions. 7.3 Incorporating additional datasets into Kiara\'92s power consumption model In chapter 6, you helped Kiara build a DeepAR model that predicted power consumption across each of the 41 sites owned by her company. The model worked well when predicting power consumption in November, but performed less well when predicting December\'92s consumption because some of the sites were on reduced operating hours or shut down altogether. Additionally, you noticed that there were seasonal fluctuations in power usage that you attributed to changes in temperature, and you noticed that different types of sites had different usage patterns. Some types of sites were closed every weekend, whereas others operated consistently regardless of the day of the week. After discussing this Figure 7.4 Number of staff rostered during a month with a shutdown Figure 7.5 Power consumption predictions incorporating historical and staff roster data Getting ready to build the model 165 with Kiara, you realized that some of the sites were retail sites, whereas others were industrial or transport-related areas. In this chapter, the notebook you\'92ll build will incorporate this data. Specifically, you\'92ll add the following datasets to the power consumption metering data you used in chapter 6: 
\f2 \uc0\u61601 
\f0  Site categories\'97Indicates retail, industrial, or transport site 
\f2 \uc0\u61601 
\f0  Site holidays\'97Indicates whether a site has a planned shutdown 
\f2 \uc0\u61601 
\f0  Site maximum temperatures\'97Lists the maximum temperature forecast for each site each day Then you\'92ll train the model using these three datasets. 7.4 Getting ready to build the model As in previous chapters, you need to do the following to set up another notebook in SageMaker and fine tune your predictions: 1 From S3, download the notebook we prepared for this chapter. 2 Set up the folder to run the notebook on AWS SageMaker. 3 Upload the notebook to AWS SageMaker. 4 Download the datasets from your S3 bucket. 5 Create a folder in your S3 bucket to store the datasets. 6 Upload the datasets to your AWS S3 bucket. Given that you\'92ve followed these steps in each of the previous chapters, we\'92ll move quickly through them in this chapter. 7.4.1 Downloading the notebook we prepared We prepared the notebook you\'92ll use in this chapter. You can download it from this location: https://s3.amazonaws.com/mlforbusiness/ch07/energy_consumption_additional_ datasets.ipynb Different types of datasets The three datasets used in this chapter can be classified into two types of data: 
\f2 \uc0\u61601 
\f0  Categorical\'97Information about the site that doesn\'92t change. The dataset site categories, for example, contains categorical data. (A site is a retail site and will likely always be a retail site.) 
\f2 \uc0\u61601 
\f0  Dynamic\'97Data that changes over time. Holidays and forecasted maximum temperatures are examples of dynamic data. When predicting power consumption for the month of December, you\'92ll use a schedule of planned holidays for December and the forecasted temperature for that month. 166 CHAPTER 7 Improving your company\'92s monthly power usage forecast Save this file on your computer. In step 3, you\'92ll upload it to SageMaker. 7.4.2 Setting up the folder on SageMaker Go to AWS SageMaker at https://console.aws.amazon.com/sagemaker/home, and select Notebook Instances from the left-hand menu. If your instance is stopped, you\'92ll need to start it. Once it is started, click Open Jupyter. This opens a new tab and shows you a list of folders in SageMaker. If you have been following along in earlier chapters, you will have a folder for each of the earlier chapters. Create a new folder for this chapter. We\'92ve called our folder ch07. 7.4.3 Uploading the notebook to SageMaker Click the folder you\'92ve just created, and click Upload to upload the notebook. Select the notebook you downloaded in step 1, and upload it to SageMaker. Figure 7.6 shows what your SageMaker folder might look like after uploading the notebook. 7.4.4 Downloading the datasets from the S3 bucket We stored the datasets for this chapter in one of our S3 buckets. You can download each of the datasets by clicking the following links: 
\f2 \uc0\u61601 
\f0  Meter data\'97https://s3.amazonaws.com/mlforbusiness/ch07/meter_data_daily.csv 
\f2 \uc0\u61601 
\f0  Site categories\'97https://s3.amazonaws.com/mlforbusiness/ch07/site_categories.csv 
\f2 \uc0\u61601 
\f0  Site holidays\'97https://s3.amazonaws.com/mlforbusiness/ch07/site_holidays.csv 
\f2 \uc0\u61601 
\f0  Site maximums\'97https://s3.amazonaws.com/mlforbusiness/ch07/site_maximums .csv The power consumption data you\'92ll use in this chapter is provided by BidEnergy (http://www.bidenergy.com), a company that specializes in power-usage forecasting and in minimizing power expenditure. The algorithms used by BidEnergy are more sophisticated than you\'92ll see in this chapter, but you\'92ll still get a feel for how machine Uploaded notebook on SageMaker Figure 7.6 Viewing the uploaded energy_consumption_additional_datasets notebook on S3 Getting ready to build the model 167 learning in general, and neural networks in particular, can be applied to forecasting problems. 7.4.5 Setting up a folder on S3 to hold your data In AWS S3, go to the bucket you created to hold your data in earlier chapters, and create another folder. You can see a list of your buckets at this link: https://s3.console.aws.amazon.com/s3/buckets The bucket we are using to hold our data is called mlforbusiness. Your bucket will be called something else (a name of your choosing). Once you click into your bucket, create a folder to store your data, naming it something like ch07. 7.4.6 Uploading the datasets to your AWS bucket After creating the folder on S3, upload the datasets you downloaded in step 4. Figure 7.7 shows what your S3 folder might look like. Uploaded datasets Figure 7.7 Viewing the uploaded CSV datasets on S3 168 CHAPTER 7 Improving your company\'92s monthly power usage forecast 7.5 Building the model With the data uploaded to S3 and the notebook uploaded to SageMaker, you can now start to build the model. As in previous chapters, you\'92ll go through the following steps: 1 Set up the notebook. 2 Import the datasets. 3 Get the data into the right shape. 4 Create training and test datasets. 5 Configure the model and build the server. 6 Make predictions and plot results. 7.5.1 Part 1: Setting up the notebook Listing 7.1 shows your notebook setup. You will need to change the values in line 1 to the name of the S3 bucket you created on S3, then change line 2 to the subfolder of that bucket where you saved the data. Line 3 sets the location of the training and test data created in this notebook, and line 4 sets the location where the model is stored. data_bucket = 'mlforbusiness' subfolder = 'ch07' s3_data_path = \\ f"s3://\{data_bucket\}/\{subfolder\}/data" s3_output_path = \\ f"s3://\{data_bucket\}/\{subfolder\}/output" The next listing imports the modules required by the notebook. This is the same as the imports used in chapter 6, so we won\'92t review these here. %matplotlib inline from dateutil.parser import parse import json import random import datetime import os import pandas as pd import boto3 import s3fs import sagemaker import numpy as np import pandas as pd import matplotlib.pyplot as plt Listing 7.1 Setting up the notebook Listing 7.2 Importing Python modules and libraries S3 bucket where the data is stored Subfolder of the bucket where the data is stored Path where training and test data will be stored Path where model will be stored Building the model 169 role = sagemaker.get_execution_role() s3 = s3fs.S3FileSystem(anon=False) s3_data_path = f"s3://\{data_bucket\}/\{subfolder\}/data" s3_output_path = f"s3://\{data_bucket\}/\{subfolder\}/output" With that done, you are now ready to import the datasets. 7.5.2 Part 2: Importing the datasets Unlike other chapters, in this notebook, you\'92ll upload four datasets for the meter, site categories, holidays, and maximum temperatures. The following listing shows how to import the meter data. daily_df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/meter_data_daily.csv', index_col=0, parse_dates=[0]) daily_df.index.name = None daily_df.head() The meter data you use in this chapter has a few more months of observations. In chapter 6, the data ranged from October 2017 to October 2018. This dataset contains meter data from November 2017 to February 2019. print(daily_df.shape) print(f'timeseries starts at \{daily_df.index[0]\} \\ and ends at \{daily_df.index[-1]\}') Listing 7.5 shows how to import the site categories data. There are three types of sites: 
\f2 \uc0\u61601 
\f0  Retail 
\f2 \uc0\u61601 
\f0  Industrial 
\f2 \uc0\u61601 
\f0  Transport category_df = pd.read_csv (f's3://\{data_bucket\}/\{subfolder\}/site_categories.csv', index_col=0 ).reset_index(drop=True) print(category_df.shape) print(category_df.Category.unique()) category_df.head() In listing 7.6, you import the holidays. Working days and weekends are marked with a 0; holidays are marked with a 1. There is no need to mark all the weekends as holidays because DeepAR can pick up that pattern from the site meter data. Although you Listing 7.3 Importing the meter data Listing 7.4 Displaying information about the meter data Listing 7.5 Displaying information about the site categories 170 CHAPTER 7 Improving your company\'92s monthly power usage forecast don\'92t have enough site data for DeepAR to identify annual patterns, DeepAR can work out the pattern if it has access to a dataset that shows holidays at each of the sites. holiday_df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/site_holidays.csv', index_col=0, parse_dates=[0]) print(holiday_df.shape) print(f'timeseries starts at \{holiday_df.index[0]\} \\ and ends at \{holiday_df.index[-1]\}') holiday_df.loc['2018-12-22':'2018-12-27'] Listing 7.7 shows the maximum temperature reached each day for each of the sites. The sites are located in Australia, so energy usage increases as temperatures rise in the summer due to air conditioning; whereas, in more temperate climates, energy usage increases more as temperatures drop below zero degrees centigrade in the winter due to heating. max_df = pd.read_csv( f's3://\{data_bucket\}/\{subfolder\}/site_maximums.csv', index_col=0, parse_dates=[0]) print(max_df.shape) print(f'timeseries starts at \{max_df.index[0]\} \\ and ends at \{max_df.index[-1]\}') With that, you are finished loading data into your notebook. To recap, for each site for each day from November 1, 2018, to February 28, 2019, you loaded data from CSV files for 
\f2 \uc0\u61601 
\f0  Energy consumption 
\f2 \uc0\u61601 
\f0  Site category (Retail, Industrial, or Transport) 
\f2 \uc0\u61601 
\f0  Holiday information (1 represents a holiday and 0 represents a working day or normal weekend) 
\f2 \uc0\u61601 
\f0  Maximum temperatures reached on the site You will now get the data into the right shape to train the DeepAR model. 7.5.3 Part 3: Getting the data into the right shape With your data loaded into DataFrames, you can now get each of the datasets ready for training the DeepAR model. The shape of each of the datasets is the same: each site is represented by a column, and each day is represented by a row. In this section, you\'92ll ensure that there are no problematic missing values in each of the columns and each of the rows. DeepAR is very good at handling missing values Listing 7.6 Displaying information about holidays for each site Listing 7.7 Displaying information about maximum temperatures for each site Building the model 171 in training data but cannot handle missing values in data it uses for predictions. To ensure that you don\'92t have annoying errors when running predictions, you fill in missing values in your prediction range. You\'92ll use November 1, 2018, to January 31, 2019, to train the data, and you\'92ll use December 1, 2018, to February 28, 2019, to test the model. This means that for your prediction range, there cannot be any missing data from December 1, 2018, to February 28, 2019. The following listing replaces any zero values with None and then checks for missing energy consumption data. daily_df = daily_df.replace([0],[None]) daily_df[daily_df.isnull().any(axis=1)].index You can see from the output that there are several days in November 2018 with missing data because that was the month the smart meters were installed, but there are no days with missing data after November 2018. This means you don\'92t need to do anything further with this dataset because there\'92s no missing prediction data. The next listing checks for missing category data. Again, there is no missing category data, so you can move on to holidays and missing maximum temperatures. print(f'\{len(category_df[category_df.isnull().any(axis=1)])\} \\ sites with missing categories.') print(f'\{len(holiday_df[holiday_df.isnull().any(axis=1)])\} \\ days with missing holidays.') The following listing checks for missing maximum temperature data. There are several days without maximum temperature values. This is a problem, but one that can be easily solved. print(f'\{len(max_df[max_df.isnull().any(axis=1)])\} \\ days with missing maximum temperatures.') The next listing uses the interpolate function to fill in missing data for a time series. In the absence of other information, the best way to infer missing values for a temperature time series like this is straight line interpolation based on time. max_df = max_df.interpolate(method='time') print(f'\{len(max_df[max_df.isnull().any(axis=1)])\} \\ days with missing maximum temperatures. Problem solved!') To ensure you are looking at data similar to the data we used in chapter 6, take a look at the data visually. In chapter 6, you learned about using Matplotlib to display multiple plots. As a refresher, listing 7.12 shows the code for displaying multiple plots. Line 1 Listing 7.8 Checking for missing energy consumption data Listing 7.9 Checking for missing category data and holiday data Listing 7.10 Checking for missing maximum temperature data Listing 7.11 Fixing missing maximum temperature data Interpolates missing values 172 CHAPTER 7 Improving your company\'92s monthly power usage forecast sets the shape of the plots as 6 rows by 2 columns. Line 2 creates a series that can be looped over. Line 3 sets which 12 sites will be displayed. And lines 4 through 7 set the content of each plot. print('Number of timeseries:',daily_df.shape[1]) fig, axs = plt.subplots( 6, 2, figsize=(20, 20), sharex=True) axx = axs.ravel() indices = [0,1,2,3,26,27,33,39,42,43,46,47] for i in indices: plot_num = indices.index(i) daily_df[daily_df.columns[i]].loc[ "2017-11-01":"2019-02-28" ].plot(ax=axx[plot_num]) axx[plot_num].set_xlabel("date") axx[plot_num].set_ylabel("kW consumption") Figure 7.8 shows the output of listing 7.12. In the notebook, you\'92ll see an additional eight charts because the shape of the plot is 6 rows and 2 columns of plots. With that complete, you can start preparing the training and test datasets. 7.5.4 Part 4: Creating training and test datasets In the previous section, you loaded each of the datasets into pandas DataFrames and fixed any missing values. In this section, you\'92ll turn the DataFrames into lists to create the training and test data. Listing 7.13 converts the category data into a list of numbers. Each of the numbers 0 to 2 represents one of these categories: Retail, Industrial, or Transport. Listing 7.12 Fixing missing maximum temperature data Sets the shape as 6 rows by 2 columns Creates a series from the 6 x 2 plot table Sets which sites will display in the plots Loops through the list of sites and gets each site number Gets the data for the plot Sets the x-axis label for the plot Sets the y-axis label for the plot Each plot shows the daily fluctuations in power consumption from November 20 7 to February 20 9. 1 1 Figure 7.8 Site plots showing temperature fluctuations from November 2017 to February 2019 Building the model 173 cats = list(category_df.Category.astype('category').cat.codes) print(cats) The next listing turns the power consumption data into a list of lists. Each site is a list, and there are 48 of these lists. usage_per_site = [daily_df[col] for col in daily_df.columns] print(f'timeseries covers \{len(usage_per_site[0])\} days.') print(f'timeseries starts at \{usage_per_site[0].index[0]\}') print(f'timeseries ends at \{usage_per_site[0].index[-1]\}') usage_per_site[0][:10] The next listing repeats this for holidays. hols_per_site = [holiday_df[col] for col in holiday_df.columns] print(f'timeseries covers \{len(hols_per_site[0])\} days.') print(f'timeseries starts at \{hols_per_site[0].index[0]\}') print(f'timeseries ends at \{hols_per_site[0].index[-1]\}') hols_per_site[0][:10] And the next listing repeats this for maximum temperatures. max_per_site = [max_df[col] for col in max_df.columns] print(f'timeseries covers \{len(max_per_site[0])\} days.') print(f'timeseries starts at \{max_per_site[0].index[0]\}') print(f'timeseries ends at \{max_per_site[0].index[-1]\}') max_per_site[0][:10] With the data formatted as lists, you can split it into training and test data and then write the files to S3. Listing 7.17 sets the start date for both testing and training as November 1, 2017. It then sets the end date for training as the end of January 2019, and the end date for testing as 28 days later (the end of February 2019). freq = 'D' prediction_length = 28 start_date = pd.Timestamp("2017-11-01", freq=freq) end_training = pd.Timestamp("2019-01-31", freq=freq) end_testing = end_training + prediction_length print(f'End training: \{end_training\}, End testing: \{end_testing\}') Listing 7.13 Converting category data to a list of numbers Listing 7.14 Converting power consumption data to a list of lists Listing 7.15 Converting holidays to a list of lists Listing 7.16 Converting maximum temperatures to a list of lists Listing 7.17 Setting the start and end dates for testing and training data Displays the first 10 days of power consumption from site 0 174 CHAPTER 7 Improving your company\'92s monthly power usage forecast Just as you did in chapter 6, you now create a simple function, shown in the next listing, that writes each of the datasets to S3. In listing 7.19, you\'92ll apply the function to the test data and training data. def write_dicts_to_s3(path, data): with s3.open(path, 'wb') as f: for d in data: f.write(json.dumps(d).encode("utf-8")) f.write("\\n".encode('utf-8')) The next listing creates the training and test datasets. DeepAR requires categorical data to be separated from dynamic features. Notice how this is done in the next listing. training_data = [ \{ "cat": [cat], "start": str(start_date), "target": ts[start_date:end_training].tolist(), "dynamic_feat": [ hols[ start_date:end_training ].tolist(), maxes[ start_date:end_training ].tolist(), ] # Note: List of lists \} for cat, ts, hols, maxes in zip( cats, usage_per_site, hols_per_site, max_per_site) ] test_data = [ \{ "cat": [cat], "start": str(start_date), "target": ts[start_date:end_testing].tolist(), "dynamic_feat": [ hols[start_date:end_testing].tolist(), maxes[start_date:end_testing].tolist(), ] # Note: List of lists \} for cat, ts, hols, maxes in zip( cats, usage_per_site, hols_per_site, max_per_site) ] Listing 7.18 Creating a function that writes data to S3 Listing 7.19 Creating training and test datasets Categorical data for site categories Dynamic data for holidays Dynamic data for maximum temperatures Building the model 175 write_dicts_to_s3(f'\{s3_data_path\}/train/train.json', training_data) write_dicts_to_s3(f'\{s3_data_path\}/test/test.json', test_data) In this chapter, you set up the notebook in a slightly different way than you have in previous chapters. This chapter is all about how to use additional datasets such as site category, holidays, and max temperatures to enhance the accuracy of time series predictions. To allow you to see the impact of these additional datasets on the prediction, we have prepared a commented-out notebook cell that creates and tests the model without using the additional datasets. If you are interested in seeing this result, you can uncomment that part of the notebook and run the entire notebook again. If you do so, you will see that, without using the additional datasets, the MAPE (Mean Average Percentage Error) for February is 20%! Keep following along in this chapter to see what it drops to when the additional datasets are incorporated into the model. 7.5.5 Part 5: Configuring the model and setting up the server to build the model Listing 7.20 sets the location on S3 where you will store the model and determines how SageMaker will configure the server that will build the model. At this point in the process, you would normally set a random seed to ensure that each run through the DeepAR algorithm generates a consistent result. At the time of this writing, there is an inconsistency in SageMaker\'92s DeepAR model\'97the functionality is not available. It doesn\'92t impact the accuracy of the results, only the consistency of the results. s3_output_path = f's3://\{data_bucket\}/\{subfolder\}/output' sess = sagemaker.Session() image_name = sagemaker.amazon.amazon_estimator.get_image_uri( sess.boto_region_name, "forecasting-deepar", "latest") data_channels = \{ "train": f"\{s3_data_path\}/train/", "test": f"\{s3_data_path\}/test/" \} Listing 7.21 is used to calculate the MAPE of the prediction. It is calculated for each day you are predicting by subtracting the predicted consumption each day from the actual consumption and dividing it by the predicted amount (and, if the number is negative, making it positive). You then take the average of all of these amounts. For example, if on three consecutive days, you predicted consumption of 1,000 kilowatts of power, and the actual consumption was 800, 900, and 1,150 kilowatts, the MAPE would be the average of (200 / 800) + (100 / 900) + (150 / 1150) divided by three. This equals 0.16, or 16%. Listing 7.20 Setting up the SageMaker session and server to create the model 176 CHAPTER 7 Improving your company\'92s monthly power usage forecast def mape(y_true, y_pred): y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 Listing 7.22 is the standard SageMaker function for creating a DeepAR model. You do not need to modify this function. You ju while in the notebook cell. st need to run it as is by clicking C+R class DeepARPredictor(sagemaker.predictor.RealTimePredictor): def __init__(self, *args, **kwargs): super().__init__( *args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs) def predict( self, ts, cat=None, dynamic_feat=None, num_samples=100, return_samples=False, quantiles=["0.1", "0.5", "0.9"]):x prediction_time = ts.index[-1] + 1 quantiles = [str(q) for q in quantiles] req = self.__encode_request( ts, cat, dynamic_feat, num_samples, return_samples, quantiles) res = super(DeepARPredictor, self).predict(req) return self.__decode_response( res, ts.index.freq, prediction_time, return_samples) def __encode_request( self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles): instance = series_to_dict( ts, Listing 7.21 Calculating MAPE Listing 7.22 The DeepAR predictor function used in chapter 6 Building the model 177 cat if cat is not None else None, dynamic_feat if dynamic_feat else None) configuration = \{ "num_samples": num_samples, "output_types": [ "quantiles", "samples"] if return_samples else ["quantiles"], "quantiles": quantiles \} http_request_data = \{ "instances": [instance], "configuration": configuration \} return json.dumps(http_request_data).encode('utf-8') def __decode_response( self, response, freq, prediction_time, return_samples): predictions = json.loads( response.decode('utf-8'))['predictions'][0] prediction_length = len(next(iter( predictions['quantiles'].values() ))) prediction_index = pd.DatetimeIndex( start=prediction_time, freq=freq, periods=prediction_length) if return_samples: dict_of_samples = \{ 'sample_' + str(i): s for i, s in enumerate( predictions['samples']) \} else: dict_of_samples = \{\} return pd.DataFrame( data=\{**predictions['quantiles'], **dict_of_samples\}, index=prediction_index) def set_frequency(self, freq): self.freq = freq def encode_target(ts): return [x if np.isfinite(x) else "NaN" for x in ts] def series_to_dict(ts, cat=None, dynamic_feat=None): # Given a pandas.Series, returns a dict encoding the timeseries. obj = \{"start": str(ts.index[0]), "target": encode_target(ts)\} if cat is not None: obj["cat"] = cat if dynamic_feat is not None: obj["dynamic_feat"] = dynamic_feat return obj 178 CHAPTER 7 Improving your company\'92s monthly power usage forecast Just as in chapter 6, you now need to set up the estimator and then set the parameters for the estimator. SageMaker exposes several parameters for you. The only two that you need to change are the first two parameters shown in lines 1 and 2 of listing 7.23: context_length and prediction_length. The context length is the minimum period of time that will be used to make a prediction. By setting this value to 90, you are saying that you want DeepAR to use 90 days of data as a minimum to make its predictions. In business settings, this is typically a good value because it allows for the capture of quarterly trends. The prediction length is the period of time you are predicting. In this notebook, you are predicting February data, so you use the prediction_length of 28 days. %%time estimator = sagemaker.estimator.Estimator( sagemaker_session=sess, image_name=image_name, role=role, train_instance_count=1, train_instance_type='ml.c5.2xlarge', # $0.476 per hour as of Jan 2019. base_job_name='ch7-energy-usage-dynamic', output_path=s3_output_path ) estimator.set_hyperparameters( context_length="90", prediction_length=str(prediction_length), time_freq=freq, epochs="400", early_stopping_patience="40", mini_batch_size="64", learning_rate="5E-4", num_dynamic_feat=2, ) estimator.fit(inputs=data_channels, wait=True) Listing 7.24 creates the endpoint you\'92ll use to test the predictions. In the next chapter, you\'92ll learn how to expose that endpoint to the internet, but for this chapter, just like the preceding chapters, you\'92ll hit the endpoint using code in the notebook. endpoint_name = 'energy-usage-dynamic' try: sess.delete_endpoint( sagemaker.predictor.RealTimePredictor( Listing 7.23 Setting up the estimator Listing 7.24 Setting up the endpoint Sets the context length to 90 days Sets the prediction length to 28 days Sets the frequency to daily Sets the epochs to 400 (leave this value as is) Sets the early stopping to 40 (leave this value as is) Sets the batch size to 64 (leave this value as is) Sets the learning rate to 0.0005 (the decimal conversion of the exponential value 5E-4) Sets the number of dynamic features to 2 for holidays and temperature (leave this as is) Building the model 179 endpoint=endpoint_name).endpoint) print( 'Warning: Existing endpoint deleted to make way for new endpoint.') from time import sleep sleep(30) except: pass Now it\'92s time to build the model. The following listing creates the model and assigns it to the variable predictor. %%time predictor = estimator.deploy( initial_instance_count=1, instance_type='ml.m5.large', predictor_cls=DeepARPredictor, endpoint_name=endpoint_name) 7.5.6 Part 6: Making predictions and plotting results Once the model is built, you can run the predictions against each of the days in February. First, however, you\'92ll test the predictor as shown in the next listing. predictor.predict( cat=[cats[0]], ts=usage_per_site[0][start_date+30:end_training], dynamic_feat=[ hols_per_site[0][start_date+30:end_training+28].tolist(), max_per_site[0][start_date+30:end_training+28].tolist(), ], quantiles=[0.1, 0.5, 0.9] ).head() Now that you know the predictor is working as expected, you\'92re ready run it across each of the days in February 2019. But before you do that, to allow you to calculate the MAPE, you\'92ll create a list called usages to store the actual power consumption for each site for each day in February 2019. When you run the predictions across each day in February, you store the result in a list called predictions. usages = [ ts[end_training+1:end_training+28].sum() for ts in usage_per_site] predictions= [] for s in range(len(usage_per_site)): # call the end point to get the 28 day prediction predictions.append( Listing 7.25 Building and deploying the model Listing 7.26 Checking the predictions from the model Listing 7.27 Getting predictions for all sites during February 2019 180 CHAPTER 7 Improving your company\'92s monthly power usage forecast predictor.predict( cat=[cats[s]], ts=usage_per_site[s][start_date+30:end_training], dynamic_feat=[ hols_per_site[s][start_date+30:end_training+28].tolist(), max_per_site[s][start_date+30:end_training+28].tolist(), ] )['0.5'].sum() ) for p,u in zip(predictions,usages): print(f'Predicted \{p\} kwh but usage was \{u\} kwh.') Once you have the usage list and the predictions list, you can calculate the MAPE by running the mape function you created in listing 7.21. print(f'MAPE: \{round(mape(usages, predictions),1)\}%') Listing 7.29 is the same plot function you saw in chapter 6. The function takes the usage list and creates predictions in the same way you did in listing 7.27. The difference in the plot function here is that it also calculates the lower and upper predictions at an 80% confidence level. It then plots the actual usage as a line and shades the area within the 80% confidence threshold. def plot( predictor, site_id, end_training=end_training, plot_weeks=12, confidence=80 ): low_quantile = 0.5 - confidence * 0.005 up_quantile = confidence * 0.005 + 0.5 target_ts = usage_per_site[site_id][start_date+30:] dynamic_feats = [ hols_per_site[site_id][start_date+30:].tolist(), max_per_site[site_id][start_date+30:].tolist(), ] plot_history = plot_weeks * 7 fig = plt.figure(figsize=(20, 3)) ax = plt.subplot(1,1,1) prediction = predictor.predict( cat = [cats[site_id]], ts=target_ts[:end_training], Listing 7.28 Calculating MAPE Listing 7.29 Displaying plots of sites Building the model 181 dynamic_feat=dynamic_feats, quantiles=[low_quantile, 0.5, up_quantile]) target_section = target_ts[ end_training-plot_history:end_training+prediction_length] target_section.plot(color="black", label='target') ax.fill_between( prediction[str(low_quantile)].index, prediction[str(low_quantile)].values, prediction[str(up_quantile)].values, color="b", alpha=0.3, label=f'\{confidence\}% confidence interval' ) ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5) The following listing runs the plot function you created in listing 7.29. indices = [2,26,33,39,42,47,3] for i in indices: plot_num = indices.index(i) plot( predictor, site_id=i, plot_weeks=6, confidence=80 Figure 7.9 shows the predicted results for several sites. As you can see, the daily prediction for each time series falls within the shaded area. One of the advantages of displaying the data in this manner is that it is easy to pick out sites where you haven\'92t predicted accurately. For example, if you look at site 3, the last site in the plot list in figure 7.10, you can see that there was a period in February with almost no power usage, when you predicted it would have a fairly high usage. This provides you with an opportunity to improve your model by including additional datasets. When you see a prediction that is clearly inaccurate, you can investigate what happened during that time and determine if there is some data source that you could incorporate into your predictions. If, for example, this site had a planned maintenance shutdown in early February and this shutdown was not already included in your holiday data, if you can get your hands on a schedule of planned maintenance shutdowns, then you can easily incorporate that data in your model in the same way that you incorporated the holiday data. Listing 7.30 Plotting several sites and the February predictions 182 CHAPTER 7 Improving your company\'92s monthly power usage forecast 7.6 Deleting the endpoint and shutting down your notebook instance As always, when you are no longer using the notebook, remember to shut down the notebook and delete the endpoint. We don\'92t want you to get charged for SageMaker services that you\'92re not using. 7.6.1 Deleting the endpoint To delete the endpoint, uncomment the code in listing 7.31, then click C+R to run the code in the cell. Shaded area shows daily predicted power consumption in February 20 9 with 80% confidence 1 Figure 7.9 Site plots showing predicted usage for February 2019 Predicted usage is higher than actual usage in early February. Figure 7.10 Predicted usage for site 3 is incorrect in early February. Checking to make sure the endpoint is deleted 183 # Remove the endpoints # Comment out these cells if you want the endpoint to persist after Run All # sess.delete_endpoint('energy-usage-baseline') # sess.delete_endpoint('energy-usage-dynamic') 7.6.2 Shutting down the notebook instance To shut down the notebook, go back to your browser tab where you have SageMaker open. Click the Notebook Instances menu item to view all of your notebook instances. Select the radio button next to the notebook instance name, as shown in figure 7.11, then select Stop from the Actions menu. It takes a couple of minutes to shut down. 7.7 Checking to make sure the endpoint is deleted If you didn\'92t delete the endpoint using the notebook (or if you just want to make sure it is deleted), you can do this from the SageMaker console. To delete the endpoint, click the radio button to the left of the endpoint name, then click the Actions menu item and click Delete in the menu that appears. When you have successfully deleted the endpoint, you will no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you see the text \'93There are currently no resources\'94 displayed on the Endpoints page (figure 7.12). Kiara can now predict power consumption for each site with a 6.9% MAPE, even for months with a number of holidays or predicted weather fluctuations. Listing 7.31 Deleting the endpoint 1. Select the radio button. 2. Select Stop. Figure 7.11 Shutting down the notebook 184 CHAPTER 7 Improving your company\'92s monthly power usage forecast Summary 
\f2 \uc0\u61601 
\f0  Past usage is not always a good predictor of future usage. 
\f2 \uc0\u61601 
\f0  DeepAR is a neural network algorithm that is particularly good at incorporating several different time-series datasets into its forecasting, thereby accounting for events in your time-series forecasting that your time-series data can\'92t directly infer. 
\f2 \uc0\u61601 
\f0  The datasets used in this chapter can be classified into two types of data: categorical and dynamic. Categorical data is information about the site that doesn\'92t change, and dynamic data is data that changes over time. 
\f2 \uc0\u61601 
\f0  For each day in your prediction range, you calculate the Mean Average Prediction Error (MAPE) for the time-series data by defining the function mape. 
\f2 \uc0\u61601 
\f0  Once the model is built, you can run the predictions and display the results in multiple time-series charts to easily visualize the predictions. Endpoint successfully deleted Figure 7.12 Confirm that all endpoints were deleted. Part 3 Moving machine learning into production The final part of this book shows you how to serve your machine learning models over the web without setting up any servers or other infrastructure. The book concludes with two case studies showing how companies are running machine learning projects in their own operations and how they are changing their workforce to take advantage of the opportunities created by machine learning and automation. 187 Serving predictions over the web Until now, the machine learning models you built can be used only in SageMaker. If you wanted to provide a prediction or a decision for someone else, you would have to submit the query from a Jupyter notebook running in SageMaker and send them the results. This, of course, is not what AWS intended for SageMaker. They intended that your users would be able to access predictions and decisions over the web. In this chapter, you\'92ll enable your users to do just that. This chapter covers 
\f2 \uc0\u61601 
\f0  Setting up SageMaker to serve predictions over the web 
\f2 \uc0\u61601 
\f0  Building and deploying a serverless API to deliver SageMaker predictions 
\f2 \uc0\u61601 
\f0  Sending data to the API and receiving predictions via a web browser 188 CHAPTER 8 Serving predictions over the web 8.1 Why is serving decisions and predictions over the web so difficult? In each of the previous chapters, you created a SageMaker model and set up an endpoint for that model. In the final few cells of your Jupyter Notebook, you sent test data to the endpoint and received the results. You have only interacted with the SageMaker endpoint from within the SageMaker environment. In order to deploy the machine learning model on the internet, you need to expose that endpoint to the internet. Until recently, this was not an easy thing to do. You first needed to set up a web server. Next, you coded the API that the web server would use, and finally, you hosted the web server and exposed the API as a web address (URL). This involved lots of moving parts and was not easy to do. Nowadays, all this is much easier. In this chapter, you\'92ll tackle the problem of creating a web server and hosting the API in a way that builds on many of the skills relating to Python and AWS that you\'92ve learned in previous chapters. At present, you can serve web applications without worrying about the complexities of setting up a web server. In this chapter, you\'92ll use AWS Lambda as your web server (figure 8.1). AWS Lambda is a server that boots on demand. Every tweet you send to the SageMaker endpoint creates a server that sends the tweet and receives the response, and then shuts down once it\'92s finished. This sounds like it might be slow from reading that Serving tweets In chapter 4, you helped Naomi identify which tweets should be escalated to her support team and which tweets could be handled by an automated bot. One of the things you didn\'92t do for Naomi was provide a way for her to send tweets to the machine learning model and receive a decision as to whether a tweet should be escalated. In this chapter, you will rectify that. Send tweet from browser to AWS Lambda Send tweet from AWS Lambda to SageMaker endpoint Escalation decision sent to browser Decision to escalate is sent from endpoint to ASW Lambda Figure 8.1 Sending a tweet from a browser to SageMaker The SageMaker endpoint 189 description, but it\'92s not. AWS Lambda can start and shut down in a few milliseconds. The advantage when serving your API is that you\'92re paying for the Lambda server only when it is serving decisions from your API. For many APIs, this is a much more costeffective model than having a permanent, dedicated web server waiting to serve predictions from your API. 8.2 Overview of steps for this chapter This chapter contains very little new code. It\'92s mostly configurations. To help follow along throughout the chapter, you\'92ll see a list of steps and where you are in the steps. The steps are divided into several sections: 1 Set up the SageMaker endpoint. 2 Configure AWS on your local computer. 3 Create a web endpoint. 4 Serve decisions. With that as an introduction, let\'92s get started. 8.3 The SageMaker endpoint Up to this point, you have interacted with your machine learning models using a Jupyter notebook and the SageMaker endpoint. When you interact with your models in this manner, it hides some of the distinctions between the parts of the system. The SageMaker endpoint can also serve predictions to an API, which can then be used to serve predictions and decisions to users over the web. This configuration works because it is a safe environment. You can\'92t access the SageMaker endpoint unless you\'92re logged into Jupyter Notebook, and anyone who is logged into the Notebook server has permission to access the endpoint. When you move to the web, however, things are a little more wild. You don\'92t want just anyone hitting on your SageMaker endpoint, so you need to be able to secure the endpoint and make it available only to those who have permission to access it. Why do you need an API endpoint in addition to the SageMaker endpoint? SageMaker endpoints don\'92t have any of the required components to allow them to be safely exposed to the wilds of the internet. Fortunately, there are lots of systems that can handle this for you. In this chapter, you\'92ll use AWS\'92s infrastructure to create a serverless web application configured to serve predictions and decisions from the SageMaker endpoint you set up in chapter 4. To do so, you\'92ll follow these steps: Serverless computing Services like AWS Lambda are often called serverless. The term serverless is a misnomer. When you are serving an API on the internet, by definition it cannot be serverless. What serverless refers to is the fact that somebody else has the headache of running your server. 190 CHAPTER 8 Serving predictions over the web 1 Set up the SageMaker endpoint by a Starting SageMaker b Uploading a notebook c Running a notebook 2 Configure AWS on your local computer. 3 Create a web endpoint. 4 Serve decisions. To begin, you need to start SageMaker and create an endpoint for the notebook. The notebook you\'92ll use is the same as the notebook you used for chapter 4 (customer _support.ipynb), except it uses a different method for normalizing the tweet text. Don\'92t worry if you didn\'92t work through that chapter or don\'92t have the notebook on SageMaker anymore, we\'92ll walk you through how to set it up. 8.4 Setting up the SageMaker endpoint Like each of the other chapters, you\'92ll need to start SageMaker (detailed in appendix C). For your convenience, that\'92s summarized here. First, go to the SageMaker service on AWS by clicking this link: https://console.aws.amazon.com/sagemaker/home Then, start your notebook instance. Figure 8.2 shows the AWS Notebook instances page. Click the Start action. In a few minutes, the page refreshes and a link to Open Jupyter appears, along with an InService status message. Figure 8.3 shows the AWS Notebook Instances page after the notebook instance is started. Figure 8.2 Starting a SageMaker instance Click Start. mlforbusiness Setting up the SageMaker endpoint 191 The next section shows you how to upload the notebook and data for this chapter. But what are the differences between the notebook used in chapter 4 and the notebook used in chapter 8? In this chapter, even though you are deciding which tweets to escalate as you did in chapter 4, you will create a new notebook rather than reuse the notebook for chapter 4. The reason for this is that we want to be able to pass the text of the tweet as a URL in the address bar of the browser (so we don\'92t have to build a web form to enter the text of the tweet). This means that the text of the tweet can\'92t contain any characters that are not permitted to be typed in the address bar of the browser. As this is not how we built the model in chapter 4, we need to train a new model in this chapter. The notebook we create in this chapter is exactly the same as the notebook in chapter 4, except that it uses a library called slugify to preprocess the tweet rather than NLTK. Slugify is commonly used to turn text into website URLs. In addition to providing a lightweight mechanism to normalize text, it also allows the tweets to be accessed as URLs. 8.4.1 Uploading the notebook Start by downloading the Jupyter notebook to your computer from this link: https://s3.amazonaws.com/mlforbusiness/ch08/customer_support_slugify.ipynb Now, in the notebook instance shown in figure 8.4, create a folder to store the notebook by clicking New on the Files page and selecting the Folder menu item as shown in figure 8.5. The new folder will hold all of your code for this chapter. Figure 8.6 shows the new folder after you click it. Once in the folder, you see an Upload button on the top right of the page. Clicking this button opens a file selection Click Open Jupyter. mlforbusiness Figure 8.3 Opening Jupyter 192 CHAPTER 8 Serving predictions over the web Click New. Figure 8.4 Creating a new notebook folder: Step 1 Click Folder. Figure 8.5 Creating a new notebook folder: Step 2 Click upload. Figure 8.6 Uploading the notebook to a new notebook folder Setting up the SageMaker endpoint 193 window. Navigate to the location where you downloaded the Jupyter notebook, and upload it to the notebook instance. Figure 8.7 shows the notebook you have uploaded to SageMaker. 8.4.2 Uploading the data Even though you can\'92t reuse the notebook from chapter 4, you can reuse the data. If you set up the notebook and the data for chapter 4, you can use that dataset; skip ahead to section 8.4.3, \'93Running the notebook and creating the endpoint.\'94 If you didn\'92t do that, follow the steps in this section. If you didn\'92t set up the notebook and the data for chapter 4, download the dataset from this location: https://s3.amazonaws.com/mlforbusiness/ch04/inbound.csv Save this file to a location on your computer. You won\'92t do anything with this file other than upload it to S3, so you can use your downloads directory or some other temporary folder. Now, head to AWS S3, the AWS file storage service, by clicking this link: https://s3.console.aws.amazon.com/s3/home Once there, create or navigate to the S3 bucket where you are keeping the data for this book (see appendix B if you haven\'92t created an S3 bucket yet). In your bucket, you can see any folders you have created. If you haven\'92t done so already, create a folder to hold the data for chapter 4 by clicking Create Folder. (We are setting up the data for chapter 4 because this chapter uses the same data as that chapter. You may as well store the data as in chapter 4, even if you haven\'92t worked through the content of that chapter.) Figure 8.8 shows the folder structure you might have if you have followed all the chapters in this book. Figure 8.7 Verifying that the notebook customer_support_slugify.ipynb was uploaded to your SageMaker folder 194 CHAPTER 8 Serving predictions over the web Inside the folder, click Upload on the top left of the page, find the CSV data file you just saved, and upload it. After you have done so, you\'92ll see the inbound.csv file listed in the folder (figure 8.9). Keep this page open because you\'92ll need to get the location of the file when you run the notebook. You now have a Jupyter notebook set up on SageMaker and data loaded onto S3. You are ready to begin to build and deploy your model in preparation for serving predictions over the web. 8.4.3 Running the notebook and creating the endpoint Now that you have a Jupyter notebook instance running and uploaded your data to S3, run the notebook and create the endpoint. You do this by selecting Cell from the menu and then clicking Run All (figure 8.10). Figure 8.8 Example of what your S3 folder structure might look like Setting up the SageMaker endpoint 195 After 5 minutes or so, all the cells in the notebook will have run, and you will have created an endpoint. You can see that all the cells have run by scrolling to the bottom of the notebook and checking for a value in the second-to-last cell (below the Test the Model heading) as shown in figure 8.11. Figure 8.9 Example of what your S3 bucket might look like once you have uploaded the CSV data Figure 8.10 Running all cells in the notebook Click Run All. 196 CHAPTER 8 Serving predictions over the web Once you have run the notebook, you can view the endpoint by clicking the Endpoints link as shown in figure 8.12. Here you will see the ARN (Amazon Resource Name) of the endpoint you have created. You will need this when you set up the API endpoint. Figure 8.13 shows an example of what your endpoint might look like. Now, you can set up the serverless API endpoint. The API endpoint is the URL or web address that you will send the tweets to. You will set up the endpoint on AWS serverless technology, which is called AWS Lambda. In order for AWS Lambda to know what to do, you will install the Chalice Python library. Chalice is a library built by AWS to make it easy to use Python to serve an API endpoint. You can read more about Chalice here: https://chalice.readthedocs.io/en/latest/. Figure 8.11 Confirming that all cells in the notebook have run Notebook has completed if the output of this cell has a value. Figure 8.12 Navigating to Endpoints to view your current endpoints Click on Endpoints. Setting up the serverless API endpoint 197 8.5 Setting up the serverless API endpoint To review where you are at in the process, you have just set up a SageMaker endpoint that is ready to serve decisions about whether to escalate a tweet to your support team. Next, you\'92ll set Chalice, the serverless API endpoint as follows: 1 Set up the SageMaker endpoint. 2 Configure AWS on your local computer by a Creating credentials b Installing the credentials on your local computer c Configuring the credentials 3 Create a web endpoint. 4 Serve decisions. It\'92s somewhat ironic that the first thing you need to do to set up a serverless API endpoint is set up software on your computer. The two applications you need are Python (version 3.6 or higher) and a text editor. Instructions for installing Python are in appendix E. Although installing Python used to be tricky, it\'92s become much easier for Windows operating systems with the inclusion of Python in the Microsoft Windows Store. And installing Python on Apple computers has been made easier for some time now by the Homebrew package manager. As we mentioned, you\'92ll also need a text editor. One of the easiest editors to set up is Microsoft\'92s Visual Studio Code (VS Code). It runs on Windows, macOS, and Linux. You can download VS Code here: https://code.visualstudio.com/. Now that you are set up to run Python on your computer, and you have a text editor, you can start setting up the serverless endpoint. Figure 8.13 Example of what your endpoint ARN might look like Endpoint ARN 198 CHAPTER 8 Serving predictions over the web 8.5.1 Setting up your AWS credentials on your AWS account To access the SageMaker endpoint, your serverless API needs to have permission to do so. And because you are writing code on your local computer rather than in a SageMaker notebook (as you have done for each of the previous chapters in the book), your local computer also needs permission to access the SageMaker endpoint and your AWS account. Fortunately, AWS provides a simple way to do both. First, you need to create credentials in your AWS account. To set up credentials, click the AWS username in the top right of the browser from any AWS page (figure 8.14). In the page that opens, there\'92s a Create access key button that allows you to create an access key, which is one of the types of credentials you can use to access your AWS account. Click this button. Figure 8.15 shows the AWS user interface for creating an access key. After clicking this button, you will be able to download your security credentials as a CSV file. Figure 8.14 Creating AWS credentials Click here. Figure 8.15 Creating an AWS access key Click here. Setting up the serverless API endpoint 199 NOTE You are presented with your one and only opportunity to download your keys as a CSV file. Download the CSV file and save it somewhere on your computer where only you have access (figure 8.16). Anyone who gets this key can use your AWS account. With the access key downloaded to your computer, you can set up your local computer to access AWS. We\'92ll cover that next. 8.5.2 Setting up your AWS credentials on your local computer To set up your local computer to access AWS, you need to install two AWS Python libraries on your local computer. This section will walk you through how to install these libraries from VS Code, but you can use any terminal application such as Bash on Unix or macOS, or PowerShell on Windows. First, create a folder on your computer that you will use for saving your code. Then open VS Code and click the Open Folder button as shown in figure 8.17. Create a new folder on your computer to hold the files for this chapter. Once you have done so, you can start installing the Python libraries you\'92ll need. The code you\'92ll write on your local computer needs Python libraries in the same way SageMaker needs Python libraries to run. The difference between your local computer and SageMaker is that SageMaker has the libraries you need already installed, whereas on your computer, you may need to install the libraries yourself. In order to install the Python libraries on your computer, you need to open a terminal shell. This is a way to enter commands into your computer using only the keyboard. Opening a terminal window in VS Code is done by pressing C+S. Alternatively, you can open a terminal window from VS Code by selecting Terminal from the menu bar and then selecting New Terminal. Figure 8.16 Downloading the AWS access key Click to download. 200 CHAPTER 8 Serving predictions over the web A terminal window appears at the bottom of VS Code, ready for you to type into. You can now install the Python libraries you need to access SageMaker. The first library you will install is called boto3. This library helps you interact with AWS services. SageMaker itself uses boto3 to interact with services such as S3. To install boto3, in the terminal window, type pip install boto3 Next, you\'92ll need to install the command-line interface (CLI) library that lets you stop and start an AWS service from your computer. It also allows you to set up credentials that you have created in AWS. To install the AWS CLI library, type pip install awscli With both boto3 and the CLI library installed, you can now configure your credentials. 8.5.3 Configuring your credentials To configure your AWS credentials, run the following command at the prompt in the terminal window: aws configure You are asked for your AWS Access Key ID and the AWS Secret Access Key you downloaded earlier. You are also asked for your AWS region. Figure 8.18 shows how to locate your SageMaker region. Figure 8.18 shows the address bar of a web browser when you are logged into SageMaker. The address shows which region your SageMaker service is located in. Use this Figure 8.17 Opening a folder in VS Code Click Open folder. Creating the web endpoint 201 region when you configure the AWS credentials. Note that you can leave the Default output format blank. AWS Access Key ID: AWS Secret Access Key: Default region name [us-east-1]: Default output format [None]: You\'92ve completed the configuration of AWS on your local computer. To recap, you set up the SageMaker endpoint, then you configured AWS on your local computer. Now you will create the web endpoint that allows you to serve decisions regarding which tweets to escalate to Naomi\'92s support team. Let\'92s update where we are in the process: 1 Set up the SageMaker endpoint. 2 Configure AWS on your local computer. 3 Create a web endpoint by a Installing Chalice b Writing endpoint code c Configuring permissions d Updating requirements.txt e Deploying Chalice 4 Serve decisions 8.6 Creating the web endpoint You are at the point in the chapter that amazed us when we first used AWS to serve an API endpoint. You are going to create a serverless function using an AWS technology called a Lambda function (https://aws.amazon.com/lambda/), and configure the API using an AWS technology called the Amazon API Gateway (https://aws.amazon .com/api-gateway/). Then you\'92ll deploy the SageMaker endpoint so it can used by anyone, anywhere. And you will do it in only a few lines of code. Amazing! Figure 8.18 Locating your SageMaker region SageMaker region Enter the access key ID you downloaded earlier. Enter the secret access key you downloaded earlier. Enter the AWS region you use with SageMaker. 202 CHAPTER 8 Serving predictions over the web 8.6.1 Installing Chalice Chalice (https://github.com/aws/chalice) is open source software from Amazon that automatically creates and deploys a Lambda function and configures an API gateway endpoint for you. During configuration, you will create a folder on your computer to store the Chalice code. Chalice will take care of packaging the code and installing it in your AWS account. It can do this because, in the previous section, you configured your AWS credentials using the AWS CLI. The easiest way to get started is to navigate to an empty folder on your computer. Right-click the folder to open a menu, and then click Open with Code as shown in figure 8.19. Alternatively, you can open this folder from VS Code in the same way you did in figure 8.17. Neither way is better than the other\'97use whichever approach you prefer. To install Chalice, once you have opened VS Code, go to the terminal window like you did when you configured the AWS CLI, and type this command: pip install chalice Depending on the permissions you have on your computer, if this produces an error, you might need to type this command: pip install --user chalice Just like the AWS CLI you used earlier, this command creates a CLI application on your system. Now you\'92re all set to use Chalice. Using Chalice is straightforward. There are two main commands: 
\f2 \uc0\u61601 
\f0  new-project 
\f2 \uc0\u61601 
\f0  deploy Figure 8.19 Opening the VS Code editor in a folder Click Open with Code. Creating the web endpoint 203 To create a new project named tweet_escalator, run the following command at the prompt: chalice new-project tweet_escalator If you look in the folder you opened VS Code from, you will see a folder called tweet_escalator that contains some files that Chalice automatically created. We\'92ll discuss these files shortly, but first, let\'92s deploy a Hello World application. In the terminal window, you\'92ll see that after running chalice new-project tweet_escalator, you\'92re still in the folder you opened VS Code from. To navigate to the tweet_escalator folder, type cd tweet_escalator You\'92ll see that you are now in the folder tweet_escalator: c:\\\\mlforbusiness\\ch08\\tweet_escalator Now that you are in the tweet_escalator folder, you can type chalice deploy to create a Hello World application: c:\\\\mlforbusiness\\ch08\\tweet_escalator chalice deploy Chalice will then automatically create a Lambda function on AWS, set up the permissions to run the application (known as an IAM role), and configure a Rest endpoint using AWS Gateway. Here\'92s Chalice\'92s process: 
\f2 \uc0\u61601 
\f0  Create a deployment package 
\f2 \uc0\u61601 
\f0  Create an IAM role (tweet_escalator-dev) 
\f2 \uc0\u61601 
\f0  Create a Lambda function (tweet_escalator-dev) 
\f2 \uc0\u61601 
\f0  Create a Rest API The resources deployed by Chalice are 
\f2 \uc0\u61601 
\f0  Lambda ARN (arn:aws:lambda_us-east-1:3839393993:function:tweet_escalator-dv) 
\f2 \uc0\u61601 
\f0  Rest API URL (https://eyeueiwwo.execute-api.us-east-1.amazonaws.com/api/) You can run the Hello World application by clicking the Rest API URL shown in the terminal. Doing so opens a web browser and displays \{"hello":"world"\} in JSON, as shown in figure 8.20. Figure 8.20 Hello World 204 CHAPTER 8 Serving predictions over the web Congratulations! Your API is now up and running and you can see the output in your web browser. 8.6.2 Creating a Hello World API Now that you have the Hello World application working, it\'92s time to configure Chalice to return decisions from your endpoint. Figure 8.21 shows the files that Chalice automatically created when you typed chalice new-project tweet_escalator. Three important components are created: 
\f2 \uc0\u61601 
\f0  A .chalice folder that contains the configuration files. The only file in this folder that you will need to modify is the policy-dev.json file, which sets the permissions that allow the Lambda function to call the SageMaker endpoint. 
\f2 \uc0\u61601 
\f0  An app.py file that contains the code that runs when the endpoint is accessed (such as when you view it in your web browser). 
\f2 \uc0\u61601 
\f0  A requirements.txt file that lists any Python libraries that your application needs to run. Listing 8.1 shows the code in the app.py file that Chalice creates automatically. The app only needs a name, a route, and a function to work. from chalice import Chalice app = Chalice(app_name='tweet_escalator') @app.route('/') def index(): return \{'hello': 'world'\} Listing 8.1 Chalice\'92s default app.py code Figure 8.21 Screenshot of the Chalice folder on your computer 1. Set permissions. 2. Configure application. 3. List software to install. Imports Chalice, the library that creates the Lambda function and API gateway Sets the name of the app Sets the default route Defines the function that runs when the default route is hit Sets the value that gets returned by the function and displayed in the web browser Creating the web endpoint 205 In listing 8.1, the name of the app (line 2) is the name that is used to identify the Lambda function and API gateway on AWS. The route (line 3) identifies the URL location that runs the function. And the function (line 4) is the code that is run when the URL location is accessed. 8.6.3 Adding the code that serves the SageMaker endpoint You can keep the Hello World code you just created and use it as the basis for your code that will serve the SageMaker endpoint. For now, at the bottom of the Hello World code, add two blank lines and enter the code shown in listing 8.2. The full code listing can be downloaded from this link: https://s3.amazonaws.com/mlforbusiness/ ch08/app.py. @app.route('/tweet/\{tweet\}') def return_tweet(tweet): tokenized_tweet = [ slugify(tweet, separator=' ')] payload = json.dumps( \{"instances" : tokenized_tweet\}) endpoint_name = 'customer-support-slugify' runtime = boto3.Session().client( service_name='sagemaker-runtime', region_name='us-east-1') response = runtime.invoke_endpoint( EndpointName=endpoint_name, ContentType='application/json', Body=payload) Accessing URLs There are many ways to access a URL. In this chapter, we\'92ll access the URL simply by typing the URL in the address bar of a browser. More commonly, when you are invoking a SageMaker endpoint, you will access the URL location from another application. For example, you would implement an app in the ticketing system that Naomi\'92s support team uses when responding to tweets. Then the app would send the tweet to the URL location and read the response returned. And finally, if the response returned recommends that the tweet be escalated, then it would be routed to a particular support channel in your ticketing system. Building this application is beyond the scope of this book. In this chapter, you will just set up the URL location that invokes the SageMaker endpoint and displays an escalation recommendation in a web browser. Listing 8.2 The default app.py code Defines the route Sets up the function Tokenizes the tweet Sets up the payload Identifies the SageMaker endpoint Prepares the endpoint Invokes the endpoint and gets the response 206 CHAPTER 8 Serving predictions over the web response_list = json.loads( response['Body'].read().decode()) response = response_list[0] if '1' in response['label'][0]: escalate = 'Yes' else: escalate = 'No' full_response = \{ 'Tweet': tweet, 'Tokenised tweet': tokenized_tweet, 'Escalate': escalate, 'Confidence': response['prob'][0] \} return full_response Just like the @app.route you set in line 3 of listing 8.1, you start your code by defining the route that will be used. Instead of defining the route as / as you did earlier, in line 1 of listing 8.2, you set the route as /tweet/\{tweet\}/. This tells the Lambda function to watch for anything that hits the URL path /tweet/ and submit anything it sees after that to the SageMaker endpoint, for example, if Chalice creates an endpoint for you at https://ifs1qanztg.execute-api.us-east-1.amazonaws.com/api/ When you go to this endpoint, it returns \{"hello": "world"\}. Similarly, the code in line 1 of listing 8.2 would send I am angry to the SageMaker endpoint when you access this endpoint: https://ifs1qanztg.execute-api.us-east-1.amazonaws.com/api/tweet/i-am-angry The code \{tweet\} tells Chalice to put everything it sees at the end of the URL into a variable called tweet. In the function you see in line 2, you are using the variable tweet from line 1 as your input to the function. Line 3 slugifies the tweet using the same function that the Jupyter notebook uses. This ensures that the tweets you send to the SageMaker endpoint are normalized using the same approach that was used to train the model. Line 4 reflects the code in the Jupyter notebook to create the payload that gets sent to the SageMaker endpoint. Line 5 is the name of the SageMaker endpoint you invoke. Line 6 ensures that the endpoint is ready to respond to a tweet sent to it, and line 7 sends the tweet to the SageMaker endpoint. Line 8 receives the response. The SageMaker endpoint is designed to take in a list of tweets and return a list of responses. For our application in this chapter, you are only sending a single tweet, so line 9 returns just the first result. Line 10 converts the escalate decision from 0 or 1 to No or Yes, respectively. And finally, line 11 defines the response format, and line 12 returns the response to the web browser. Converts the response to a list Gets the first item in the list Sets escalate decision to Yes or No Sets the full response format Returns the response Creating the web endpoint 207 8.6.4 Configuring permissions At this point, your Chalice API still cannot access your AWS Lambda function. You need to give the AWS Lambda function permission to access your endpoint. Your Hello World Lambda function worked without configuring permissions because it did not use any other AWS resources. The updated function needs access to AWS SageMaker, or it will give you an error. Chalice provides a file called policy-dev.json, which sets permissions. You\'92ll find it in the .chalice folder that\'92s located in the same folder as the app.py file you\'92ve just worked on. Once you navigate into the .chalice folder, you\'92ll see the policy-dev.json file. Open it in VS Code and replace the contents with the contents of listing 8.3. NOTE If you don\'92t want to type or copy and paste, you can download the policy-dev.json file here: https://s3.amazonaws.com/mlforbusiness/ch08/ policy-dev.json. \{ "Version": "2012-10-17", "Statement": [ \{ "Sid": "VisualEditor0", "Effect": "Allow", "Action": [ "logs:CreateLogStream", "logs:PutLogEvents", "logs:CreateLogGroup" ], "Resource": "arn:aws:logs:*:*:*" \}, \{ "Sid": "VisualEditor1", "Effect": "Allow", "Action": "sagemaker:InvokeEndpoint", "Resource": "*" \} ] \} Your API now has permission to invoke the SageMaker endpoint. There is still one more step to do before you can deploy the code to AWS. 8.6.5 Updating requirements.txt You need to instruct the Lambda function to install the slugify so it can be used by the application. To do this, you add the line in listing 8.4 to the requirements.txt file located in the same folder as the app.py file. NOTE You can download the file here: https://s3.amazonaws.com/ mlforbusiness/ch08/requirements.txt. Listing 8.3 Contents of policy-dev.json Adds permission to invoke a SageMaker endpoint 208 CHAPTER 8 Serving predictions over the web python-slugify The requirements.txt update is the final step you need to do before you are ready to deploy Chalice. 8.6.6 Deploying Chalice At last, it\'92s time to deploy your code so that you can access your endpoint. In the terminal window in VS Code, from the tweet_escalator folder, type: chalice deploy This regenerates your Lambda function on AWS with a few additions: 
\f2 \uc0\u61601 
\f0  The Lambda function now has permission to invoke the SageMaker endpoint. 
\f2 \uc0\u61601 
\f0  The Lambda function has installed the slugify library so it can be used by the function. 8.7 Serving decisions To recap, in this chapter, you have set up the SageMaker endpoint, configured AWS on your computer, and created and deployed the web endpoint. Now you can start using it. We\'92re finally at the last step in the process: 1 Set up the SageMaker endpoint. 2 Configure AWS on your local computer. 3 Create a web endpoint. 4 Serve decisions. To view your API, click the Rest API URL link that is shown in your terminal window after you run chalice deploy (figure 8.22). This still brings up the Hello World page because we didn\'92t change the output (figure 8.23). Listing 8.4 Contents of requirements.txt Figure 8.22 The Rest API URL used to access the endpoint in a web browser. Figure 8.23 Hello World, again Serving decisions 209 To view the response to a tweet, you need to enter the route in the address bar of your browser. An example of the route you need to add is shown in figure 8.24. At the end of the URL in the address bar of your browser (after the final /), you type tweet/thetext-of-the-tweet-with-dashes-instead-of-spaces and press R. The response displayed on the web page now changes from \{"hello": "world"\} to \{"Tweet":"I-am-very-angry","Tokenized tweet":["i am very angry"], "Escalate":"Yes","Confidence":1.0000098943710327\} The response shows the tweet it pulled from the address bar, the tokenized tweet after running it through slugify, the recommendation on whether to escalate the tweet or not (in this case the answer is Yes), and the confidence of the recommendation. To test additional phrases, simply type them into the address bar. For example, entering thanks-i-am-happy-with-your-service generates the response shown in figure 8.25. As expected, the recommendation is to not escalate this tweet. It is interesting to see the results for negating a tweet such as turning \'93I am very angry\'94 to \'93I am not angry.\'94 You might expect that the API would recommend not escalating this, but that is often not the case. Figure 8.26 shows the response to this tweet. You can see it still recommends escalation, but its confidence is much lower\'97 down to 52%. To see why it was escalated, you need to look at the data source for the tweets. When you look at the negated tweets, you see that many of the tweets were labeled Escalate Figure 8.24 Tweet response: I am very angry Figure 8.25 Tweet response: I am happy with your service Figure 8.26 Tweet response: I am not angry 210 CHAPTER 8 Serving predictions over the web because the negated phrase was part of a longer tweet that expressed frustration. For example, a common tweet pattern was for a person to tweet \'93I\'92m not angry, I\'92m just disappointed.\'94 Summary 
\f2 \uc0\u61601 
\f0  In order to deploy a machine learning model on the internet, you need to expose that endpoint to the internet. Nowadays, you can serve web applications containing your model without worrying about the complexities of setting up a web server. 
\f2 \uc0\u61601 
\f0  AWS Lambda is a web server that boots on demand and is a much more costeffective way to serve predictions from your API. 
\f2 \uc0\u61601 
\f0  The SageMaker endpoint can also serve predictions to an API, which can then be used to serve predictions and decisions to users over the web, and you can secure the endpoint and make it available only to those who have permission to access it. 
\f2 \uc0\u61601 
\f0  To pass the text of the tweet as a URL, the text of the tweet can\'92t contain any characters that are not permitted to be typed in the address bar of the browser. 
\f2 \uc0\u61601 
\f0  You set up a SageMaker endpoint with slugify (rather than NLTK) to normalize tweets. Slugify is commonly used to turn text into website URLs. 
\f2 \uc0\u61601 
\f0  You set up the serverless API SageMaker endpoint on AWS serverless technology, which is called AWS Lambda. In order for AWS Lambda to know what to do, you install the Chalice Python library. 
\f2 \uc0\u61601 
\f0  To access the SageMaker endpoint, your serverless API needs to have permission to do so. Using Microsoft\'92s Visual Studio Code (VS Code), you set up credentials in your AWS account by creating an access key, then setting up your AWS credentials on your local computer. 
\f2 \uc0\u61601 
\f0  You set up the AWS command-line interface (CLI) and boto3 libraries on your local computer so you can work with AWS resources from your local machine. 
\f2 \uc0\u61601 
\f0  To create a serverless function, you learned about AWS Lambda functions and the AWS API Gateway services and how easy it is to use them with Chalice. 
\f2 \uc0\u61601 
\f0  You deployed an API that returns recommendations about whether to escalate a tweet to Naomi\'92s support team. 211 Case studies Throughout the book, you have used AWS SageMaker to build solutions to common business problems. The solutions have covered a broad range of scenarios and approaches: 
\f2 \uc0\u61601 
\f0  Using XGBoost supervised learning to solve an approval routing challenge 
\f2 \uc0\u61601 
\f0  Reformatting data so that you could use XGBoost again, but this time to predict customer churn 
\f2 \uc0\u61601 
\f0  Using BlazingText and Natural Language Processing (NLP) to identify whether a tweet should be escalated to your support team 
\f2 \uc0\u61601 
\f0  Using unsupervised Random Cut Forest to decide whether to query a supplier\'92s invoice This chapter covers 
\f2 \uc0\u61601 
\f0  Review of the topics in this book 
\f2 \uc0\u61601 
\f0  How two companies using machine learning improved their business \'96 Case study 1: Implementing a single machine learning project in your company \'96 Case study 2: Implementing machine learning at the heart of everything your company does 212 CHAPTER 9 Case studies 
\f2 \uc0\u61601 
\f0  Using DeepAR to predict power consumption based on historical trends 
\f2 \uc0\u61601 
\f0  Adding datasets such as weather forecasts and scheduled holidays to improve DeepAR\'92s predictions In the previous chapter, you learned how to serve your predictions and decisions over the web using AWS\'92s serverless technology. Now, we\'92ll wrap it all up with a look at how two different companies are implementing machine learning in their business. In chapter 1, we put forward our view that we are on the cusp of a massive surge in business productivity and that this surge is going to be due in part to machine learning. Every company wants to be more productive, but they find it\'92s difficult to achieve this goal. Until the advent of machine learning, if a company wanted to become more productive, they needed to implement and integrate a range of best-in-class software or change their business practices to conform exactly to how their ERP (Enterprise Resource Planning) system worked. This greatly slows the pace of change in your company because your business either comprises a number of disparate systems, or it\'92s fitted with a straightjacket (also known as your ERP system). With machine learning, a company can keep many of their operations in their core systems and use machine learning to assist with automating decisions at key points in the process. Using this approach, a company can maintain a solid core of systems yet still take advantage of the best available technology. In each of the chapters from 2 through 7, we looked at how machine learning could be used to make a decision at a particular point in a process (approving purchase orders, reconnecting with customers at risk of churning, escalating tweets, and reviewing invoices), and how machine learning can be used to generate predictions based on historical data combined with other relevant datasets (power consumption prediction based on past usage and other information such as forecasted weather and upcoming holidays). The two case studies we look at in this chapter show several different perspectives when adopting machine learning in business. The first case study follows a labor-hire company as it uses machine learning to automate a time-consuming part of its candidate interview process. This company is experimenting with machine learning to see how it can solve various challenges in their business. The second case study follows a software company that already has machine learning at its core but wants to apply it to speed up more of its workflow. Let\'92s jump in and look at how the companies in these case studies use machine learning to enhance their business practices. 9.1 Case study 1: WorkPac WorkPac is Australia\'92s largest privately held labor-hire company. Every day, tens of thousands of workers are contracted out across thousands of clients. And every day, to maintain a suitable pool of candidates, WorkPac has teams of people interviewing candidates. The interview process can be thought of as a pipeline where candidates go into a funnel at the top and are categorized into broad categories as they progress down the Case study 1: WorkPac 213 funnel. Recruiters who are experts in a particular category apply metadata to the candidates so they can be filtered based on skills, experience, aptitude, and interest. Applying these filters allows the right pool of candidates to be identified for each open position. Figure 9.1 shows a simplified view of the categorization process. Candidate resumes go into the top of the funnel and are classified into different job categories. The anticipated benefit of automating the categorization funnel is that it frees up time for recruiters to focus on gathering metadata about candidates rather than classifying candidates. An additional benefit of using a machine learning model to perform the classification is that, as a subsequent phase, some of the metadata gathering can also be automated. Job categories Categorization process Candidates Truck driver Assayer Engineer Figure 9.1 Funnel to categorize candidates into different types of jobs 214 CHAPTER 9 Case studies Before implementing the machine learning application, when a candidate submits their resume through WorkPac\'92s candidate portal, it\'92s categorized by a generalist recruiter and potentially passed on to a specialist recruiter for additional metadata. After passing through this process, the candidate would then be available for other recruiters to find. For example, if the candidate was classified as a truck driver, then recruiters looking to fill a truck driver role would be able to find this candidate. Now that WorkPac has implemented the machine learning application, the initial classification is performed by a machine learning algorithm. The next phase of this project is to implement a chat bot that can elicit some of the metadata, further freeing up valuable recruiter time. 9.1.1 Designing the project WorkPac considered two approaches to automating the classification of candidates: 
\f2 \uc0\u61601 
\f0  A simple keyword classification system for candidates 
\f2 \uc0\u61601 
\f0  A machine learning approach to classify candidates The keyword classification system was perceived as low risk but also low reward. Like the approval-routing scenario you looked at in chapter 2, keyword classification requires ongoing time and effort to identify new keywords. For example, if Caterpillar releases a new mining truck called a 797F, WorkPac has to update their keyword list to associate that term with truck driver. Adopting a machine learning approach ensures that as new vehicles are released by manufacturers, for example, the machine learning model learns to associate 797F vehicles with truck drivers. The machine learning approach was perceived as higher reward but also higher risk because it would be WorkPac\'92s first time delivering a machine learning project. A machine learning project is a different beast than a standard IT project. With a typical IT project, there are standard methodologies to define the project and the outcomes. When you run an IT project, you know in advance what the final outcome will look like. You have a map, and you follow the route to the end. But with a machine learning project, you\'92re more like an explorer. Your route changes as you learn more about the terrain. Machine learning projects are more iterative and less predetermined. To help overcome these challenges, WorkPac retained the services of Blackbook.ai to assist them. Blackbook.ai is an automation and machine learning software company that services other businesses. WorkPac and Blackbook.ai put together a project plan that allowed them to build trust in the machine learning approach by delivering the solution in stages. The stages this project progressed through are typical of machine learning automation projects in general: 
\f2 \uc0\u61601 
\f0  Stage 1\'97Prepare and test the model to validate that decisions can be made using machine learning. 
\f2 \uc0\u61601 
\f0  Stage 2\'97Implement proof of concept (POC) around the workflow. 
\f2 \uc0\u61601 
\f0  Stage 3\'97Embed the process into the company\'92s operations. Case study 1: WorkPac 215 9.1.2 Stage 1: Preparing and testing the model Stage 1 involved building a machine learning model to classify existing resumes. WorkPac had more than 20 years of categorized resume data, so they had plenty of data to use for training. Blackbook.ai used OCR technology to extract text from the resumes and trained the model on this text. Blackbook.ai had enough data that they were able to balance out the classes by selecting equal numbers of resumes across each of the job categories. After training and tuning the model, the model was able to hit an F Score of 0.7, which was deemed to be suitable for this activity. F scores An F score (also known as an F1 score) is a measure of the performance of a machine learning model. In chapter 3, you learned how to create a confusion matrix showing the number of false positive and false negative predictions. An F score is another way of summarizing the results of a machine learning model. An example is the best way to see how an F score is calculated. The following table summarizes the results of a machine learning algorithm that made 50 predictions. The algorithm was attempting to predict whether a particular candidate should be classified as a truck driver, assayer, or engineer. The top row of the table indicates that out of 15 candidates who were actually truck drivers, the algorithm correctly predicted that 11 were truck drivers and incorrectly predicted that 4 were assayers. The algorithm did not incorrectly predict that any truck drivers were engineers. Likewise for the second row (Assayer), the algorithm correctly predicted that nine assayer candidates were indeed assayers, but it incorrectly predicted that four truck drivers were assayers and two engineers were assayers. If you look at the top row (the actual truck driver), you would say that 11 out of 15 predictions were correct. This is known as the precision of the algorithm. A result of 11/15 means the algorithm has a precision of 73% for truck drivers. You can also look at each column of data. If you look at the first column, Prediction (truck driver), you can see that the algorithm predicted 18 of the candidates were truck drivers. Out of 18 predictions, it got 11 right and 7 wrong. It predicted 4 assayers were truck drivers and 3 engineers were assayers. This is known as the recall of the algorithm. The algorithm correctly recalls 11 out of 18 predictions (61%). Table 9.1 Data table showing candidate predictions Prediction (truck driver) Prediction (assayer) Prediction (engineer) Total Actual (truck driver) 11 4 0 15 Actual (assayer) 4 9 2 15 Actual (engineer) 3 3 14 20 Total 18 16 16 50 216 CHAPTER 9 Case studies During this stage, Blackbook.ai developed and honed their approach for transforming resumes into data that could be fed into their machine learning model. In the model development phase, a number of the steps in this process were manual, but Blackbook.ai had a plan to automate each of these steps. After achieving an F score in excess of 0.7 and armed with a plan for automating the process, Blackbook.ai and WorkPac moved on to stage 2 of the project. 9.1.3 Stage 2: Implementing proof of concept (POC) The second stage involves building a POC that incorporates the machine learning model into WorkPac\'92s workflow. Like many business process-improvement projects involving machine learning, this part of the project took longer than the machine learning component. From a risk perspective, this part of the project was a standard IT project. In this stage, Blackbook.ai built a workflow that took resumes uploaded from candidates, classified the resumes, and presented the resumes and the results of the classification to a small number of recruiters in the business. Blackbook.ai then took feedback from the recruiters and incorporated recommendations into the workflow. Once the workflow was approved, they moved on to the final stage of the project\'97 implementation and rollout. 9.1.4 Stage 3: Embedding the process into the company\'92s operations The final stage of the project was to roll out the process across all of WorkPac. This is typically time consuming as it involves building error-catching routines that allow the process to function in production, and training staff in the new process. Although time consuming, this stage can be low risk, providing the feedback from the stage 2 users is positive. (continued) From this example, the importance of both precision and recall can be seen. The precision result of 73% looks pretty good, but the results look less favorable when you consider that only 61% of its truck driver predictions are correct. The F score reduces this number to a single value using the following formula: ((Precision * Recall) / (Precision + Recall)) * 2 Using the values from the table, the calculation is ((.73 * .61) / (.73 + .61)) * 2 = 0.66 so the F score of the first row is 0.66. Note that if you average the F scores across a table for a multiclass algorithm (as in this example), the result will typically be close to the precision. But it\'92s useful to look at the F score for each class to see if any of these have wildly different recall results. Case study 2: Faethm 217 9.1.5 Next steps Now that resumes are being automatically classified, WorkPac can build and roll out chatbots that are trained on a particular job type to get metadata from candidates (such as work history and experience). This allows their recruiters to focus their efforts on the highest-value aspects of their jobs, rather than spending time gathering information about the candidates. 9.1.6 Lessons learned One of the time-consuming aspects of a machine learning project is getting the data to feed the model. In this case, the data was locked away in resume documents in PDF format. Rather than spending time building their own OCR data-extraction service, Blackbook.ai solved this problem by using a commercial resume data-extraction service. This allowed them to get started right away at a low cost. If the cost of this service becomes too high down the track, a separate business case can be prepared to replace the OCR service with an in-house application. To train the machine learning model, Blackbook.ai also required metadata about the existing documents. Getting this metadata required information to be extracted from WorkPac\'92s systems using SQL queries, and it was time consuming to get this data from WorkPac\'92s internal teams. Both WorkPac and Blackbook.ai agreed this should have been done in a single workshop rather than as a series of requests over time. 9.2 Case study 2: Faethm Faethm is a software company with artificial intelligence (AI) at its core. At the heart of Faethm\'92s software is a system that predicts what a company (or country) could look like several years from now, based on the current structure of its workforce and the advent of emerging technologies like machine learning, robotics, and automation. Faethm\'92s data science team accounts for more than a quarter of their staff. 9.2.1 AI at the core What does it mean to have AI at the core of the company? Figure 9.2 shows how Faethm\'92s platform is constructed. Notice how every aspect of the platform is designed to drive data to Faethm\'92s AI engine. Faethm combines their two main data models\'97Technology Adoption Model and Workforce Attribution Model\'97with client data in their AI engine to predict how a company will change over the coming years. 9.2.2 Using machine learning to improve processes at Faethm This case study doesn\'92t focus on how Faethm\'92s AI engine predicts how a company will change over the coming years. Instead, it focuses on a more operational aspect of their business: how can it onboard new customers faster and more accurately? Specifically, how can it more accurately match their customers\'92 workforce to Faethm\'92s job 218 CHAPTER 9 Case studies categorization? This process fits section 4, Contextual Client Data, which is shown in Faethm\'92s Platform Construct (figure 9.2). Figure 9.3 shows a company\'92s organizational structure being converted to Faethm\'92s job classification. Correctly classifying jobs is important because the classified jobs serve as a starting point for Faethm\'92s modeling application. If the jobs do not reflect their customer\'92s current workforce, the end result will not be correct. At first glance, this looks like a challenge similar to what WorkPac faced, in that both Faethm and WorkPac are classifying jobs. The key difference is the incoming data: WorkPac has 20 years of labeled resume data, whereas Faethm has only a few years of job title data. So Faethm broke the project down into four stages: 
\f2 \uc0\u61601 
\f0  Stage 1\'97Get the data 
\f2 \uc0\u61601 
\f0  Stage 2\'97Identify features 
\f2 \uc0\u61601 
\f0  Stage 3\'97Validate the results 
\f2 \uc0\u61601 
\f0  Stage 4\'97Implement in production Every aspect of the company feeds data to its Al engine. Figure 9.2 Every aspect of Faethm\'92s operating model drives data toward its AI engine. Title 1 Customer workforce Faethm job classification Title 2 Title 3 Title 4 Title 5 Title 6 Job A Job B Job C Figure 9.3 Job description categorization funnel Case study 2: Faethm 219 9.2.3 Stage 1: Getting the data When Faethm started operations in 2017, the team manually categorized their customers\'92 job title data. Over time, it developed several utility tools to speed up the process, but categorizing the job titles for incoming clients still required manual effort from expert staff. Faethm wanted to use its considerable machine learning expertise to automate this process. Faethm decided to use SageMaker\'92s BlazingText algorithm. This was due, in part, to the fact that BlazingText handles out-of-vocabulary words by creating vectors from sub-words. The first problem Faethm needed to surmount was getting sufficient training data. Instead of waiting until it had manually classified a sufficient number of clients, Faethm used its utility tools to create a large number of classified job titles similar to, but not exactly the same as, existing companies. This pool of companies formed the training dataset. An additional complication with Faethm\'92s data was that their classes were imbalanced. Some of the jobs they classified into titles had hundreds of samples (Operations Manager, for example). Others had only one. To address this imbalance, Faethm adjusted the weighting of each category (like you did using XGBoost in chapter 3). Now armed with a large labeled dataset, Faethm could begin building the model. What are out-of-vocabulary words? As discussed in chapter 4, BlazingText turns words into a string of numbers called a vector. The vector represents not only a word, but also the different contexts it appears in. If the machine learning model only creates vectors from whole words, then it cannot do anything with a word that it hasn\'92t been specifically trained on. With job titles, there are lots of words that might not appear in training data. For example, the model might be trained to recognize gastroenterologist and neuroradiologist, but it can stumble when it comes across a gastrointestinal radiologist. BlazingText\'92s sub-word vectors allow the model to handle words like gastrointestinal radiologist, for example, because it creates vectors from gas, tro, radio, and logist, even though these terms are only sub-words of any of the words the model is trained on. Training data You might not have to worry about labeling your data. WorkPac was able to use undersampling and oversampling to balance their classes because they had 20 years of labeled data. When you are looking at machine learning opportunities in your business, you might find yourself in a similar position in that the processes most amenable to implementing machine learning are those that have been done by a person for a long time, and you have their historical decisions to use as training data. 220 CHAPTER 9 Case studies 9.2.4 Stage 2: Identifying the features Once they had the data, Faethm looked at other features that might be relevant for classifying job titles into roles. Two features found to be important in the model were industry and salary. For example, an analyst in a consulting firm or bank is usually a different role than an analyst in a mining company, and an operations manager earning $50,000 per year is likewise a different role than an operations manager earning $250,000 per year. By requesting the anonymised employee_id of each employee\'92s manager, Faethm was able to construct two additional features: first, the ratio of employees with each title who have direct reports; and second, the ratio of employees with each title who have managers reporting to them. The addition of these two features resulted in a further significant improvement in accuracy. 9.2.5 Stage 3: Validating the results After building the model in SageMaker, Faethm was able to automatically categorize a customer\'92s workforce into jobs that serve as inputs into Faethm\'92s predictive model. Faethm then classified the workforce using its human classifiers and identified the anomalies. After several rounds of tuning and validation, Faethm was able to move the process into production. 9.2.6 Stage 4: Implementing in production Implementing the algorithm in production was simply a matter of replacing the human decision point with the machine learning algorithm. Instead of making the decision, Faethm\'92s expert staff spend their time validating the results. As it takes less time to validate than it does to classify, their throughput is greatly improved. 9.3 Conclusion In the case studies, you progressed from a company taking its first steps in machine learning to a company with machine learning incorporated into everything it does. The goal of this book has been to provide you with the context and skills to use machine learning in your business. Throughout the book, we provided examples of how machine learning can be applied at decision points in your business activities so that a person doesn\'92t have to be involved in those processes. By using a machine learning application, rather than a human, to make decisions, you get the dual benefits of a more consistent and a more robust result than when using rules-based programming. In this chapter, we have shown different perspectives on machine learning from companies using machine learning today. In your company, each of the following perspectives is helpful in evaluating which problems you should tackle and why. Summary 221 9.3.1 Perspective 1: Building trust WorkPac and Blackbook.ai made sure that the projects had achievable and measurable outcomes, delivered in bite-sized chunks throughout the project. These companies also made sure they reported progress regularly, and they weren\'92t overpromising during each phase. This approach allowed the project to get started without requiring a leap of faith from WorkPac\'92s executive team. 9.3.2 Perspective 2: Geting the data right There are two ways to read the phrase Getting the data right. Both are important. The first way to read the phrase is that the data needs to be as accurate and complete as possible. The second way to read the phrase is that you need to correctly build the process for extracting the data and feeding it into the machine learning process. When you move into production, you need to be able to seamlessly feed data into your model. Think about how you are going to do this and, if possible, set this up in your training and testing processes. If you automatically pull your data from source systems during development, that process will be well tested and robust when you move to production. 9.3.3 Perspective 3: Designing your operating model to make the most of your machine learning capability Once you have the ability to use machine learning in your company, you should think about how to you can use this functionality in as many places as possible and how you can get as many transactions as possible flowing through the models. Faethm\'92s first question when considering a new initiative was probably, \'93How can this feed our AI engine?\'94 In your company, when looking at a new business opportunity, you\'92ll want to ask, \'93How can this new opportunity fit into our existing models or be used to bolster our current capability?\'94 9.3.4 Perspective 4: What does your company look like once you are using machine learning everywhere? As you move from your first machine learning projects to using machine learning everywhere, your company will look very different. In particular, the shape of your workforce will change. Preparing your workforce for this change is key to your success. Armed with these perspectives and the skills you picked up as you\'92ve worked your way through the chapters in this book, we hope you are ready to tackle processes within your own company. If you are, then we\'92ve achieved what we set out to do, and we wish you every success. Summary 
\f2 \uc0\u61601 
\f0  You followed WorkPac as they embarked on their first machine learning project. 
\f2 \uc0\u61601 
\f0  You saw how Faethm, an experienced machine learning company, incorporated machine learning into yet another of its processes. 222 appendix A Signing up for Amazon AWS AWS is Amazon\'92s cloud service. At the time of writing this book, it has a larger share of the cloud service market than either Microsoft or Google, its two biggest competitors. AWS consists of a large number of services ranging from servers to storage to specialized machine learning applications for text and images. In fact, one of the most difficult aspects of using a cloud service is understanding what each of the components does. While you are encouraged to explore the breadth of AWS services, for this book, we are only going to use two of them: S3 (AWS\'92s file storage service) and SageMaker (AWS\'92s machine learning platform). This appendix takes you through setting up an AWS account, appendix B takes you through setting up and using S3, and appendix C takes you through setting up and using SageMaker. If you have an AWS account already, skip this appendix and go to appendix B, which shows you how to configure S3. You\'92ll need to provide your credit card number to get an AWS account, but you won\'92t be charged until you exceed your free tier limits. NOTE You should be able to do all the exercises in this book without going over the free tier limits offered to new accounts. Signing up for AWS 223 A.1 Signing up for AWS To sign up for AWS, go to the following link: https://portal.aws.amazon.com/billing/signup Once there, click Sign Up and walk through the prompts. The first form (figure A.1) asks you for an email address, a password, and a username. Next, choose an account type (we used Personal for this book) and information about your location (figure A.2). 1. Enter your details. 2. Choose a name for your AWS account. Figure A.1 Creating an AWS account: Step 1 (entering an email address, a password, and a username) 224 APPENDIX A Signing up for Amazon AWS Next, enter your credit card details (figure A.3). 1. Click Personal. 2. Enter your name and other details. Figure A.2 Creating an AWS account: Step 2 (choosing an account type) Enter your credit card details (you won\'92t be charged until you exceed your free tier). Figure A.3 Creating an AWS account: Step 3 (entering credit card details) Signing up for AWS 225 The next pages\'92 forms relate to verifying your account. Likely, as a provider of compute services, AWS is subject to attempts by technically savvy users trying to get free services. Accordingly, there are a few steps to the verification process. The first verification step is a security check by typing annoyingly displayed characters (figure A.4). Clicking Call Me Now kicks off the second verification step, where you\'92ll receive an automated call from AWS. It will ask you to key in the four-digit code that is displayed on the page (figure A.5). 1. Provide your phone number. 2. Enter the security check characters. Figure A.4 Verifying your account information: matching captcha characters 226 APPENDIX A Signing up for Amazon AWS You\'92re now verified. You can continue on to the next step (figure A.6). You\'92ll receive a call from AWS asking you to key in the number you see on your screen. Figure A.5 Verifying your account information: entering the four-digit code Click Continue. Figure A.6 Creating an AWS account: Step 4 (identity verification confirmation) Signing up for AWS 227 Next, you select the plan you want (figure A.7). The Free plan will suffice for working through this book. Congratulations, you\'92re now signed up (figure A.8). You can sign into the AWS console and move on to appendix B to learn how to set up S3 (AWS\'92s file storage service) and SageMaker (AWS\'92s machine learning service). But first a word on AWS charges. Figure A.7 Creating an AWS account: Step 5 (selecting a support plan) 228 APPENDIX A Signing up for Amazon AWS A.2 AWS Billing overview AWS charges by the second for using resources such as SageMaker, the machine learning service we use in the book. When you open a new AWS account, for the first 12 months, you get free use of the resources you will need to work through the exercises in this book. There are limits on the amount of resources available to you; but the free tier will be sufficient to complete all the exercises in this book. NOTE You can get up-to-date pricing information for AWS services on this page: https://aws.amazon.com/sagemaker/pricing/. If you have an existing AWS account, you will be charged for your use of AWS resources. However, if you are careful to ensure you shut down your resources when you are not using them, you can complete all the exercises in this book spending only US$10\'96$20 on AWS resources. Click to sign in. Figure A.8 Creating an AWS account: Success! 229 appendix B Setting up and using S3 to store files S3 is AWS\'92s file storage system. Throughout this book, you\'92ll use S3 to store your data files for machine learning and your machine learning models after you create them in SageMaker. This appendix walks you through how to set up a bucket to hold your code for the examples in this book. NOTE If you haven\'92t already signed up for Amazon Web Services, go to appendix A, which provides detailed information on how to do this. To log into the AWS console, go to http://console.aws.amazon.com, and enter your email address and password. Once you have logged in, you will see an AWS Services heading. In the text box under AWS Services, type S3 to find the S3 service, then press R on your keyboard. AWS uses the concept of buckets to identify where you store your files. The first thing you\'92ll do when you go to S3 is to set up a bucket to store your files for this book. If you have already created your bucket, when you go to S3, you should see a bucket list that shows your bucket (figure B.1). You\'92ll create a folder in this bucket for each of the datasets you work with in this book. This is good practice for all of your work. Use buckets to separate your work by who should have access to it, and use folders to separate your datasets. B.1 Creating and setting up a bucket in S3 You can think of a bucket as a top-level folder in a directory. AWS calls them buckets because they are globally unique. This means that you cannot have a bucket with the same name as a bucket someone else has created. The advantage of this is that each bucket can be assigned a unique address on the web and can be navigated 230 APPENDIX B Setting up and using S3 to store files to by anyone who knows the name of the bucket (of course, you would need to give them access to the bucket before they could get to the bucket or see anything in it). When you first open the S3 service in a new account, you are notified that you don\'92t have any buckets (figure B.2). To create your first bucket, click Create Bucket. You are now asked to provide information about your bucket. A wizard walks you through four steps: 
\f1 \uc0\u9632 
\f0  Name your bucket. 
\f1 \uc0\u9632 
\f0  Set properties for your bucket. 
\f1 \uc0\u9632 
\f0  Set permissions. 
\f1 \uc0\u9632 
\f0  Review settings. Click the bucket where you are storing your datasets. Figure B.1 List of buckets in S3 used to store the code and data used in this book You see this screen if you don\'92t have any buckets yet. Figure B.2 S3 Dashboard before you have created any buckets Creating and setting up a bucket in S3 231 B.1.1 Step 1: Naming your bucket Figure B.3 shows step 1 of the wizard that helps you create your bucket. In this step, you name your bucket and say what region you want your bucket in. For the purposes of the exercises in this book, create a bucket name that is something unique (like your name) followed by mlforbusiness. If someone has already created a bucket with the same name, you might need to add some random numbers after your name. NOTE The name of your bucket can only contain characters that can appear in a valid web address. This means it can\'92t contain spaces. It is also common practice to use a dash (-) to separate words. Change the region to US East (N. Virginia). SageMaker is in the process of being rolled out across all regions, but it is not available in all regions yet. So, for the purposes of this book, use US East. Enter the name of your bucket. Choose the US East region. Figure B.3 Step 1 of the bucket creation wizard: Name your bucket 232 APPENDIX B Setting up and using S3 to store files AWS regions such as US East do not refer to where you can access the services from. It refers to where the AWS servers physically reside. When you select US East as the region, it means that you can use the AWS S3 bucket from anywhere, but that the server sits somewhere on the east coast of the United States (North Virginia to be more precise). B.1.2 Step 2: Setting properties for your bucket Next, you need to set properties for your bucket. Figure B.4 shows step 2. This is where you say how you want files in the bucket to be versioned, logged, and tagged. No need to change anything here, so click Next. Click Next. Figure B.4 Step 2 of the bucket creation wizard: Set properties Creating and setting up a bucket in S3 233 B.1.3 Step 3: Setting permissions Permissions allow you to determine who can access your bucket. Figure B.5 shows step 3. For most purposes, you probably only want yourself to access your bucket, so you can leave the permissions as they are set by default. Click Next on this page too. B.1.4 Step 4: Reviewing settings Here you can check your settings and make any required changes, as shown in figure B.6. If you followed the previous instructions, you don\'92t need to make any changes, so click Create Bucket. Once you click Submit, you are taken back to S3. Figure B.7 shows the bucket you have just created. Now that you have a bucket set up, you can set up folders in your bucket. Click Next. Figure B.5 Step 3 of the bucket creation wizard: Set permissions 234 APPENDIX B Setting up and using S3 to store files Click Create bucket. Figure B.6 Step 4 of the bucket creation wizard: Review settings Click the bucket where you are storing your datasets. Figure B.7 List of buckets in S3 including the bucket you just created Setting up folders in S3 235 B.2 Setting up folders in S3 In the previous section, you created a bucket to hold all of your files and code for this book. In this section, you set up a folder to hold your files and code for chapter 2. Once you get the hang of this, you can easily set up folders for the other chapters. You can think of a bucket in S3 as a top-level folder. The folder that you\'92ll create in this appendix is a subfolder of that top-level folder. In this book, you\'92ll see terminology such as \'93a folder\'94 when describing the contents of a bucket, but this terminology is not entirely accurate. In reality, there is no such thing as a folder in an S3 bucket. It looks like there is in the user interface, but an S3 bucket doesn\'92t actually store things hierarchically. It is more accurate to say that a bucket in S3 is a web location that you can easily restrict access to. Every file that sits in that S3 bucket sits at the top level of the bucket. When you create a folder in S3, it looks like a folder, but it is simply a file stored at the top level of a bucket named in such a way that it looks like a folder. For example, in the bucket you set up in this appendix, you will create a folder called ch02 and put in it a file called orders_with_predicted_value.csv. In reality, you are just creating a file of that name in your bucket. To use more accurate terminology, the file name is a key, and the file contents are a value. So a bucket is just a web location that stores key/value pairs. You are going to create a separate folder within the bucket you just created for each of the machine learning datasets you work with. To begin with, click Create Bucket, and then click Create Folder and name it ch02, as shown in figure B.8. Click Create folder. Figure B.8 Create a folder in S3. 236 APPENDIX B Setting up and using S3 to store files After you\'92ve named your bucket (figure B.9), click Save. Once you are returned to the S3 page, you should see that you are in the bucket you just created, and you have a folder called ch02, as shown in figure B.10. Name the folder ch02. Figure B.9 Name the folder in S3. Click the folder. Figure B.10 Click into the folder in S3. Uploading files to S3 237 Now that you have a folder set up in S3, you can upload your data file and start setting up the prediction model in SageMaker. B.3 Uploading files to S3 To upload your data file, after clicking the folder, download the data file at this link: https://s3.amazonaws.com/mlforbusiness/ch02/orders_with_predicted_value.csv Then upload it into the ch02 folder by clicking Upload, as shown in figure B.11. Once you have uploaded the file, it will appear in S3 (figure B.12). In the next appendix, you will learn how to set up AWS SageMaker. Click Upload. Figure B.11 Upload the data into S3. After uploading, the file appears here. Figure B.12 Dataset listing on S3 238 appendix C Setting up and using AWS SageMaker to build a machine learning system SageMaker is Amazon\'92s environment for building and deploying machine learning models. Let\'92s look at the functionality it provides. SageMaker is revolutionary because it 
\f1 \uc0\u9632 
\f0  Serves as your development environment in the cloud so you don\'92t have to set up a development environment on your computer 
\f1 \uc0\u9632 
\f0  Uses a preconfigured machine learning model on your data 
\f1 \uc0\u9632 
\f0  Uses inbuilt tools to validate the results of the machine learning model 
\f1 \uc0\u9632 
\f0  Hosts your machine learning model 
\f1 \uc0\u9632 
\f0  Automatically sets up an endpoint that takes in new data and returns predictions C.1 Setting up To begin, you need to set your AWS region to a region provided by SageMaker. Figure C.1 shows the dropdown menu you use to select an AWS region to deploy SageMaker into. Set this region to US East (N. Virginia). The SageMaker interface lets you work with four main components: 
\f1 \uc0\u9632 
\f0  Dashboard\'97Your SageMaker home 
\f1 \uc0\u9632 
\f0  Notebook instances\'97An EC2 server that hosts your notebook Creating a notebook instance 239 
\f1 \uc0\u9632 
\f0  Models\'97The machine learning models that you create in the Jupyter notebook 
\f1 \uc0\u9632 
\f0  Endpoints\'97An EC2 server that hosts your model and allows you to make predictions First, you\'92ll set up SageMaker to work with your data. The next section takes you through how to do this. Then, you\'92ll see how to start using SageMaker and how to upload the file you\'92ll work with in chapter 2. You\'92ll also learn how to access the file. C.2 Starting at the Dashboard When you first navigate to the SageMaker service, you can see a workflow that contains an orange button that reads Create Notebook Instance. Click it to set up a server to run your Jupyter notebooks. C.3 Creating a notebook instance Figure C.2 shows the fields you need to complete to set up your notebook instance. The first field sets your notebook instance name. You\'92ll use the same instance name as you work through all the chapters in the book. We\'92ve called ours mlforbusiness. Next is the notebook instance type (the type of AWS server that will run your Jupyter notebook). This sets the size of the server that your notebook will use. For the datasets you\'92ll use in this book, a medium-sized server is sufficient, so select ml.t2.medium. The third setting is the IAM role. It\'92s best to create a new role to run your notebook instance. Click Create New Role, give it permission to access any S3 Bucket by selecting the option with that label, and click Create Role. After this, you can accept all the rest of the defaults. 1. Click the region displayed in the top-right corner of AWS. 2. Select US East (N. Virginia) as your region. Figure C.1 Select the US East AWS region to deploy SageMaker into. 240 APPENDIX C Setting up and using AWS SageMaker to build a machine learning system AWS and resources AWS servers come in a variety of sizes. Unless you are on the Free plan (which you can be on for 12 months after signing up), AWS charges you for the computer resources you use; this includes AWS servers. Fortunately, they charge you by the second. But you might want to make sure that you use the smallest servers you can get away with. For the exercises in this book, the ml.t2.medium server is sufficient for working with our datasets. At the time of writing, the cost of this server is less than US$0.05 per hour. You can view current prices by clicking this link: https://aws.amazon.com/ec2/ pricing/on-demand. Enter a name for your notebook instance. Choose ml.t2.medium. Create a new IAM role to run your notebook instance. Figure C.2 To create a new instance, you need to complete three fields of information: the name of the instance, the instance type, and the IAM role. Uploading the notebook to the notebook instance 241 C.4 Starting the notebook instance You will now see your notebook instance in a list. The status should say Pending for about 5 mins while SageMaker sets up the EC2 server for you. To avoid unexpected charges from AWS, remember to come back here and click the Stop link that appears under Actions once the EC2 server is ready to go. When you see Open appear under Actions, click it. A Jupyter notebook launches in another tab. You are a few steps away from your completing your (first) machine learning model. C.5 Uploading the notebook to the notebook instance When the notebook instance starts, you\'92ll see a table of contents with a couple of folders in it. These folders contain sample SageMaker models, but we won\'92t look at these now. Instead, as shown in figure C.3, create a new folder to hold the code for this book by clicking New and selecting Folder at the bottom of the dropdown menu. When you tick the checkbox next to Untitled Folder, you\'92ll see the Rename button appear. Click this button and change the folder name to ch02. Then click the ch02 folder to see an empty notebook list. Figure C.4 shows the empty notebook list. Just like we have already prepared the CSV data you uploaded to S3, we have already prepared the Jupyter Notebook you will now use. You can download it to your computer by clicking this link: https://s3.amazonaws.com/mlforbusiness/ch02/tech_approval_required.ipynb 2. Select Folder. 1. Select New. Figure C.3 List of notebooks available to you on your SageMaker instance 242 APPENDIX C Setting up and using AWS SageMaker to build a machine learning system Then click the Upload button to upload the tech-approval-required notebook to this folder. After uploading the file, you will see the notebook in your list. Figure C.5 shows your list of notebooks. Click tech-approval-required.ipynb to open it. You are a few keystrokes away from your completing your machine learning model. C.6 Running the notebook You can run the code in one of your notebook cells, or you can run the code in more than one of the cells. To run the code in one cell, click the cell to select it, and then press C+R. When you do so, you\'92ll see an asterisk (*) appear to left of the cell. This means that the code in the cell is running. When the asterisk is replaced by a number, the code has finished running. (The number shows how many cells have run since you opened the notebook.) To run the code in more than one cell (or for the entire notebook), click Cell in the toolbar at the top of the Jupyter notebook, then click Run All. That\'92s it. You\'92re now ready to dive into the scenario in chapter 2 and begin your journey in creating machine learning applications. Click Upload to upload the notebook. Figure C.4 Empty notebook list and Upload button Click tech-approval-required.ipynb. Figure C.5 Notebook list: tech-approval-required.ipynb 243 appendix D Shutting it all down The last step in getting acquainted with SageMaker is to shut down the notebook instance and delete the endpoint you created. If you don\'92t do this, AWS continues to charge you a few cents per hour for the notebook instance and the endpoint. Shutting down the notebook instance and deleting the endpoint requires you to do the following: 
\f1 \uc0\u9632 
\f0  Delete the endpoint. 
\f1 \uc0\u9632 
\f0  Shut down the notebook instance. D.1 Deleting the endpoint To stop your account from being charged for your endpoint, you need to delete the endpoint. But, don\'92t worry about losing your work. When you rerun the notebook, the endpoint will be recreated automatically for you. To delete the endpoint, click Endpoints on the left-hand menu you see when you are looking at the SageMaker tab. Figure D.1 shows the Endpoints menu item on the Inferences dropdown. You will see a list of all of your running endpoints. To ensure you are not charged for endpoints you are not using, you should delete all the endpoints you are not using. (Remember that endpoints are easy to create, so, even if you will not use the Figure D.1 Endpoints menu item 244 APPENDIX D Shutting it all down endpoint for a few hours, you might want to delete it.) Figure D.2 shows a list of endpoints and what an active endpoint looks like. To delete the endpoint, click the radio button to the left of the order-approval name. Then click the Actions menu item and click the Delete menu item that appears, as shown in figure D.3. You have now deleted the endpoint so you will no longer incur AWS charges for it. You can confirm that all of your endpoints have been deleted when you see the text \'93There are currently no resources\'94 displayed on the Endpoints page as shown in figure D.4. Shows endpoint is active Figure D.2 Active endpoints are shown in the endpoint list. Delete any endpoints you are not using. Click Delete. Figure D.3 Dropdown option that allows you to delete an endpoint Shutting down the notebook instance 245 D.2 Shutting down the notebook instance The final step is to shut down the notebook instance. Unlike endpoints, you do not delete the notebook. You just shut it down so you can start it up again, and it will have all the code in your Jupyter notebook ready to go. To shut down the notebook instance, click Notebook Instances in the left-hand menu on SageMaker. Figure D.5 shows the Notebook Instances option in the SageMaker menu. To shut down the notebook, you just click the Stop link. SageMaker will take a couple of minutes to shut down. Figure D.6 shows the stop notebook action. After it has shut down, you can confirm the notebook instance is no longer running by checking Status to ensure it says Stopped (figure D.7). Endpoint successfully deleted Figure D.4 Verifying that all of your endpoints have been deleted Click Notebook Instances. Figure D.5 Notebook instances menu item in SageMaker menu 246 APPENDIX D Shutting it all down Congratulations. You have successfully shut down your SageMaker notebook instance(s) and endpoint(s). This will allow you to avoid incurring unnecessary costs. Click Stop. Figure D.6 Shutting down the notebook by clicking the Stop link Check status. Figure D.7 Verifying the notebook instance is stopped and shut down 247 appendix E Installing Python Installing Python on your computer has gotten much easier over the past few years. If you use Microsoft Windows, you can install it directly from the Windows Store at this link: https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p For the purposes of this book, you can accept all the defaults. If you are using a macOS or Linux machine, you probably already have Python installed, but it might be Python 2.7 rather than Python 3.x. To install the most recent version, go to the Download Python page at https://www.python.org/downloads The website will show you a Download Python button that should be set for the operating system version you need. If it does not detect your operating system, there are links to the right versions lower on the page. Again, for the purposes of this book, accept all the defaults, and head back to chapter 8. 249 index Symbols * (asterisk) 33, 242 % symbol 134 %% command 135 %matplotlib inline 135 - (dash) 231 A .abs() function 38 access key 198 accuracy of data 221 of predictions 157\'96158 AI (artificial intelligence) 217 algorithms 220 Amazon API Gateway 201 Amazon Resource Name (ARN) 196 Amazon SageMaker dashboard 239 endpoints on 189\'96196 adding code to serve 205\'96206 for running notebooks 194\'96196 for uploading data 193\'96194 for uploading notebooks 191\'96193 folders on 166 notebook instances creating 239\'96240 starting 241 notebooks on running 242 setting up 60\'9661, 86, 115, 141 uploading 166 uploading to notebook instances 241\'96242 overview of 18\'9619 setting up 19\'9620, 238\'96239 Amazon Simple Storage Service. See S3 Amazon Transcribe 18 Amazon Web Services. See AWS API endpoints, serverless 197\'96201 configuring credentials 200\'96201 setting up AWS credentials on AWS accounts 198\'96199 setting up AWS credentials on local computers 199\'96200 app.py file 204 area under the curve (AUC) 57\'9658, 69 ARN (Amazon Resource Name) 196 artificial intelligence (AI) 217 asterisk (*) 33, 242 AUC (area under the curve) 57\'9658, 69 auc argument 68 automation defined 9 importance of 8\'9610 productivity and improving with machine learning 9\'9610 overview of 9 AWS (Amazon Web Services) accounts on 198\'96199 billing overview 228 buckets in 167 credentials on AWS accounts 198\'96199 on local computers 199\'96200 overview of 18\'9619 signing up for 223\'96227 AWS Access Key ID 200 AWS CLI library 200 AWS console 30, 229 AWS DeepLens 18 AWS Lambda 189 AWS Secret Access Key 200 AWS Services heading 229 B best-of-breed systems 5, 7 BidEnergy 129, 166 bigrams 82, 94 binary mode 41 binary target variable 13 binary trees 106 binary:logistic 42, 57, 68 Blackbook.ai 214, 216 BlazingText algorithm 83\'9684, 91, 95, 211, 219 boto3 AWS library 63, 88, 117, 142 250 INDEX boto3 library 34, 62, 200 buckets 229 in AWS 167 in S3 166\'96167, 231\'96233 naming 231\'96232 reviewing settings for 233 setting permissions 233 setting properties for 232 C categorical data 165 categorical features 27 categorical variables 13 chalice deploy command 203, 208 Chalice library 196\'96197 deploying 208 installing 202\'96204 chalice new-project tweet_escalator 204 charting predictions 154\'96156 time-series data 134\'96139 creating multiple charts 138\'96139 displaying columns of data with loops 137\'96138 charts 138\'96139 chatbots 217 churn_data.csv file 62, 66 churning. See customers at risk of churning cloud-based servers (EC2) 18 comma separated files 117 command-line interface (CLI) library 200, 202 concat function 123 confidence argument 155 configuring credentials 200\'96201 models 175\'96179 permissions 207 confusion matrix 72 containers 94 context_length parameter 152, 178 continuous features 27 continuous variables 13 corr function 37 correlation 37 Create New Role option 239 Create Notebook Instance button 239 credentials configuring 200\'96201 for AWS on AWS accounts 198\'96199 on local computers 199\'96200 CSV format 40\'9641, 114 cur_row list 91 customers at risk of churning building models for 61\'9673 creating test datasets 65\'9667 creating training datasets 65\'9667 creating validation datasets 65\'9667 examining data 62\'9664 hosting models 70 loading data 62\'9664 organizing data 65 testing models 70\'9673 training models 67\'9669 decisions about 50 deleting endpoints 73\'9674 preparing datasets 52\'9654 calculating changes 54 normalizing data 53 preparing to build models for 58\'9661 setting up notebooks on SageMaker 60\'9661 uploading datasets to S3 59 process flow for 50\'9651 shutting down notebook instances 74 verifying deletion of endpoints 74 XGBoost 54\'9658 AUC (area under the curve) 57\'9658 overview of 54\'9657 D dash (-) 231 dashboard in Amazon SageMaker 238\'96239 data accuracy of 221 columns of 137\'96138 examining 32\'9635, 62\'9664, 87\'9690, 116\'96119, 142\'96143 loading data 34\'9635 setting up notebooks 34 viewing data 34\'9635 extracting 221 for sending purchase orders 27\'9628 in folders on S3 167 loading 32\'9635, 62\'9664, 87\'96 90, 116\'96119, 142\'96143 setting up notebooks 34 viewing data 34\'9635 normalizing 53 organizing 36\'9639, 65, 90\'9693, 120\'96121, 170\'96172 converting datasets 144\'96145 missing values 145\'96146 viewing data 146\'96147 receiving 219 time-series charting 134\'96139 incorporating with DeepAR 163\'96164 loading Jupyter Notebook for 133 uploading 193\'96194 viewing 34\'9635 data_bucket value 32 DataFrame 34\'9635, 38, 44, 65, 89, 117, 121, 137, 147, 153 datasets downloading from S3 buckets 166\'96167 for escalating incidents 78\'9679 for forecasting power usage 134\'96139 creating multiple charts 138\'96139 displaying columns of data with loops 137\'96138 for questioning invoices 103\'96104 for testing 39\'9641, 65\'9667, 147\'96150, 172\'96175 for training 39\'9641, 65\'9667, 93, 121, 147\'96150, 172\'96 175 for validation 39\'9641, 65\'9667, 93, 121 importing 169\'96170 incorporating into models 164\'96165 uploading to AWS buckets 167 uploading to S3 59, 85, 114, 141 INDEX 251 decision making pattern-based 11 rules-based 10\'9611 decisions serving 208\'96210 serving over web 188\'96189 with machine learning 10\'9612 machine learning to improve business systems 12 pattern-based decision making 11 rules-based decision making 10\'9611 DeepAR 140, 145, 147, 149, 153, 158, 212 incorporating related timeseries data 163\'96164 periodic events 161\'96163 DeepARPredictor class 153 default app.py 205 deleting endpoints 46\'9647, 73\'96 74, 97\'9698, 126\'96127, 158\'96 159, 182\'96183, 243\'96244 deploying Chalice 208 detect_suspicious_lines.ipynb notebook 115 df. random_state dataset 66 df.head() function 35 df.plot() command 135 Download Python page 247 downloading datasets from S3 buckets 166\'96167 notebooks 165\'96166 drop function 65 dynamic data 165 E early_stopping hyperparameter 94 early_stopping_rounds hyperparameter 68\'9669 EC2 server 239, 241 embedding processes 216 encode() function 41 encoding data 65 endpoints deleting 46\'9647, 73\'9674, 97\'96 98, 126\'96127, 158\'96159, 182\'96183, 243\'96244 on SageMaker 189\'96190 adding code to serve 205\'96206 setting up 190\'96196 on web 201\'96208 adding code to serve SageMaker endpoint 205\'96206 configuring permissions 207 creating Hello World API 204\'96205 deploying Chalice 208 installing Chalice 202\'96204 updating requirements.txt 207\'96208 serverless API endpoints, setting up 197\'96201 configuring credentials 200\'96201 setting up AWS credentials on AWS accounts 198\'96199 setting up AWS credentials on local computers 199\'96200 Endpoints component, SageMaker 239 end-to-end enterprise software systems 5 end-to-end systems 7 ensemble machine learning model 55 enumerate function 137\'96138 epochs 93\'9695 ERP (Enterprise Resource Planning) systems 12, 212 error rate 118 escalating incidents BlazingText 83\'9684 building models for 86\'9696 creating training datasets 93 creating validation datasets 93 examining data 87\'9690 hosting models 95\'9696 loading data 87\'9690 organizing data 90\'9693 testing models 96 training models 93\'9695 decisions about 77 deleting endpoints 97 NLP (natural language processing) 79\'9683 creating word vectors 80\'9682 including words in groups 82\'9683 preparing datasets for 78\'9679 preparing models for 84\'9686 setting up notebooks on SageMaker 86 uploading datasets to S3 85 process flow for 77\'9678 shutting down notebook instances 97 verifying deletion of endpoints 97\'9698 estimator hyperparameters 68 estimator variable 42 eval_metric hyperparameter 68 events. See periodic events in DeepAR Excel, Microsoft 40, 51 except block 44 extracting data 221 Extreme Gradient Boosting. See XGBoost F F Scores 215\'96216 Faethm 217\'96221 false positive (FP) values 58 false positive rate 57 fastText algorithm 83 features 13, 27 identifying 220 in decision making 13\'9614 file storage bucket, AWS 19 files, uploading to S3 237 fillna function 145 Flink framework 56 flows. See process flows folders on S3 setting up 235\'96237 to hold data 167 on SageMaker 166 for loop 137 forecasting power usage building models for 141\'96158 creating test datasets 147\'96150 creating training datasets 147\'96150 examining data 142\'96143 hosting models 152\'96153 loading data 142\'96143 making predictions 153\'96158 organizing data 144\'96147 plotting results 153\'96158 training models 150\'96152 252 INDEX forecasting power usage (continued) charting time-series data 134\'96139 creating multiple charts 138\'96139 displaying columns of data with loops 137\'96138 decisions about 129\'96132 deleting endpoints 158 loading Jupyter Notebook for time-series data 133 neural networks 139\'96140 preparing datasets for 134\'96139 creating multiple charts 138\'96139 displaying columns of data with loops 137\'96138 preparing models for 140\'96141 setting up notebooks on SageMaker 141 uploading datasets to S3 141 shutting down notebook instances 158 verifying deletion of endpoints 159 FP (false positive) values 58 Free plan, AWS 240 freq variable 148 frontier firms 9 functions 14 G get_dummies function 36\'9637, 40, 80, 120 get_execution_role function 34 get_prediction function 44, 70 gradient boosting 55 H Hadoop 56 head function 63, 65, 120 .head() function 89 header argument 40 Hello World API 204\'96205 hosting models 43\'9644, 70, 95\'9696, 122\'96123, 152\'96153 hyperbole adjective noun pattern 83 hyperbole noun noun pattern 83 hyperparameters 42\'9643, 68\'9669, 93 I IAM role 203, 239\'96240 importing datasets 169\'96170 modules 34 improving forecasts building models for 168\'96181 configuring models 175\'96179 creating test datasets 172\'96175 creating training datasets 172\'96175 importing datasets 169\'96170 making predictions 179\'96181 organizing data 170\'96172 plotting results 179\'96181 setting up notebooks 168\'96169 setting up servers to build models 175\'96179 DeepAR incorporating related timeseries data 163\'96164 periodic events 161\'96163 deleting endpoints 182 incorporating datasets into models 164\'96165 preparing models for 165\'96167 downloading datasets from S3 buckets 166\'96167 downloading notebooks 165\'96166 setting up folders on S3 to hold data 167 setting up folders on SageMaker 166 uploading datasets to AWS bucket 167 uploading notebooks to SageMaker 166 shutting down notebook instances 183 verifying deletion of endpoints 183 index=False argument 41 indicies variable 146 installing Chalice 202\'96204 instance type 239\'96240 instances of notebooks creating 239\'96240 shutting down 47\'9648, 74, 97, 126, 158, 183, 245\'96246 starting 241 uploading notebooks to 241\'96242 interpolate function 171 invoices. See questioning invoices J JSON 88, 117, 142, 147 Jupyter Notebook 19, 29\'9645, 86, 241 creating training datasets 39\'9641 creating validation datasets 39\'9641 downloading 60 examining data 32\'9635 hosting models 43\'9644 loading data 32\'9635 loading for time-series data 133 organizing data 36\'9639 overview of 19 testing models 44\'9645 training models 41\'9643 K Kaggle 78 key/value pairs 235 keyword classification system 214 L labeled datasets 17 Lambda function 201 libsvm format 40 list comprehension 149 loading data 32\'9635, 62\'9664, 87\'9690, 116\'96119, 142\'96143 Jupyter Notebook 133 local computers, AWS credentials on 199\'96200 loops 137\'96138 INDEX 253 M m5.large server 43 machine learning 10\'9611, 28 automation and 8\'9610 decision making with 10\'9614 features 13\'9614 pattern-based decision making 11 rules-based decision making 10\'9611 target variables 13 designing operating models for 221 improving business systems with 12 improving processes with 217\'96218 improving productivity with 9\'9610 overview of 14\'9617 preparing workforce for 221 SageMaker for creating notebook instances 239\'96240 dashboard 239 running notebooks 242 setting up 19\'9620, 238\'96239 starting notebook instances 241 uploading notebooks to notebook instances 241\'96242 seeking approval for 17\'9618 tools for 18\'9619 AWS 18\'9619 Jupyter Notebook 19 SageMaker 18\'9619 training models 28\'9629 machine learning algorithm 29 magic commands 135 MAPE (Mean Absolute Percentage Error) 157\'96159, 175, 179, 183 mape function 180 matplotlib library 134\'96135, 138 median function 124 Meter data link 166 meter_data.csv file 142 Microsoft Excel 40, 51 Mikolov\'92s algorithm 81 min_epochs hyperparameter 94 models configuring 175\'96179 for customers at risk of churning 58\'9673 creating test datasets 65\'9667 creating training datasets 65\'9667 creating validation datasets 65\'9667 examining data 62\'9664 loading data 62\'9664 organizing data 65 setting up notebooks on SageMaker 60\'9661 uploading datasets to S3 59 for escalating incidents 84\'9696 creating training datasets 93 creating validation datasets 93 examining data 87\'9690 loading data 87\'9690 organizing data 90\'9693 setting up notebooks on SageMaker 86 uploading datasets to S3 85 for forecasting power usage 140\'96158 creating test datasets 147\'96150 creating training datasets 147\'96150 examining data 142\'96143 loading data 142\'96143 making predictions 153\'96158 organizing data 144\'96147 plotting results 153\'96158 setting up notebooks on SageMaker 141 uploading datasets to S3 141 for improving forecasts 165\'96181 creating test datasets 172\'96175 creating training datasets 172\'96175 downloading datasets from S3 buckets 166\'96167 downloading notebooks 165\'96166 importing datasets 169\'96 170 making predictions 179\'96181 organizing data 170\'96172 plotting results 179\'96181 setting up folders on S3 to hold data 167 setting up folders on SageMaker 166 setting up notebooks 168\'96169 uploading datasets to AWS bucket 167 uploading notebooks to SageMaker 166 for questioning invoices 114\'96125 creating training datasets 121 creating validation datasets 121 examining data 116\'96119 loading data 116\'96119 organizing data 120\'96121 setting up notebooks on SageMaker 115 uploading datasets to S3 114 hosting 43\'9644, 70, 95\'9696, 122\'96123, 152\'96153 incorporating datasets into 164\'96165 machine learning 28\'9629 preparing 215\'96216 setting up servers to build 175\'96179 testing 44\'9645, 70\'9673, 96, 123\'96125, 215\'96216 training 41\'9643, 67\'9669, 93\'96 95, 121\'96122, 150\'96152 Models component, SageMaker 239 multi-class target variables 13 multidimensional vectors 81 N naming buckets 231\'96232 natural language processing. See NLP negatively correlated values 37 neural networks 129, 139\'96140 neural-net-based models 151 New Terminal option 199 Newton boosting 56 N-grams 94 254 INDEX NLP (natural language processing) 79, 82\'9683, 211 creating word vectors 80\'9682 including words in groups 82\'9683 NLTK library 88, 91, 191 normalizing data 53 Notebook Instance Type setting 239 Notebook Instances option 97, 166, 245 notebooks downloading 165\'96166 for time-series data 133 in Jupyter 29\'9645 creating training datasets 39\'9641 creating validation datasets 39\'9641 examining data 32\'9635 hosting models 43\'9644 loading data 32\'9635 organizing data 36\'9639 overview of 19 testing models 44\'9645 training models 41\'9643 in SageMaker creating notebook instances 239\'96240 running 242 setting up 60\'9661, 86, 115, 141 starting notebook instances 241 uploading notebooks to notebook instances 241\'96242 running 194\'96196 setting up 34, 168\'96169 shutting down instances 47\'9648, 74, 97, 126, 158, 183, 245\'96246 uploading 166, 191\'96193 num_round hyperparameter 68 num_round parameter 93 num_samples_per_tree parameter 121 num_trees hyperparameter 113, 122 O objective hyperparameter 68 order-approval endpoint 43 organizing data 65, 90\'9693, 120\'96121, 170\'96172 converting datasets 144\'96145 missing values 145\'96146 viewing data 146\'96147 out-of-core computation 56 overfitted model 56 overfitting 68\'9669 P pandas library (pd) 34, 62 patience hyperparameter 94 pattern-based decision making 10\'9611 periodic events in DeepAR 161\'96163 permissions configuring 207 setting 233 plot function 155, 180 plotting results 153\'96158, 179\'96181 calculating accuracy of predictions 157\'96158 charting predictions 154\'96156 making predictions 154 plot_weeks argument 155 POC (proof of concept) 214 positively correlated values 37 precision 125, 215 prediction function 155 prediction length 178 prediction period 148 prediction_length parameter 152 predictions 29\'9645, 153\'96158, 179 calculating accuracy of 157\'96158 charting 154\'96156 creating training datasets 39\'9641 creating validation datasets 39\'9641 examining data 32\'9635 hosting models 43\'9644 loading data 32\'9635 making 179\'96181 organizing data 36\'9639 serving over web creating web endpoints 201\'96208 overview of 189 SageMaker endpoints 189\'96190 serving decisions 208\'96210 setting up SageMaker endpoints 190\'96196 setting up serverless API endpoints 197\'96201 why so difficult 188\'96189 testing models 44\'9645 training models 41\'9643 predictor argument 155 predictor class 153 predictor endpoint 44 predictor function 154 predictor variable 44 preprocess function 91\'9692 process flows 50\'9651 for escalating incidents 77\'9678 for questioning invoices 101\'96102 processes embedding 216 improving with machine learning 217\'96218 productivity improving with machine learning 9\'9610 overview of 9 proof of concept (POC) 214, 216 properties, setting for buckets 232 public datasets 78 purchase orders. See sending purchase orders Python libraries 34 Q questioning invoices anomalies 104\'96105 building models for 115\'96125 creating training datasets 121 creating validation datasets 121 examining data 116\'96119 hosting models 122\'96123 loading data 116\'96119 organizing data 120\'96121 testing models 123\'96125 training models 121\'96122 decisions about 100\'96101 deleting endpoints 126 INDEX 255 questioning invoices (continued) preparing datasets for 103\'96104 preparing models for 114\'96115 setting up notebooks on SageMaker 115 uploading datasets to S3 114 process flow for 101\'96102 Random Cut Forest 106\'96112 shutting down notebook instances 126 supervised versus unsupervised machine learning 105\'96106 verifying deletion of endpoints 126\'96127 R Random Cut Forest algorithm 106\'96113, 120, 123, 211 RandomCutForest function 121 random_state argument 40 rcf variable 121 rcf_endpoint.predict function 123 read_csv function 35, 63 recall 125, 215 Receiver Operator Characteristic (ROC) curve 58 receiving data 219 regularization 56 requirements.txt file 204, 207\'96208 resample function 144 results plotting 153\'96158, 179\'96181 calculating accuracy of predictions 157\'96158 charting predictions 154\'96156 making predictions 154 validating 220 results_above_cutoff DataFrame 124 results_df DataFrame 124\'96125 ROC (Receiver Operator Characteristic) curve 58 rules-based decision making 10\'9611 S S3 (Amazon Simple Storage Service) 175, 193 buckets in 229\'96233 bucket creation wizard 231\'96233 downloading datasets from 166\'96167 setting up folders in 167, 235\'96237 uploading datasets to 59, 85, 114, 141 uploading files to 237 s3fs module 34, 41, 63 s3_input files 41 s3_input function 41 sagemaker library 34, 62 SageMaker. See Amazon SageMaker scale_pos_weight hyperparameter 68 scores_df DataFrame 123 security credentials 198 sending purchase orders data for 27\'9628 decisions about 26\'9627 deleting endpoints 46\'9647 making predictions 29\'9645 creating training datasets 39\'9641 creating validation datasets 39\'9641 examining data 32\'9635 hosting models 43\'9644 loading data 32\'9635 organizing data 36\'9639 testing models 44\'9645 training models 41\'9643 running Jupyter Notebook 29\'9645 creating training datasets 39\'9641 creating validation datasets 39\'9641 examining data 32\'9635 hosting models 43\'9644 loading data 32\'9635 organizing data 36\'9639 testing models 44\'9645 training models 41\'9643 shutting down notebook instances 47\'9648 training machine learning models 28\'9629 serverless API endpoints 197\'96201 AWS credentials on AWS accounts 198\'96199 AWS credentials on local computers 199\'96200 configuring credentials 200\'96201 serverless computing 189 serving decisions 188\'96189, 208\'96210 predictions creating web endpoints 201\'96208 overview of 189 SageMaker endpoints 189\'96190 setting up SageMaker endpoints 190\'96196 setting up serverless API endpoints 197\'96201 why so difficult 188\'96189 SageMaker endpoints 205\'96206 sess variable 42 set_hyperparameters function 94 shape function 63, 90 shuffle function 142 single dimensional vectors 80 Site categories dataset 165\'96166 Site holidays dataset 165\'96166 Site maximum temperatures dataset 165 Site maximums link 166 sklearn library 34, 63, 117 sklearn train_test_split module 88 sklearn.confusion_matrix function 72 sklearn.metrices module 63 slugify library 191 SNS (Simple Notification Service) 18 Solow Paradox 8 Spark 56 start element 147 start variable 149 stratify parameter 65 subfolder value 32 sub-word vectors 219 supervised machine learning 16 synthetic data 103 256 INDEX T t2.medium server 44 target element 147 target variables 13, 15 test_data dictionary 150 testing datasets for 39\'9641, 65\'9667, 147\'96150, 172\'96175 models 44\'9645, 70\'9673, 96, 123\'96125, 215\'96216 test_input file 41 time-series data 132 charting 134\'96139 creating multiple charts 138\'96139 displaying columns of data with loops 137\'96138 incorporating with DeepAR 163\'96164 loading Jupyter Notebook for 133 overview of 130\'96132 time-series forecasting 129 timestamp format 148 to_csv function 40, 92 tokenizing text 88, 91 TP (true positive) values 58 train_data dataset 41 trained models 28 training datasets for 39\'9641, 65\'9667, 93, 121, 147\'96150, 172\'96175 machine learning models 28\'9629 models 41\'9643, 67\'9669, 93\'9695, 121\'96122, 150\'96152 training data 92, 219 training_data dictionary 150 train_input file 41 train_test_split function 34, 90 train_test_split module 63 transform_instance function 91 trees 106 trigrams 82, 94 true positive (TP) values 58 true positive rate 57 trust, building 221 try block 44 try-except block 43 tweet variable 206 U unigrams 82, 94 unseen data 69 unsupervised machine learning 16, 105 updating requirements.txt 207\'96208 uploading data 193\'96194 datasets to AWS bucket 167 to S3 59, 85, 114, 141 files to S3 237 notebooks 191\'96193 notebooks to SageMaker 166 URL (Uniform Resource Locator), accessing 205 usages 179 V val_data dataset 41 val_df DataFrame 123 val_df_no_result DataFrame 123 val_df_no_result dataset 123 validating results 220 validation, datasets for 39\'9641, 65\'9667, 93, 121 value_counts function 63, 66, 71, 90, 118 value_counts property 35 variables. See target variables vector_dim hyperparameter 93 vectors 219 vectors. See word vectors verifying endpoint deletion 74, 97\'9698, 126\'96127, 159, 183 VS Code (Visual Studio Code) 197, 199 W web endpoints on 201\'96208 adding code to serve SageMaker endpoint 205\'96206 configuring permissions 207 creating Hello World API 204\'96205 deploying Chalice 208 installing Chalice 202\'96204 updating requirements.txt 207\'96208 serving decisions over 188\'96189 serving predictions over creating web endpoints 201\'96208 overview of 189 SageMaker endpoints 189\'96190 serving decisions 208\'96210 setting up SageMaker endpoints 190\'96196 setting up serverless API endpoints 197\'96201 why so difficult 188\'96189 Windows Store 247 word vectors 80\'9682 word_ngrams hyperparameter 94 word_tokenize function 88 write function 41 X XGBoost (Extreme Gradient Boosting) 29, 36, 40, 50, 53\'9658, 65, 67\'9668, 73, 90, 95, 105, 120, 139, 211 AUC (area under the curve) 57\'9658 overview of 54\'9657 Z zip function 157 Each scenario is divided into five steps that make it easy to run a complete application. Step 1 Read the first part of the chapter to understand the problem you will solve. Download a Jupyter notebook and a dataset. Upload a Jupyter notebook and a dataset to your AWS SageMaker account. Run the Jupyter notebook to see how it works. Read the rest of the chapter to see why it works like it does. Step 2 Step 3 Step 4 Step 5 Hudgeon 
\f1 \uc0\u9679 
\f0  Nichol ISBN-13: 978-1-61729-583-6 ISBN-10: 1-61729-583-3 M achine learning can deliver huge benefi ts for everyday business tasks. With some guidance, you can get those big wins yourself without complex math or highly paid consultants! If you can crunch numbers in Excel, you can use modern ML services to effi ciently direct marketing dollars, identify and keep your best customers, and optimize back offi ce processes. This book shows you how. Machine Learning for Business teaches business-oriented machine learning techniques you can do yourself. Concentrating on practical topics like customer retention, forecasting, and back offi ce processes, you\'92ll work through six projects that help you form an ML-for-business mindset. To guarantee your success, you\'92ll use the Amazon SageMaker ML service, which makes it a snap to turn your questions into results. What\'92s Inside 
\f1 \uc0\u9679 
\f0  Identifying tasks suited to machine learning 
\f1 \uc0\u9679 
\f0  Automating back offi ce processes 
\f1 \uc0\u9679 
\f0  Using open source and cloud-based tools 
\f1 \uc0\u9679 
\f0  Relevant case studies For technically inclined business professionals or business application developers. Doug Hudgeon and Richard Nichol specialize in maximizing the value of business data through AI and machine learning for companies of any size. To download their free eBook in PDF, ePub, and Kindle formats, owners of this book should visit manning.com/books/machine-learning-for-business $39.99 / Can $52.99 [INCLUDING eBOOK] Machine Learning for Business MACHINE LEARNING MANNING \'93A clear and well-explained set of practical examples that demonstrates how to solve everyday problems, suitable for technical and nontechnical readers alike. \'97John Bassil, Fethr \'94 \'93Answers the question of how machine learning can help your company automate processes. \'97James Black \'94 Nissan North America \'93A great resource for introducing machine learning through real-world examples. \'97Shawn Eion Smith \'94 Penn State University \'93Makes AI accessible to the regular business owner. \'97Dhivya Sivasubramanian \'94 Science Logic See first page\
\
\
The Defnitive Guide to Machine Learning for Business Leaders by Hugo Bowne-Anderson The business world is overloaded with buzz terms like artifcial intelligence, machine learning, AI transformation, deep learning, and data science. We know that these felds, technologies, and tools are changing the competitive landscape across verticals and are soon to become more table stakes and foundational than disruptive. However, it\'92s possible to know they\'92re important but not understand what they really mean. If you're confused, that's understandable as these are all loaded terms and they're not even used consistently. My goal here is to dispel any confusion by demystifying these questions: What is artifcial intelligence, machine learning, and data science? Where do they intersect and where do they diverge? 1 CHAPTER 1 The Diference Between AI, Machine Learning, and Data Science Defning Artifcial Intelligence, Machine Learning, and Data Science Artifcial Intelligence (AI) is a \'93a huge set of tools for making computers behave intelligently\'94 and in an automated fashion. This includes voice assistants, recommendation systems, and self-driving cars. Machine Learning (ML) is the \'93feld of study that gives computers the ability to learn without being explicitly programmed\'94. The lion\'92s share of machine learning involves computers learning paterns from existing data and applying it to new data in the form of making predictions, such as predicting whether an email is spam or not, whether a customer will churn or not, and diagnosing a particular piece of medical imaging. 2 1 Data Science (DS) is about making discoveries and creating insights from data, and communicating these insights and discoveries to non-technical stakeholders. Machine learning feeds into both artifcial intelligence and data science If the output of your machine learning model is fed into a computational system that performs an action in an automated fashion\'97such as recommending a movie, decelerating a self-driving car, or serving search results\'97it can be viewed as a component in your AI system. If the output of your machine learning model is fed into a human decision making process, it can be considered data science work. For example, when predicting a customer may churn results in a human deciding to incentivize the customer to stay, this insight or discovery informs a data science decision. Much of machine learning and artifcial intelligence relies on high quality data, meaning the most impactful and efective artifcial intelligence will stand on the shoulders of robust data science capabilities. Andrew Ng, co-founder of Google Brain and former Chief Scientist at Baidu. 1 Arthur Samuel, pioneer in artifcial intelligence and computer gaming. 2 How are these related? Artifcial Intelligence In his Coursera course AI for Everyone, Andrew Ng, co-founder of Google Brain and former Chief Scientist at Baidu, defnes artifcial intelligence as \'93a huge set of tools for making computers behave intelligently.\'94 This defnition casts a wide net and it\'92s worth providing some examples to make clear what \'93behaving intelligently\'94 means: \'95 Voice assistants, such as Siri \'95 Recommendation systems, such as Netfix \'95 Self-driving cars \'95 Drones that fy over felds and capture footage to optimize crop yield \'95 Google Search \'95 Surfacing algorithms, such as those employed by Twiter and Facebook, that decide what content to show you in your feed 3 ScienceSof, an IT consulting company, provides a useful breakdown of the types of artifcial intelligence. Source: ScienceSof It\'92s important to recognize that artifcial intelligence means that actions and decisions are automated. It\'92s also key to note that all of these are examples of artifcial narrow intelligence, or algorithms that can do one thing well. This is not to be confused with artifcial general intelligence, which is a hypothetical, futuristic artifcial intelligence that can do anything a human is capable of. Nor is it a superintelligent artifcial intelligence, a hypothetical sofware agent whose intelligence surpasses that of humans. Both artifcial general intelligence and superintelligent artifcial intelligence are a long way of, if at all possible, and serve as distractions for real, present, and necessary conversations around the capabilities and limitations of artifcial intelligence as we know it today, resulting in headlines such as An AI god will emerge by 2042 and write its own bible. WIll you worship it? This hypothetical intelligence is clearly absurd and distracts from all the current examples of artifcial intelligence that allow computers to perform tasks that mimic aspects of human intelligence, such as recognizing stop signs and people in images and videos (selfdriving cars), holding basic conversations, retrieving information, performing tasks (voice assistants), and ranking text documents based on their relevance to a particular query (Google Search). If artifcial intelligence is a huge set of tools, what tools are we talking about? Let\'92s explore the tool of central importance to modern artifcial intelligence\'97machine learning.3 4 Note that there are parts of artifcial intelligence that do not leverage machine learning: for instance, logical programming and automated reasoning (which can be used for proving mathematical theorems). 3 "Artifcial intelligence is not to be confused with artifcial general intelligence, which is a hypothetical, futuristic artifcial intelligence that can do anything a human is capable of." Machine Learning Machine learning powers recommendation systems, content discovery, search engines, email spam flters, and many other \'93matching\'94 problems in tech. In healthcare, it\'92s being leveraged for drug discovery and high throughput diagnostic imaging diagnosis. In fnance, machine learning is now foundational for fraud detection, process automation, algorithmic trading, and robo-advisory. In retail, Walmart is at the forefront of using machine learning to reinvent supply chain management. The list goes on. So what actually is it? Machine learning was a term popularized in 1959 by Arthur Samuel, a pioneer in artifcial intelligence and computer gaming. Samuel defned it as the \'93feld of study that gives computers the ability to learn without being explicitly programmed.\'94 5 Supervised learning The majority of machine learning involves computers learning paterns from existing data and then applying it to new data in the form of making predictions. Examples include: \'95 Predicting whether a customer will churn \'95 Predicting whether an email is spam or not, given the email sender, subject, and body \'95 Predicting the diagnosis of a particular piece of medical imaging \'95 Predicting outcomes of sports games These prediction and classifcation problems are the most important ML techniques for business leaders to know in the short and medium term\'97referred to as supervised learning. The patern of the label that you\'92re trying to predict\'97such as spam or not\'97is said to supervise the learning process. The power of modern machine learning rests frmly on having good quality data for your algorithm to learn from or be \'93trained on,\'94 and that such training data needs to be labeled. In the spam classifcation example, you\'92ll need many emails labeled with whether they were spam or not. In the diagnostic imaging example, you\'92d require at least thousands of images labeled with their diagnosis. 6 Then, your ML algorithms are able to pick up paterns in your training data and generalize those paterns to unlabeled data, where you don\'92t know the outcome that you\'92re trying to predict. It\'92s for this reason that mathematician and Stanford Professor David Donoho prefers the term recycled intelligence over artifcial intelligence for machine learning, as no new intelligence is created, whereas human intelligence, as captured by humans with domain expertise hand-labeling datasets, is recycled and re-applied to new data. There is a huge and hidden supply chain behind the worlds of machine learning and artifcial intelligence. Companies and individuals leverage services such as Amazon Mechanical Turk to crowd-source labeled data. And Scale AI, a start-up that works with tens of thousands of contractors worldwide to hand-label data, recently raised $100 million, which speaks to growing market needs. Unsupervised learning Unsupervised learning is about discovering general paterns in data. The most popular example is clustering or segmenting customers and users. This type of segmentation is generalizable and can be applied broadly, such as to documents, companies, and genes. Anomaly detection is usually formulated as an unsupervised learning problem. Anomaly detection algorithms help you pinpoint which observations are out of the ordinary or very diferent from others. For example, if you have a dataset with a lot of transactions and would like to identify which ones may be fraudulent. Of course, most data comes unlabeled. This makes unsupervised learning useful in every domain, and it\'92s only grown in importance over the years. There is a lot to be learned from the data in an unsupervised manner. An expert in the feld, Yann LeCun, famously compared machine learning to a cake. In his analogy, reinforcement learning is the fnishing touch\'97the cherry on the cake. Supervised learning gives the data some extra depth\'97it\'92s the icing on the cake. Unsupervised learning is the most substantial part of understanding data\'97it\'92s the cake itself. Too many people don't perform enough unsupervised learning and apply supervised learning blindly. 7 Reinforcement learning Although the vast majority of artifcial intelligence and machine learning relies on labeled data and recycling intelligence contained therein, there is a growing subfeld of machine learning called reinforcement learning that relies far less, if at all, on pre-existing training data. With reinforcement learning, which draws on behavioral psychology, sofware agents are placed in constrained environments and given \'93rewards\'94 and \'93punishments\'94 based on their activity. If playing games sounds like a relevant application of reinforcement learning to you, you\'92re spot on: it was how AlphaGo Zero became the world Go champion in 2017, beating AlphaGo, which was trained on human data. More recently, in 2019, Pluribus beat the best professional players in six-player no-limit Texas Hold\'92em poker. Reinforcement learning also has meaningful applications in other felds like self-driving vehicles and algorithmic trading, and we\'92ll defnitely see more applications in coming years. In some domains like text and image processing, we have massive amounts of data to learn from, but the catch is that they are usually unlabeled. An efective way to mine this data is to use self-supervised learning techniques. You can think of selfsupervised learning as the missing link between supervised and unsupervised learning. In self-supervised learning, humans don't explicitly label the data. Instead, intrinsic labels are added to the data creatively by considering the problem domain. For example, with natural language processing, massive sets of text-based documents are collected from the web, and for each sentence, the middle word is replaced by a blank. It\'92s then used as a label. Finally, a model is trained to predict the middle word from its context. Self-supervised learning enables models to learn intricate paterns without the need for human-labeled data. Is Machine Learning a Form of Artifcial Intelligence? So is machine learning a form of artifcial intelligence? It is commonly regarded as a form of artifcial intelligence, but if we\'92re thinking of artifcial intelligence as a \'93set of tools for making computers behave intelligently,\'94 then ML becomes one of these tools. For example, the Google spam flter is an example of an AI system, and the ML algorithm that classifes a given email as spam or not is one component of this artifcial intelligence. Another component is the sofware that pushes emails classifed as spam by the ML algorithm into your spam folder. Now that we\'92ve got a handle on artifcial intelligence and machine learning, let\'92s see what data science is all about. Data Science In their seminal 2012 Harvard Business Review article Data Scientist: The Sexiest Job of the 21st Century, Thomas Davenport and DJ Patil state unequivocally that \'93more than anything, what data scientists do is make discoveries while swimming in data.\'94 Data science is about creating insights from data, ofen in a business seting. How do data scientists do this, though? Data science is a multidisciplinary feld that uses scientifc methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. How does this play out in practice? There are so many tools and techniques in a modern data scientist\'92s toolbox that it\'92s helpful to partition the space. One way to slice the data science space is into descriptive analytics, predictive analytics, and prescriptive analytics. 8 Descriptive analytics Also called business intelligence (BI), descriptive analytics is essentially about geting the right pre-existing data in front of the right people, typically in the form of dashboards, reports, or emails. This can include both past and real-time time data about revenue, customer engagement, churn, or company and employee performance. Predictive analytics Predictive analytics is synonymous with machine learning and is the realm of predicting the future, such as whether a customer will churn or not, and more general classifcation tasks: Is an email spam or not? Is a tumor in a diagnostic image benign or malignant? Data scientists might also be interested in why a model makes a prediction, not just the prediction itself. A black-box model, or a model that is created directly from data by an algorithm and therefore doesn't reveal its inner workings to humans, might not be as interesting to them as highly interpretable models. There are also key ethical and regulatory considerations to requiring models to be interpretable. 9 FOCUS AREAS FOR DATA SCIENCE AND ANALYTICS Descriptive analytics What happened? What will happen? Predictive analytics How can we make it happen? Prescriptive analytics Prescriptive analytics Prescriptive analytics is the realm of decision science and decision analytics, and is concerned with how to make decisions based on data. If, for example, your machine learning model tells you that a particular customer will churn, you don\'92t yet know what to do about it. Prescriptive analytics is concerned with facilitating seamless collaboration between people working with data and those making business decisions\'97or fnding a path from data to insights. Exciting spaces to watch are data translation (a burgeoning feld for those with both domain expertise and technical know-how), advances in reinforcement learning (which bleeds into machine learning, as we\'92ve seen), and decision science\'97especially the work of Cassie Kozyrkov, Chief Decision Scientist at Google Cloud, whose work we discussed on DataFramed, the DataCamp podcast. So is machine learning part of data science or part of artifcial intelligence? And what is the relationship between data science and artifcial intelligence? As discussed, these terms are used in a variety of inconsistent ways, but a good rule of thumb is this: 10 \'95 If the output of your machine learning model is fed into a human decision making process, it can be considered data science work. For example, when predicting a customer may churn results in a human deciding to incentivize the customer to stay, this can be considered insights or discoveries made from the data. \'95 If the output of your machine learning model is fed into a computational system that performs an action in an automated fashion\'97such as recommending a movie, decelerating a self-driving car, or serving search results\'97it can be viewed as a component in your AI system. The main distinction between artifcial intelligence and data science is that although many of the tools, techniques, infrastructures, and processes are the same, data science is ofen fed into human decision-making processes while artifcial intelligence is concerned with automation. However, remember that much of machine learning and artifcial intelligence relies on high quality data. This means that the most impactful and efective AI strategies will stand on the shoulders of robust data science capabilities. 11 THE RELATIONSHIP BETWEEN ARTIFICIAL INTELLIGENCE, MACHINE LEARNING, DEEP LEARNING, AND DATA SCIENCE Artifcial intelligence (AI): Programs with the ability to learn and reason like humans Machine learning (ML): Algorithms with the ability to learn without being explicitly programmed Deep learning (DL): Subset of machine learning in which artifcial neural networks adapt and learn from large datasets Data Science: A cross-disciplinary feld that seeks to extract meaningful insights from data Source: corpnce.com Note that there may be overlaps between these felds not shown in the diagram. If the output of your machine learning model is fed into a human decision-making process, it can be considered data science work. If the output of your machine learning model is fed into a computational system that performs an action in an automated fashion, it can be viewed as a component in your AI system. In chapter one, we discovered that machine learning, deep learning, and artifcial intelligence are buzzwords for good reason\'97these technologies are fundamentally shifing the nature of business, society, and our lives. More importantly, across many verticals, they\'92re shifing from being disruptive technologies to being foundational and table stakes for businesses to remain competitive. 12 CHAPTER 2 Machine Learning For Business Leaders The power of machine learning across verticals Now, let\'92s look at several examples of machine learning\'92s impact across various verticals. Machine learning powers recommendation systems, content discovery, search engines, email spam flters, and matching problems. Tech Machine learning facilitates drug discovery and diagnostic imaging diagnosis. Healthcare Machine learning is now foundational for fraud detection, process automation, algorithmic trading, and robo-advisory. Finance Machine learning is reinventing supply chain management by optimizing supply and demand planning, improving shipping processes and reducing transportation expenses, and improving strategic sourcing. Retail Burgeoning industries are growing rapidly with machine learning. \'95 LegalTech is imagining a future in which machine learning is leveraged to predict outcomes of court cases based on natural language analysis of precedents. \'95 AgTech (agriculture technology) is deploying drones at scale to capture footage, and machine learning is being used to estimate crop yields. Other 13 The power of machine learning across teams There are also many gains in the development of ML algorithms that are verticalindependent, supporting diferent business functions. Machine learning helps flter applicants in the hiring fow\'97but hiring models must be carefully monitored so as not to perpetuate social biases at scale. Machine learning is used for call center routing and chatbots. Support ML algorithms are used for paid advertising, customer churn prediction, and targeted nurture campaigns. Marketing HR In fact, any company that has an app can beneft from leveraging machine learning to determine the most efective push notifcations, and any organization that has a website can leverage machine learning to personalize their customer experience by surfacing content and features that are most relevant. Machine learning improves collaboration and helps teams become more nimble in the way they conduct business. 14 The 10 Machine Learning Commandments for Business Leaders 15 According to Gartner\'92s Annual Chief Data Ofcer Survey, poor data literacy is the second-biggest internal roadblock to success for chief data ofcers. Gartner expects that by 2020, 50% of organizations will lack sufcient artifcial intelligence and data literacy skills to achieve business value, and 80% of organizations will initiate targeted data literacy initiatives to overcome defciencies. The data is clear: to keep your competitive advantage, you\'92ll need to leverage machine learning in one form or another. So as a business leader, what do you need to know about it? 1 Embrace the paradigm shif of models learning from data In fact, any company that has an app can beneft from leveraging machine learning to determine the most efective push notifcations, and any organization that has a website can leverage machine learning to personalize their customer experience by surfacing content and features that are most relevant. In Sofware 1.0, or code that is writen by a human, you would write code that explicitly specifes that, were a tumor above a given size and of a certain texture, among other conditions, it would be classifed as malignant. Below a certain size, it\'92s classifed as benign. Andrej Karpathy, the director of artifcial intelligence and Autopilot Vision at Tesla, defnes Sofware 2.0 as \'93code writen by an optimization function based on an evaluation criterion.\'94 16 In Sofware 2.0, you specify the types of algorithms you want to use and feed them labeled data, that is, images that have already been classifed as benign or malignant, and the algorithm discovers paterns in these in order to generalize the classifcation to new, unlabeled data. As mentioned in chapter one, this training data is ofen hand labeled by humans and, for this reason, researchers such as David Donoho prefer the term recycled intelligence to artifcial intelligence because the machine is merely recycling the human intelligence contained in the labeled examples, and not creating any new forms of intelligence. Source: EBC Untrained model Predictive model Labeled input Unlabeled input Labeled output Predicted ouput "The machine is merely recycling the human intelligence contained in the labeled examples, and not creating any new forms of intelligence." 2 Choose your evaluation metric with care In addition to labeled training data, you need to supply a ML model with an evaluation metric, which tells the algorithm what you\'92re optimizing for. One commonly used evaluation metric is accuracy, that is, what percentage of your data your model makes the correct prediction for. This may seem like a great choice: who would want a model that isn\'92t the most accurate? Actually, there are many cases where you wouldn't want to optimize for accuracy\'97 the most prevalent being when your data has imbalanced classes. Say you\'92re building a spam flter to classify emails as spam or not, and only 1% of emails are actually spam (this is what is meant by imbalanced classes: 1% of the data is spam, 99% is not). Then a model that classifes all emails as non-spam has an accuracy of 99%, which sounds great, but is a meaningless model. There are alternative metrics that account for such class imbalances. It is key that you speak with your data scientists about what they\'92re optimizing for and how it relates to your business question. A good place to start these discussions is not by focusing on a single metric but by looking at the confusion matrix of the model, which contains: \'95 False negatives (e.g., real spam incorrectly classifed as non-spam) \'95 False positives (non-spam incorrectly classifed as spam) \'95 True negatives (non-spam correctly classifed) \'95 True positives (spam correctly classifed) Actually Positive (1) False Negatives (FNs) Predicted Negative (0) False Positives (FPs) True Positives (TNs) True Positives (TPs) Predicted Positive (1) Actually Negative (0) CONFUSION MATRIX Source: Glass Box Medicine 17 A lot of atention is currently focused on the importance of the data you feed your ML models and how it relates to your evaluation metric. YouTube had to learn this the hard way: When they optimized for revenue based on view time (how long people stay glued to videos), this had the negative efect of recommending more violent and incendiary content, along with more conspiracy videos and fake news. An interesting lesson here is that optimizing for revenue\'97since viewing time is correlated with the number of ads YouTube can serve you, and thus, revenue\'97may not be aligned with other goals, such as showing truthful content. This is an algorithmic version of Goodhart\'92s Law, which states: \'93When a measure becomes a target, it ceases to be a good measure." The most well-known example is a Soviet nail factory, in which the workers were frst given a target of a number of nails and produced many small nails. To counter this, the target was altered to the total weight of the nails, so they then made a few giant nails. But algorithms also fall prey to Goodhart\'92s law, as we\'92ve seen with the YouTube recommendation system. "An interesting lesson here is that optimizing for revenue\'97since viewing time is correlated with the number of ads YouTube can serve you, and thus, revenue\'97may not be aligned with other goals, such as showing truthful content." 18 Untrained model Labeled input Train data Labeled data Test data Labeled output Predictive model Unlabeled input Predicted ouput Labeled output TRAINING TESTING Performance 3 Remember to split your data The atentive reader may be asking: How do we calculate the accuracy of the model if we\'92ve already used all our labeled data to train it? This is a key and crucial point: If you train your model on a dataset, you\'92d expect it to perform beter on that data than on new data. To get around this, before even training the model, you can split the data into a training set and a test set\'97this procedure is called train test split. We split the data into a training and test set so that we can estimate the model on the training data and evaluate its performance on the test data. This prevents overfting, which occurs when a model tries to predict a trend in data that\'92s too noisy, and makes the model more generalizable. Source: EBC 19 4 Focus on solid data foundations and tooling Having good quality data is a huge challenge in itself! This is why when executives ask me how they can make their companies AI-driven, I respond by showing them Monica Rogati\'92s AI Hierarchy of Needs, which has machine learning close to the top as one of the fnal pieces of the puzzle. This hierarchy illustrates that before machine learning can happen, you need solid data foundations and tools for extracting, loading, and transforming data (ETL), as well as tools for cleaning and aggregating data from disparate sources. AI, Deep Learning A/B Testing, Experimentation, Simple ML Algorithms Analytics, Metrics, Segments, Aggregates, Features, Training data Cleaning, Anomaly Detection, Prep Reliable Data Flow, Infrastructure, Pipelines, ETL, Structured and Unstructured Data Storage Instrumentation, Logging, Sensors, External Data, User Generated Content THE DATA SCIENCE HIERARCHY OF NEEDS Learn/Optimize Aggregate/Label Explore/Transform Move/Store Collect Source: Hackernoon 20 5 Beware of bias in your data and algorithms 6 Pry open the black box of your model Machine learning can only be as good as the data you feed it. If your data is biased, your model will be too. For example, Amazon built a ML recruiting tool to predict the success of applicants based on resumes with ten years\'92 worth of training data that favored males due to historic male dominance across the tech industry\'97which caused the ML tool to also be biased against women. As Cassie Kozyrkov has analogized, a teacher is only as good as the books they\'92re using to teach the students. If the books are biased, their lessons will be too. We\'92ve seen that accuracy\'97the percentage of your data that your model predicts or classifes correctly\'97is not always the best metric to measure the success of your model, such as when your classes are imbalanced (for example, when 99% of emails are spam and 1% non-spam). Another space where metrics such as accuracy may not be enough is when you need your model to be interpretable. Interpretability is the characteristic of being able to say why your model makes the predictions it does, which is necessary for many models deployed in fnancial markets due to regulation. It is also essential for algorithms that impact stakeholders\'92 lives, such as Northpointe\'92s COMPAS recidivism risk model, which is used by judges to make decisions during parole hearings. There\'92s an inherent tradeof between accuracy and interpretability, in that the most accurate models are generally black box and not interpretable since they\'92re created directly from data by an algorithm\'97so even the humans who design them can\'92t understand how variables are being combined to make predictions. Interpretability is an important part of the conversation to be aware of even if your particular industry isn\'92t concerned with it at the moment, since regulation will bring it to many other industries in the coming years. 21 7 Keep tabs on your model and improve it Remember that the job of machine learning doesn\'92t end when your model is in production, making predictions, or performing classifcations. Models that are deployed and doing work still need to be monitored and maintained. If you have a model predicting credit card fraud based on transaction data, you get useful information every time your model makes a prediction and you act on it. On top of this, the activity you\'92re trying to monitor and predict\'97in this case, credit card fraud \'97may be dynamic and change over time. This process, where data that\'92s generated is constantly in fux, is called data drif\'97and it proves how essential it is to regularly update your model. TIME MODEL QUALITY Static models TIME MODEL QUALITY Refresh models Source: DataBricks 8 Delve into applications for deep learning Deep learning is a form of machine learning that uses models called artifcial neural networks, which are loosely inspired by biological neural networks in human brains. As we\'92ve noted in chapter one, this is the extent to which the analogy holds\'97deep learning is not equivalent to human intelligence. And deep learning only applies to artifcial neural networks that have several hidden layers\'97not all artifcial neural networks perform deep learning. This distinction is important because, in practice, very deep models require several parameters to be tuned well. Problems can arise when an artifcial neural network becomes too deep, like exploding or vanishing gradients and the amount of storage required for the model and its huge dataset. Many deep learning applications occur in the supervised learning world, in the form of image classifcation (self-driving cars, drone footage utilized to estimate crop yield in AgTech, facial recognition ).... There are other applications in time-series prediction, such as fnancial prediction problems. Deep learning systems are rarely good at more than one task: an algorithm that is built for self-driving cars will not be any good at classifying legal documents. Although you may like to call deep learning a form of artifcial intelligence, it is only so in the sense of narrow artifcial intelligence. Deep learning doesn\'92t always require labeled data. Deep learning for natural language processing or image classifcation ofen works in a self-supervised way. For instance, NLP systems are trained on bodies of raw text where it is trained to predict the middle word from the context around it. It is self-supervised in the sense that it makes its own labels by taking out words and marking them as labels. 4 Examples of deep learning IMAGE CLASSIFICATION NATURAL LANGUAGE PROCESSING Self-driving cars Drone footage Facial recognition Google translation Document classifcation Sentiment analysis I personally don't see any productive use cases cases for facial recognition that don't introduce far more challenges than they solve and we need to think as a society whether we want to use them at all. 4 23 9 Explore pre-trained models with transfer learning In a world where building competitive ML models relies on state-of-the-art, domainspecifc data, you might be concerned about not having enough data yourself or the ability to collect it. But much of the future of machine learning will involve using pretrained models, or models already trained on other data. This is the world of transfer learning. For example, you can buy a pre-trained image classifcation model that recognizes and classifes cars. With transfer learning, you can fne-tune pre-trained models for your particular question and domain. For example, if you want an algorithm that classifes trucks, you could take one that classifes cars and train it further on truck photo data. Pre-trained model to classify cars Task A: Classify cars Additional information: Truck photo data New model to classify trucks Task B: Classify trucks We are currently seeing the emergence of algorithm marketplaces, such as Booz Allen\'92s Modzy, where you can buy and sell pre-trained models. There are key concerns, however, such as how to think about data and algorithmic bias, along with model governance, when trading algorithms without having access to the datasets that they\'92re trained on. The space is ripe for growth, but it\'92s also ripe for abuse and regulation. 24 10 Embed machine learning into your decision making The fnal point that I cannot stress enough is that machine learning\'97and all data work\'97needs to be directly embedded in the decision function. Machine learning doesn\'92t exist in a vacuum, it\'92s there to serve decision making. This can be automated (as is Google Search), embedded in a scientifc process (as when an algorithm fags an MRI for a specialist to look at), or embedded in organizational processes (such as when decisions are made around what to do with customers who are predicted to churn). You want to make sure that the data work always refects your real-world concerns and that you avoid Type III errors, where you get the right answer but to the wrong question. This is why the data translation space is heating up and why it\'92s so important to establish a culture of data work in your organization. This will require the workforce to understand what data science and machine learning can and can\'92t do, along with how to ask good questions from data professionals. The goal is to establish healthy, productive lines of communication between the data function and the rest of the company at large. Is a given drug efective for treating a particular disease? TYPE I ERROR (FALSE POSITIVE) TYPE II ERROR (FALSE NEGATIVE) TYPE III ERROR (CORRECT ANSWER, WRONG QUESTION) An inefective drug is recommended to treat a particular disease. An efective drug is rejected as a treatment for a particular disease. An efective drug is recommended to treat an unrelated disease. To recap, the 10 machine learning commandments you should follow are: Embrace machine learning as a new paradigm of sofware development in which your models learn from data. 1 Choose your evaluation metric (e.g., accuracy or revenue) with care. Remember to split your data into training and test sets. 2 Focus on solid data foundations and tooling upon which to build your machine learning initiatives. 3 4 Watch out for any bias in your data that may produce biased algorithms. Pry open the black box of your ML models, especially if you\'92re in a regulated market like fnance. 5 There\'92s an inherent trade-of between accuracy and interpretability, but interpretability\'97the characteristic of being able to say why your model makes the predictions it does\'97is becoming more important. 6 Monitor and maintain ML models that have been deployed to prevent data and model drif. 7 Delve into applications for deep learning for your business, which can be very good at specifc supervised learning tasks like image classifcation and natural language processing. 8 Explore the growing feld of transfer learning, which allows you to retrain pretrained models\'97especially if you\'92re concerned about collecting enough appropriate data for a specifc business problem. 9 Tie machine learning and all data work to decision making. This will surface real-world concerns and establish productive lines of communication between the data function and the rest of the company at large. 10 25 In the frst two chapters, we demystifed buzz terms like artifcial intelligence, machine learning, and data science, and took a deep dive into what business leaders need to know about machine learning. Now let\'92s look at what business leaders need to know about their data strategy. The fve things that business leaders need to know about data strategy are: The 80/20 rule for data science. Big data ain't all that big. The future of data work isn\'92t just in coding\'97it'll include even more point-andclick. Data culture is an important piece of your data strategy. Data strategy requires considering the diferent people who are impacted. CHAPTER 3 Five Things Business Leaders Need to Know About Data Strategy 1 2 3 4 5 26 27 Focus on the 20% that maters Vilfredo Pareto (pictured below), while thinking about land ownership in Italy, discovered that 80% of land was owned by 20% of people. This 80/20 rule, also known as the Pareto principle, states that for many events, roughly 80% of the efects come from 20% of the causes. 1 The 80/20 rule for data science 80% of time expended 20% of time 80% of results 20% of results Many trivial tasks Few vital tasks For example, in 2018, roughly 80 to 90% of taxes in the US were paid by the top 20% of earners. It isn\'92t always 80/20, though: 1% of Wikipedia users generate 99% of their content. We've observed with the companies we work with that when applying the Pareto principle to data science, work done in 20% of the time generates approximately 80% of the results. So when prioritizing work in your data strategy, you should focus on what really drives value. 28 To be clear, this might not be work where you see an immediate return on investment. For example, seting up a robust data infrastructure is essential work that doesn\'92t drive immediate return, but will end up demonstrating results. It is key to recognize that we\'92re not talking about large and sexy wins for data involving the hotest deep learning, artifcial intelligence, or machine learning. We\'92re talking about what has business impact and what can really move the needle. Let\'92s look at some examples. Unify your data In any organization, data is ofen siloed in diferent departments. If you have customers who are serviced by diferent departments\'97for example, the marketing team tracks customers in one database and the customer support team tracks them in another\'97there are ofen all types of inconsistencies. Customer names or addresses may be stored using diferent conventions. Developing a unifed data source will move the needle a great deal for future data initiatives. This essentially comes down to data infrastructure. Create accessible dashboards Generating data views and dashboards so that everybody in your organization has access to the data in the way that they need it can be very impactful. Build customer-centric processes Call center routing and customer churn models can be impactful, as can sales propensity models, which will be incredibly important for your sales and marketing teams. Leverage conversational AI The growing feld of conversational AI can have a huge impact even in its most basic form of simple chatbots for customer support. This example is not only a huge win for customer service and support\'97it's also something you can pilot internally and iterate on before releasing into the wild. 29 Make sure your data is actionable Recall that an instructive way to think about data work is by breaking it down into descriptive analytics, predictive analytics, and prescriptive analytics. A huge amount of the value that data can create comes in the form of descriptive analytics\'97if it's strongly tied to the decision function. So when thinking about the 20% of work that can deliver 80% of the value for most businesses, it won't be through AI\'97it will be through descriptive analytics and geting the right data in the hands of the right people. That's taking data the company already has and geting it to the right people in whatever form they can consume it and utilize it, typically via dashboards, reports, and emails. As we'll see later, tying the decision function to this data generation and descriptive analytics function is also a huge cultural challenge, as is making sure that people actually use it. The purpose that data scientists serve in organizations is to analyze data to facilitate answering business questions. Ideally, the decision function is tied to the data function as tightly as possible. When I discussed this with Renee Teate, Director of Data Science at HelioCampus, on the DataFramed podcast, she framed it as follows: We want to go from a business question to a business answer, and we want to factor this through a data question and a data answer. Your stakeholders, managers, and C-suite executives will likely not be concerned with the data question on the data answer. What they want is the business answer with logic to support how you arrived there. This is what it means to tie the data function to the decision function. CALL TO ACTION List three to fve projects based on descriptive analytics that could inform your decision making, and then order them in terms of what projects could have the greatest business impact. 30 Defning big data Big data is one of the buzziest words in the data space. But just how big is big? One way to think about it is in terms of volume, or the amount of data you have. Based on volume, we can defne big data as data that is far more than you can store on a modern laptop or hard drive. Thus, it needs to be distributed across clusters of computers to work with, transmit, and analyze. We can extend this defnition of big data to cover the three Vs: volume, velocity, and variety. Is big data the end of theory? In 2008, Chris Anderson wrote a provocative article in Wired called The End of Theory, The Data Deluge Makes the Scientifc Method Obsolete. The premise was that we had enough data to make satisfactory predictions about the world that we didn\'92t need theory to understand the world. Part of the impetus for such arguments was what we were seeing happen with Google\'97they were able to operate on huge amounts of data and then provide predictive models in the forms of data products for programmatic ad buying. Moreover, they were able to do so with sufciently advanced predictive analytics without needing to understand or theorize about the system under study. This was all about using captured human behavior in order to build beter products and services. Google enabled those who wanted to buy an ad to easily purchase one on Google AdWords, and the model behind AdWords didn't rely on any theory behind whether someone would click or not. Google had just enough data to make a \'93good enough\'94 prediction. Chris Anderson\'92s provocative hypothesis is that big data contains so much information that we don\'92t need to model the world anymore, and we don't need to understand the theory behind it or what's actually happening. 2 Big Data Ain\'92t All That (Big) 31 TIME VISIBILITY Source: Wikipedia Peak of Infated Expectations Plateau of Productivity Slope of Enlightenment Trough of Disillusionment Technology Trigger Has big data actually fulflled its promises to us? One way to think about this is modeling it on the Gartner Hype Cycle. The Gartner Hype Cycle tells us about a technological innovation and the expectations around it as a function of time. We begin with an Innovation Trigger, which in the case of big data was the ability to store, transmit, and analyze large amounts of data. Then people get buzzed about it, leading to Infated Expectations. Afer that, we don't see the value delivered against expectations, so we enter the Trough of Disillusionment. Only afer this do we see the actual real value start to be delivered across several diferent verticals\'97and we enter the Slope of Enlightenment. Where is big data currently in the Gartner Hype Cycle? One way to think about expectations is to see what people have searched for on Google as a function of time. Below are the Google trends for \'93big data\'94 since 2004. You can see that Chris Anderson was ahead of his time\'97but he was wrong about big data being the end of theory due to the importance of small data and thick data. 32 If we accept the Gartner Hype Cycle as a valid model for thinking about big data, and if we accept Google trends as a proxy for expectations, we can see that the Peak of Infated Expectations was around 2014 and we haven't necessarily reached the Trough of Disillusionment yet. Small data is also powerful While a lot of the recent innovations in data science have centered on our ability to efciently handle increasingly large volumes of data, it\'92s important to recognize that a vast majority of the data analyzed in the real world does ft in the memory on a modern laptop. As a business leader, you should carefully consider the needs of your data organization before deciding which tools and architectures to adopt. 33 I want to take you back in time to Johannes Kepler, who discovered three laws of planetary motion, and Tycho Brahe, a Danish astronomer who collected the data that Kepler eventually analyzed to build his three laws of motion, which then informed Newton's theory of gravitation. We have a huge amount of scientifc knowledge developed from the data that Brahe collected, which consisted of around just 2,000 data points. This is a tiny dataset compared to datasets we talk about today, which sometimes contain hundreds of millions of data points. But the data Brahe collected was of high quality. If you have good, properly collected data, strong analytical and principled theoretical models, and a way of doing statistical modeling, you can get a huge amount out of your data. "We have a huge amount of scientifc knowledge developed from the data that Brahe collected, which consisted of around just 2,000 data points. This is a tiny dataset compared to datasets we talk about today, which sometimes contain hundreds of millions of data points." 34 Let\'92s look at another example explored in Andrew Gelman\'92s article, How can a poll of only 1,004 Americans represent 260 million people with only a 3 percent margin of error? Gelman takes you through the math to show that when you increase the amount of data, you get seriously diminishing returns on the reduction of the margin of error. Disclaimer: This result is true only if you have some sort of representative sampling of your population, which defnitely isn't the case in polls. But statisticians now have sophisticated correction methods for non-representative samples, which they can use to learn about the voting preferences of a larger population. Again, we see a signifcant result from a small amount of data, and it tells us about the nature of human behavior and human preferences. The same principle is true of business\'97it's about understanding and being able to predict future human behavior, especially in terms of your stakeholders. So why do we talk about big data so much when it\'92s not necessary? A primary reason is that it's readily accessible and so computable these days. The botom line is Harvard Business Review\'92s point in Sometimes \'93Small Data\'94 Is Enough to Create Smart Products that \'93it's not about mountains of data, it's about small, high precision data.\'94 "We see a signifcant result from a small amount of data, and it tells us about the nature of human behavior and human preferences. The same principle is true of business\'97it's about understanding and being able to predict future human behavior, especially in terms of your stakeholders." 35 Don\'92t underestimate the power of thick data Now that we\'92ve discussed the power of small data, I want to move onto another type of data called thick data, or qualitative data. Thin data involves numbers and tables, whereas thick data, a term from sociology and anthropology, is more qualitative and descriptive. A consulting group called ReD Associates has done fantastic work using thick data to help people build analytic models and machine learning models. One example that I want to mention is their work in detecting credit card fraud. This is a huge challenge and machine learning has been used to detect credit card fraud in the past, using features such as the amount of transaction, the time of transaction, location, and so on. ReD Associates atempted to collect thick data to solve this problem taking a sociological approach. To do so, ReD Associates sat down with credit card fraudsters to fnd out what they actually do and what their processes are like. They found a community of credit card fraudsters on the dark web and met them in real life to learn about their processes and habits. They discovered that the point at which credit card fraudsters have the highest likelihood of geting caught is when they actually have to do something in the real world\'97like picking up deliveries. They\'92re tech savvy enough to rarely be detectable online. And fraudsters are also careful in the physical realm\'97they typically don\'92t send parcels to their own address, their work address, or their friends\'92 addresses. Instead, they send deliveries to addresses of properties that are abandoned or on the real estate market. "Thin data involves numbers and tables, whereas thick data, a term from sociology and anthropology, is more qualitative and descriptive." 36 Equipped with this knowledge, ReD Associates built a credit card fraud detection model using the location that the parcel was being sent to as a feature, and joined that with publicly available data around abandoned houses and houses on the market. They observed that this model based on qualitative data obtained a signifcant lif in accuracy when compared with more traditional models for fraudulent transactions. This is a wonderful example of the importance of thick data and how taking a sociological approach can provide increased value. I recommend these two articles to explore how good quality data is more important than big data in further detail: The Power of 'Thick' Data and Big Data Is Only Half the Data Marketers Need. CALL TO ACTION Choose a data source that's valuable to your business and think about how much of this data you really need to inform decision making. One way you can gauge this is by considering the added value of increasing the amount 2X, 5X, and 10X, particularly with respect to the investment of collecting, storing, and analyzing it. Then, answer this question: What thick data could you use to enhance the quality of this data source? "They observed that this model based on qualitative data obtained a massive increase in accuracy when compared with more traditional models for fraudulent transactions." 37 One of the main reasons behind the rapid innovation and advances in data science has been the proliferation of open-source code-centric tools. This has allowed data scientists and sofware engineers to build powerful abstractions that make it really easy to analyze data. For example, R packages like dplyr and tidymodels allow users to manipulate data and build sophisticated machine learning models with a few lines of code. Similarly, Python packages like tensorfow and pytorch allow anyone to build a deep learning model with litle efort. While code-driven tools have served data work well, I believe that the future of data work will be much broader, and involve graphical user interfaces (GUIs) that will let a broader audience engage in data science using point-and-click tools. Note that this does not by any means imply the death of the coding data scientist. Similar to the industrial revolution, this move will allow coders to redirect their focus to solving more complex problems, and building interfaces to tackle problems, leaving the domain-specifc work to the domain experts. This is already happening across diferent slices of data work: descriptive, predictive, and prescriptive analytics. Descriptive analytics is accessible with business intelligence Descriptive analytics has been made accessible to a wider audience by the dragand-drop business intelligence (BI) tools that are everywhere now. Just look at last year\'92s $2.6 billion acquisition of Looker, a very popular BI tool, by none other than Google. Also last year, Salesforce completed the acquisition of Tableau, another very popular BI tool. Furthermore, Microsof has invested in its own tool, Power BI. (DataCamp ofers courses on both Tableau and Power BI.) Business intelligence tools make data discovery accessible for all skill levels\'97not just advanced analytics professionals. They are one of the simplest ways to work with data, providing the ability to collect data in one place, gain insight into what will move the needle, forecast outcomes, and much more. The Future of Data Work Includes Even More Point-and-Click 3 38 How drag-and-drop interfaces reduce the barrier to entry for predictive and prescriptive analytics The democratization of data work is evolving to also include predictive and prescriptive analytics. For example, Alteryx is a $10 billion dollar company that provides a drag-and-drop data science platform that allows business analysts to manipulate data and build predictive models using a drag-and-drop interface. Google is investing a huge amount in their AutoML tool, which makes \'93AI as simple as drag and drop.\'94 We see that Microsof is also entering the space: Above is an example of a basic regression model with a data pipeline. You can see that the data fows from rectangle to rectangle. First data is ingested, then columns are selected, the data is cleaned, and the model splits and trains the data. 39 There are many potential use cases for drag-and-drop machine learning. This can be reframed as: what type of people in an organization would want to use ML tools that aren't coders? Perhaps your customer success team wants to model customer churn. They\'92ll be able to do this using automated, drag-and-drop tools to forgo writing new models and new code every time a new customer comes in. Similarly, your Chief People Ofcer and HR team will be using more and more automated machine learning models in hiring fows, including screening resumes (we\'92ll get to the potential perils of this in a minute). And your marketing team may be interested in using such tools for the marketing funnel, which changed with the advent of ML tools for programmatic ad buying. One other telling example, is supply chain optimization, which can greatly reduce costs within an organization and is relatively low-hanging fruit. This trend can also be observed in complex spaces like deep learning. For example, Lobe is a startup that makes deep learning accessible to all, allowing users to build models using a GUI. Lobe was acquired by Microsof even before they came out of a beta, which is testament to the importance of GUI tools in shaping the future. Another example is an image search engine built by Google that lets medical doctors explore model predictions and fne-tune them further with human intelligence. Such tools can increase adoption with great efect by opening up the black box and supplementing ML models with human intelligence. We're going to see more drag-and-drop interfaces that will reduce the barrier to entry. This will allow us all to think more about which types of tools we want to build in-house and which ones we want to source from vendors. "There are many potential use cases for drag-anddrop machine learning. This can be reframed as: what type of people in an organization would want to use ML tools that aren't coders?" 40 Algorithmic marketplaces allow non-coders to purchase models In chapter two, we mentioned that we\'92ll see more and more marketplaces for these ML products, like Booz Allen\'92s Modzy. Modzy is a point-and-click marketplace where you can buy models that others have created and incorporate them into your tech stack. There are dangers to allowing non-coders and non-statisticians to use these tools. Namely, we must ensure that everyone knows of the inherent risks associated with these tools and how to mitigate them. I hope that increasingly, product development advancements in point-and-click tools will focus on helping people recognize when they\'92re given biased results, and not to take them at face value. Beware of the danger zone of drag-and-drop ML I encourage you to check out an interesting educational game out of Mozilla Labs, Survival of the Best Fit. It\'92s a 10-minute educational game about hiring bias in AI where you get to play a CEO who atempts to scale the hiring process with an automated tool that causes hiring practices to go wrong. To recap: we'll see more data work done in GUIs, via drag-and-drop and point-andclick interfaces. The space is geting more and more competitive with key investments by big players like Google and Microsof, with much more to come. And even if a lot of your data science work is currently done in-house, more data work is increasingly shifing to vendors\'97which means there needs to be widespread education of potential risks and consequences to adopting AI. CALL TO ACTION Write down how much of the day-to-day work in your organization currently happens in code. You could write this in human hours or how much it actually costs your organization. Then calculate how much is happening in GUIs, pointand-click, and drag-and-drop interfaces. How do you see this changing over the next 24 months? 41 Now it\'92s time to dive into how data culture is a key component of data strategy. When I spoke with Taras Gorishnyy about data science at McKinsey, I asked him to identify the key moving parts for data science, AI, and data strategy within an organization. The key moving parts Taras identifed were: What do we mean when we talk about establishing a data culture? As Tanya Cashorali said to me on DataFramed, "Everyone at any level, whether it's C-level or entry level, should be looking and diving into data the same way you were expected to start using email 20 years ago." THE STEPS TO DATA FLUENCY \'95 Vision for analytics \'95 Robust data foundations \'95 A distribution of data skills and a data culture \'95 Executive support \'95 Establishing the impact of analytics early on 4 Data Strategy Means Data Culture 42 Build data fuency across your organization What implications does this have on data culture and data fuency? A data-fuent organization is one in which everybody knows how to dive into the data that they need to do their job. For example, our VP of Marketing might not need to write SQL code or any code at all, but to do her job well, she does need to know how to access the analytics dashboards and interact with them. She also needs to know how to ask the right questions of data scientists and how to use the results that they give her. The way we think about this at DataCamp is summarized in the fgure below: FINANCE AND ACCOUNTING SALES HUMAN RESOURCES PRODUCT MARKETING Data Scientist Data Engineer Data Analyst Business Analyst Other Departments Data Fluency Programming Fundamentals Statistics Fundamentals Data Visualization Importing & Cleaning Data Reporting Data Manipulation Machine Learning Advanced Statistics AI & Deep Learning Advanced Programming Data Engineering TOPICS TO MASTER Company 100% data fuent A strong data foundation requires starting with a data engineer, and then hiring data scientists and data analysts. Then, we can bring business analysts into the fold, along with other departments. This process helps everyone become data literate, leading to the company becoming 100% data fuent. Basic understanding of data tools and resources across a company greatly improves the quality of interaction among colleagues, allows teams to make beter requests, and empowers everyone to make decisions autonomously. 43 In order to gauge the state of data fuency across industries, we conducted a survey in which we asked over 300 organizations to what extent they have taken actions to become data fuent, summarized in the fgure below: On the lef, we have the mature companies with mature data fuency competencies, and on the right, immature competencies. What I want to highlight is that of the mature companies, only 39% had implemented process redesign and culture change with respect to data and only 7% of the immature companies had. There's a huge amount of improvement to be made here. "Of the mature companies, only 39% had implemented process redesign and culture change with respect to data and only 7% of the immature companies had." Mature DF competencies, N=150 Immature DF competencies, N=95 0% 10% 20% 30% 40% 50% 60% 44% 6% Create a vision for analytics aligned with company strategy 35% 7% Establish strong Clevel support 32% 12% Extract value early from at least several use cases 39% 7% Implement process redesign and culture change 43% 20% Build data infrastructure to provide integrated, trusted, and timely data To what extent has your company taken (or plans to take) the following actions to become data fuent? 44 Involve your employees It is important to prepare your employees for the future of work when crafing your data strategy. This requires ongoing conversations with all of your employees about what type of tasks you think will be automated and what won't, how your employee base can coexist with the tasks that are automated, and what that human-machine interface actually looks like. For some roles, we will see job automation, but the bigger challenge across the board will be task automation, since certain parts of someone's job may be automated away. I think it's far beter to fgure out how to upskill and re-skill these employees to transition them to do more high-level, meaningful work. Prioritize upskilling employees Another question we asked in our survey of 300 organizations was, \'93What type of business challenges prevent companies from building or improving data fuency?\'94 The majority of organizations fagged their difculty in hiring top talent, as so much data talent chooses to work in tech these days. This means you won\'92t be able to hire as many unicorns as you\'92d like, so you should prioritize upskilling and reskilling everyone in your organization. To what extent do the following business challenges prevent your company from building/improving data fuency? 0% 10% 20% 30% 40% 50% 18% 34% Lack of digital strategy 30% 39% Complexity of core business operations 18% 36% Lack of C-level support 29% 46% Difculty hiring top talent 20% 34% Instability of business plans Already invested in DF, N=157 Future investment in DF, N=114 To build data skills, many of the organizations we surveyed have considered the following options: \'95 Establishing data universities or center of excellence \'95 In-person training \'95 Online learning platforms \'95 Internal campaigns to promote and incentivize learning \'95 Recruiting \'95 Sourcing solutions from vendors What actions has your company taken to build data skills? Mature DF competencies, N=150 Immature DF competencies, N=95 0% 37% 15% 60% 57% 64% 62% 51% 37% 49% 48% 35% 23% 10% 20% 30% 40% 50% 60% 70% Established a data university/Center of Excellence Provide on-site/in-person training Provided access to online learning platforms Launched internal campaigns to promote/incentivize learning Recruited employees with existing, relevant skills in data fuency Sourced solutions from a vendor to bridge gaps in data fuency CALL TO ACTION List three to fve outputs of data work in your organization and audit them with respect to how much your organization actually uses them to inform decision making. Too ofen, organizations hire a lot of great data analysts and data scientists to build dashboards and machine learning models, only to see them not be utilized in the intended way due to cultural challenges. All this good work can be wasted, resulting in a poor return on investment. In polls I\'92ve conducted on DataCamp webinars, over 50% of respondents have said that less than 50% of their data work was actually used to inform decision making. This leakage is due to culture! 46 As with any other business strategy, data strategy requires you to consider who you're impacting and how you are impacting them. This is especially important with data strategy, since products are built and business decisions are implemented at a large scale and can impact many stakeholders. The three elements to carefully explore while evaluating your data strategy are bias, fairness, and privacy. Explore vulnerability to bias and acknowledge your blind spots It\'92s tempting for businesses to use AI models to maximize profts, but they must consider whether their models are biased before deploying them. Responsible corporate citizens make eforts to mitigate any biases in their models. Tech companies are especially vulnerable to AI bias. They create a lot of value when building products and services at scale, but they can also create signifcant damage. For example, last year the New York Times reported that the Apple Card was discriminating against women applying for credit, and there was litle transparency on how these algorithms were built to decide which ofers to extend to individual applicants. This led New York State regulators to investigate the algorithm that Apple used to determine creditworthiness. Whether intentional or not, discriminatory treatment of protected classes may violate laws and damage a company\'92s brand, and such algorithms will increasingly become subject to regulation. Thinking about how to implement diferent models in diferent contexts is incredibly important. Data is not one-size-fts-all\'97insights must consider who the stakeholders are and whether vulnerable groups may experience harmful consequences. Abeba Birhane\'92s The Algorithmic Colonization of Africa uncovers the misappropriation of \'93mining\'94 people for data, which hearkens back to \'93the colonizer atitude that declares humans as raw material free for the taking." Data Strategy Means Considering Your Stakeholders 4 47 ProPublica\'92s Machine Bias is now a notorious example of a model that purportedly predicted recidivism rates for people on parole. These risk assessments were used in parole hearings and are becoming increasingly common in courtrooms across America. ProPublica demonstrated that the model was actually biased against Black people, assigning them disproportionately high risk scores, and that this was happening at scale. As a result of trends like these, Cathy O\'92Neil, author of Weapons of Math Destruction, said, \'93Data science doesn't just predict the future, it causes the future.\'94 This machine bias example illustrates that you have data science work as an input into the future of whether somebody gets parole or not. When we discussed this, O'Neil mentioned that she is working in the space of algorithmic audits and proposing a tool called the Ethical Matrix, in which the rows are the stakeholders and the columns are the concerns. Court Black Defendants White Defendants Public Northpointe Efciency Fairness False +s\'92 False -s\'92 Transparency Predictive Parity Consistency Data Quality Source: KDnuggets Red denotes areas of big concern\'97for example, data quality seems to be a major concern across stakeholder groups, while predictive parity and fairness seem to be impacting Black defendants disproportionately. Understanding these metrics across groups of stakeholders is important to beter empathize across groups and build unbiased models. There will inevitably be groups that are impacted that we overlook. To protect against this, we must also consider who decides who the stakeholders are and what the concerns are. It\'92s a good idea to consult as many people as possible, especially for companies building products at scale and developing large-scale data strategy. Listing your stakeholders and concerns is the frst step. 48 Measure fairness implications for your algorithms Since much of machine learning is supervised learning, where previous data is used to train models, this can perpetuate systemic biases and stereotypes that hinder fairness. For example, consider a machine learning model that aids banks in their decision to provide credit or loans. Such models are typically built to maximize economic profts. However, proft maximization can result in a model that unfairly lowers the percentage of loans provided to marginalized groups. There are several examples from the popular press that highlight these issues, like Google AI\'92s visual exploration of Measuring Fairness. Another example is IBM\'92s AI Fairness 360 toolkit, which computes various bias metrics for a machine learning model and suggests modifcations to the algorithm to mitigate bias. "Proft maximization can result in a model that unfairly lowers the percentage of loans provided to marginalized groups." Source: AI Fairness 360 49 Prioritize privacy to safeguard your stakeholders One blind spot that\'92s easy to overlook is the stakeholders behind data that you collect. This group requires protection from abuse in regard to privacy rights and consumer protection\'97which will increasingly take the form of regulation. In recent years, we\'92ve seen landmark data privacy regulations become implemented, like GDPR and CCPA. GDPR has substantially changed the game in Europe, and CCPA is a similar standard adopted in the state of California. Businesses owe it to their stakeholders to adopt principled thinking around how to build their products and services and construct their data strategies. CALL TO ACTION Choose a current data initiative at your company and list all the stakeholders impacted by it. Compare your notes with a colleague to examine who you may have missed. Then list two ways that each stakeholder could be positively impacted by the data initiative and two ways they could be negatively impacted. \
\
MIT SLOAN SCHOOL OF MANAGEMENT MIT COMPUTER SCIENCE AND MACHINE LEARNING IN BUSINESS ARTIFICIAL INTELLIGENCE LABORATORY (CSAIL) MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE Incorporate machine learning into your business strategy, and explore the impact and value of this transformative technology. MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE ABOUT THIS COURSE How will machine learning affect your business? How can your company capitalize on this dynamic technology? This 6-week online program from the MIT Sloan School of Management and the MIT Computer Science and Artifcial Intelligence Laboratory (CSAIL) will help you to answer these questions. Machine learning, a branch of artifcial intelligence, is the science of programming computers to improve their performance by learning from data. Dramatic progress has been made in the last decade, driving machine learning into the spotlight of conversations surrounding disruptive technology. This online executive program aims to demystify machine learning for the modern business professional \'96 offering you a frm, foundational understanding of the advantages, limitations, and scope of machine learning from a management perspective. MIT Faculty will guide you to understand the current and future capabilities of this transformative technology, in order to effectively unlock its potential within business. You\'92ll also have the opportunity to design a strategic roadmap for the successful integration of machine learning \'96 tailored for an organization of your choice. At the end of the course, you\'92ll walk away with a concrete plan for immediate and practical business action. WHAT THE PROGRAM COVERS This program views the technical elements of machine learning through the lens of business and management, and thus equips you with the relevant knowledge to discover opportunities to drive innovation and effciency in your organization. Although you can expect to explore the technical aspects of machine learning, the focus is on empowering you, as a business leader, to ask the right questions about whether machine learning applications will beneft a particular business problem, or make your organization more effcient. Through a mix of research insights reinforced by case examples, you\'92ll have the opportunity to critically apply your learning. You\'92ll learn to identify the realistic opportunities of this transformative technology as you develop an implementation plan for machine learning in a business of your choice. Whether you work in a strategic, operational, or managerial function, you\'92ll be equipped with an understanding of how machine learning can impact your organization\'92s business objectives, as well as knowledge of the key aspects of six weeks, you\'92ll learn how to successfully lead teams tasked with executing technical machine learning projects, and strategically leverage machine learning for a powerful competitive edge in business. $3,200 6 weeks, excluding orientation 6\'968 hours/week of self-paced learning, entirely online* *The recommended weekly time commitment for core content is 3-5 hours, taking into account the busy lifestyles of working professionals, with an additional 2-3 hours recommended for non-compulsory weekly extension related implementation strategies. Over the course of activities, should you have the time. MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE THIS PROGRAM IS FOR YOU IF: You want to gain a sound You\'92re interested in You want proof of your understanding of the successfully integrating knowledge in the form of current and future machine learning technology a certificate of completion capabilities of machine into an organization, with a from the MIT Sloan School learning, and how to leverage strategic action plan. of Management.** it in a business context. ** Certifcates of completion are issued in your legal name upon successfully completing a program according to the program completion criteria outlined during the program. No certifcate will be issued to you if you do not meet the stipulated requirements for the award of a certifcate. WHO SHOULD TAKE THIS COURSE? This online program is for business leaders, mid to senior managers, data specialists, consultants, and business professionals interested in exploring the strategic implications of integrating machine learning into an organization. Whether you\'92re interested in upskilling or are seeking an understanding of disruptive technologies in the business environment, this program will assist you in identifying business areas that could benefit from the strategic application of machine learning. If you\'92ve been tasked with managing a team or project with roots in machine learning, or you\'92re interested in using knowledge of technical innovation to fnd a competitive edge in the market, the skills you\'92ll develop will help you realize your potential. At MIT Sloan Executive Education, we are focused on bridging the energy, engagement, and idea fow of physical in-person teaching and learning into online experiences. We aim to positively modify individual and collective behaviors that participants will take back to their teams and propagate throughout their organizations. \'96 PAUL MCDONAGH-SMITH, DIGITAL CAPABILITY LEADER, MIT SLOAN EXECUTIVE EDUCATION WHAT YOU WILL LEARN This online program integrates rich, interactive media such as videos, infographics, and e-learning activities as well as traditional didactic components such as written study guides (course notes). There are also opportunities for collaborative learning through discussion forums. The following modules contribute to the holistic approach your learning path takes: ORIENTATION MODULE 1 WEEK WELCOME TO YOUR ONLINE CAMPUS You\'92ll be welcomed with a personal call and get introduced to your online teaching and technical support network. Begin connecting with fellow participants while exploring the navigation and tools of your Online Campus. Be alerted to key milestones in the learning path, and review how your results will be calculated and distributed. You\'92ll be required to complete your participant profle, confrm your certifcate delivery address, and submit a digital copy of your passport/identity document. MODULE 1 INTRODUCTION TO MACHINE LEARNING Learn about machine learning and its growing role in business. MODULE 2 IMPLEMENTING MACHINE LEARNING IN A BUSINESS Learn about where machine learning is useful, the role of data, and the importance of an implementation plan. Find out more about THE MIT SLOAN SCHOOL OF MANAGEMENT WHAT IS MIT SLOAN? MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE MODULE MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE MODULE 3 SENSING THE PHYSICAL WORLD Explore the business implementation considerations for machine learning using sensor data. MODULE 4 HELPING MACHINES TO LEARN TO USE LANGUAGE Investigate the business requirements for the implementation of machine learning using language data. MODULE 5 FINDING PATTERNS IN HUMAN TRANSACTIONS Evaluate the requirements for the implementation of machine learning using transaction data in business. MODULE 6 MACHINE LEARNING CHALLENGES AND FUTURE Develop an implementation plan for machine learning, and consider the future of machine learning in business. We should think about AI, machine learning, and robots as tools. These technologies are more intelligent than the screwdrivers and the hammers we have today, but ultimately they remain tools for us to be in control of. Machines can do some things better than we can, and we can do things better than machines, so by combining our respective Find out more about THE MIT COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE LABORATORY WHAT IS MIT CSAIL? skills we can do so much more. \'96 DANIELA RUS, Director of the MIT Computer Science and Artifcial Intelligence Laboratory (CSAIL) WHO YOU\'92LL LEARN FROM MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE YOUR FACULTY DIRECTORS The design of this MIT online program is guided by faculty who will share their experience and in-depth subject knowledge with you throughout the course. THOMAS MALONE Patrick J McGovern (1959) Professor of Management, and Founding Director of the MIT Center for Collective Intelligence Thomas W. Malone is a Professor of Information Technology and of Organizational Studies at the MIT Sloan School of Management, and his research focuses on how new organizations can be designed to take advantage of the possibilities provided by information technology. He has published his groundbreaking research in the book The Future of Work, in over 100 articles, research papers, and book chapters. His newest book, Superminds, appeared in May 2018. He holds 11 patents, co-founded three software companies, and is quoted in numerous publications such as Fortune, The New York Times, and Wired. Malone holds a BA from Rice University, two master\'92s degrees and a Ph.D. from Stanford University. He also has degrees in applied mathematics, engineering-economic systems, and psychology. DANIELA RUS Director of the MIT Computer Science and Artifcial Intelligence Laboratory (CSAIL) Daniela Rus is the Andrew (1956) and Erna Viterbi Professor of Electrical Engineering and Computer Science and Director of the Computer Science and Artifcial Intelligence Laboratory (CSAIL) at MIT. She serves as the Director of the Toyota-CSAIL Joint Research Center and is a member of the science advisory board of the Toyota Research Institute. Rus\'92 research interests are in robotics, mobile computing, and data science. Rus is a Class of 2002 MacArthur Fellow, a fellow of ACM, AAAI and IEEE, and a member of the National Academy of Engineering and the American Academy of Arts and Sciences. She is the recipient of the 2017 Engelberger Robotics Award from the Robotics Industries Association. She earned her PhD in Computer Science from Cornell University. MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE MIT FACULTY AND INDUSTRY EXPERTS ERIK BRYNJOLFSSON Professor, MIT Sloan Director, MIT Initiative on the Digital Economy CATHERINE TUCKER Sloan Distinguished Professor of Management, MIT Sloan ANTONIO TORRALBA Professor, MIT CSAIL JIM GLASS Senior Research Scientist, MIT CSAIL ANDREW LO Director of the Laboratory for Financial Engineering, MIT Sloan JOSHUA TENENBAUM Paul E. Newton Career Development Professor of Cognitive Science and Computation, MIT CSAIL ALEX \'91SANDY\'92 PENTLAND Founding Faculty Director of MIT Connection Science JEANNE W. ROSS Principal Research Scientist, MIT Sloan STEFANIE JEGELKA Assistant Professor, MIT CSAIL YOUR SUCCESS TEAM Receive a personalized approach to online education that ensures you\'92re supported by GetSmarter throughout your learning journey. HEAD LEARNING FACILITATOR A subject expert from GetSmarter and approved by the University, who\'92ll guide you through contentrelated challenges. SUCCESS MANAGER Your one-on-one support at GetSmarter, available during University hours (9am - 5pm EST) to resolve technical and administrative challenges. GLOBAL SUCCESS TEAM This team from GetSmarter is available 24/7 to solve your tech-related queries and concerns. MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE A POWERFUL COLLABORATION MIT Sloan Executive Education is collaborating with online education provider GetSmarter to create a new class of learning experience \'96 one that is higher-touch, intimate, and personalized for the working professional. WHAT IS MIT SLOAN? The MIT Sloan School of Management is one of the world\'92s leading business schools,*** emphasizing innovation in practice and research, with a mission to develop principled, innovative leaders who improve the world, and to generate ideas that advance management practice. The School\'92s focus on action learning means that students are able to apply concepts learned in the classroom to real-world business settings and, through its collaborative spirit, MIT Sloan welcomes and celebrates diverse viewpoints, creating an environment where new ideas grow and thrive. WHAT IS MIT SLOAN EXECUTIVE EDUCATION? MIT Sloan Executive Education offers non-degree executive programs led by MIT Sloan faculty to provide business professionals from around the world with a targeted and fexible means to advance their career development goals and position their organizations for future growth. By collaborating with GetSmarter, a leader in online education, MIT Sloan Executive Education is able to broaden access to its on-campus offerings in a collaborative and engaging format that stays true to the quality of MIT Sloan and MIT as a whole. WHAT IS MIT CSAIL? The MIT Computer Science and Artifcial Intelligence Laboratory (CSAIL) is the largest research laboratory at MIT and one of the world\'92s most important centers of information technology research****, with an AI Lab founded in 1959. MIT CSAIL believes that computation is the key to creating a successful future. Members focus on the future of computing, on making computers more capable, and developing the science and capabilities of computing through advances in all aspects of computer science including the theory of computation, systems research, and artifcial intelligence. ****CSAIL (2018). MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE WHAT IS GETSMARTER? GetSmarter, a brand of 2U, Inc., is a digital education company that partners with the world\'92s leading universities to select, design and deliver premium online short courses with a data-driven focus on learning gain. Technology meets academic rigor in our people-mediated model which enables lifelong learners across the globe to obtain industry-relevant skills that are certifed by the world\'92s most reputable academic institutions. MIT SLOAN CERTIFICATE OF COMPLETION This program offers you the opportunity to earn a certifcate of completion from one of the world\'92s leading business schools \'96 the MIT Sloan School of Management.*** Assessment is continuous and based on a series of practical assignments completed online. Your certifcate will be issued in your legal name and couriered to you, at no additional cost, upon successful completion of the program, as per the stipulated requirements. This program also counts towards an MIT Sloan Executive Certifcate. *** Bloomberg (Nov, 2018) MIT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE HOW YOU\'92LL LEARN Every course is broken down into manageable, weekly modules, designed to accelerate your learning process through diverse learning activities: \'95 Work through your downloadable and online instructional material \'95 Interact with your peers and learning facilitators through weekly class-wide forums and reviewed small group discussions \'95 Enjoy a wide range of interactive content, including video lectures, infographics, live polls, and more \'95 Investigate rich, real-world case studies \'95 Apply what you learn each week to quizzes and ongoing project submissions, culminating in the creation of an implementation plan for introducing machine learning in an organization of your choice. Each module is released weekly, allowing a fexible but structured approach to learning. You\'92ll be supported as you engage in individual activities and group discussions, ensuring you feel confdent to submit your best work at each weekly deadline. TECHNICAL REQUIREMENTS BASIC REQUIREMENTS In order to complete a course, you\'92ll need a current email account and access to a computer and the internet. You should be familiar with using a computer and accessing the internet, as you may need to read documents in Adobe PDF Reader, view Microsoft PowerPoint presentations, and read and create documents in Microsoft Word. Installing Adobe Flash Player will give you full access to certain course content, such as interactive infographics. However, you\'92ll still have access to this content in the form of a downloadable PDF transcript if you\'92d prefer not to use Flash. BROWSER REQUIREMENTS We recommend that you use Google Chrome as your internet browser when accessing the Online Campus. Although this is not a requirement, we have found that this browser performs best for ease of access to course material. This browser can be downloaded here. ADDITIONAL REQUIREMENTS Certain courses may require additional software and resources. These additional software and resource requirements will be communicated to you upon registration and/or at the beginning of the course. Please note that Google, Vimeo, and YouTube may be used in our course delivery, and if these services are blocked in your jurisdiction, you may have diffculty in accessing course content. Please check with a Course Consultant before registering for this course if you have any concerns about this affecting your experience with the Online Campus. MMIT SLOAN SCHOOL OF MANAGEMENT MACHINE LEARNING IN BUSINESS ONLINE SHORT COURSE Incorporate machine learning into your business strategy, and explore the impact and value of this transformative technology. REGISTER NOW CONTACT US +1 617 997 4979 | mitsloan@getsmarter.com \
\
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/326200736 Artificial Intelligence and Machine Learning for Business: A No-Nonsense Guide to Data Driven Technologies (Third Edit.... Book \'b7 July 2018 CITATIONS 42 READS 18,307 1 author: Some of the authors of this publication are also working on these related projects: Artificial Intelligence View project Steven Finlay Lancaster University 68 PUBLICATIONS\'a0\'a0\'a0764 CITATIONS\'a0\'a0\'a0 SEE PROFILE All content following this page was uploaded by Steven Finlay on 27 August 2018. The user has requested enhancement of the downloaded file. Artificial Intelligence and Machine Learning for Business A No-Nonsense Guide to Data Driven Technologies Third Edition SAMPLE Steven Finlay ii Relativistic iii Relativistic e-mail: AI@relativistic.co.uk Copyright \'a9 2018 Steven Finlay. All rights reserved. First published in Great Britain by Relativistic. Unless permitted under UK copyright law, this work may only be reproduced, in any written, printed or electronic form, with prior permission of the publisher. ISBN-13: 978-1-999-73034-5 ISBN-10: 1-999-73034-8 No plants or animals were mistreated in the writing of this book. Cover image: \'93Fire in the blood.\'94 Thanks to Pixabay. iv To Sam and Ruby v Contents Acknowledgements vii Foreword ix 1. Introduction 1 2. What are Machine Learning and Artificial Intelligence (AI)? 6 3. What Do the Scores Generated by a Predictive Model Represent? 19 4. Why Use Machine Learning? What Value Does it Add? 26 5. How Does Machine Learning Work? 31 6. Using a Predictive Model to Make Decisions 42 7. That\'92s Scorecards, but What About Decision Trees? 47 8. Neural Networks and Deep Learning 53 9. Unsupervised and Reinforcement Learning 64 10. How to Build a Predictive Model 77 11. Operationalizing Machine Learning 93 12. The Relationship Between Big Data and Machine Learning 105 13. Ethics, Law and the GDPR 111 14. The Cutting Edge of Machine Learning 125 15. When Can I Buy a Self-Driving Car? 134 16. Concluding Remarks 142 Appendix A. Evaluating Predictive Models 143 Appendix B. Further Information and Recommended Reading 151 Appendix C. Popular Terms in Machine Learning and AI 157 Appendix D. A Checklist for Business Success 169 About the Author 173 Notes 174 vi vii Acknowledgements I would like to thank my family, and my wife Samantha in particular, for their support in writing this book. I would also like to extend my gratitude to all those readers of the previous edition who contacted me with suggestions, corrections, thoughts and ideas. I have incorporated the majority of these into this edition. I am particularly grateful to the reader who told me where I could buy an orange washing machine. viii ix Foreword 56 minutes one way. 112 minutes for the round trip. That\'92s the time it takes my commuter train to travel from my home town of Preston to the great British city of Manchester and back. I find this is an ideal time to catch up on a bit of reading. Therefore, I thought that a concise book about artificial intelligence and machine learning, which could be read in about this time, would be useful to people with not much free time on their hands. An understanding of machine learning is important because it\'92s having a huge impact across many aspects of our lives. In particular, it\'92s driving the explosion in \'93Artificial intelligence\'94 applications in many areas such as language translation, autonomous robots and medical diagnosis. Artificial intelligence and machine learning are also having a direct impact on many everyday business functions. Automated systems, based on machine learning, are replacing numerous tasks that were once undertaken by people. This is giving organizations which embrace these technologies a competitive advantage over their rivals because of the efficiency savings and improved customer service that such systems can deliver. This new and updated edition of the book is considerably longer than the previous one. In particular, there are several new chapters covering a broader set of topics than before. However, I have striven to retain the \'93concise no-nonsense\'94 style of the original. Not least, because this was a key feature that readers said they liked about it. Therefore, the book may now take a little longer than 112 minutes to read (maybe two round trips to Manchester rather than one), but I hope you find the time well spent. x 1 1. Introduction Do you have a smartphone or a credit card? Do you buy stuff from supermarkets or play computer games? Are you employed or use health care? If the answer to any of these questions is yes, then artificial intelligence and machine learning will be having an impact on your life in one way or another. This is because they are the primary tools organizations use to leverage the data they hold about you in order to decide how they are going to deal with you. They are used to inform organizations about how you are likely to behave under different circumstances, and hence the way that they should treat you in order to maximize their (and sometimes your) objectives. These technologies are now being used in almost every walk of life to improve processes and enhance peoples\'92 everyday experiences via \'93artificially intelligent\'94 machines and computer interfaces. Amazon\'92s Echo, Apple\'92s Siri and Google Translate are just three well known software products that demonstrate the benefits that these technologies can deliver. These days, many products and services are also adaptive. They tailor their responses to the behavior of individual users. TV and music streaming services learn to identify the content you like, and present you with recommendations that you\'92ll no doubt be interested in. Change the type of music you listen to and their recommendations will change too. Likewise, you can buy heating systems which learn to anticipate when it\'92s the best time to turn the heating on so that you don\'92t have to bother, while at the same time optimizing energy usage to reduce your bills. These are further examples of artificial intelligence in action. This concise text provides a managerial (i.e. non-technical and no Artificial Intelligence and Machine Learning for Business 2 complex formulas) overview of artificial intelligence and machine learning, what they are and how they are used. No prior knowledge is assumed. To put it another way, if you can read and write and do basic arithmetic (there is a bit of arithmetic, but not that much) then you should be OK with the material in this book. A good question to ask at this point is: why do I need to know about these things? One reason is personal. Intelligent decisionmaking systems, based on machine learning, are widely used by organizations to decide how to treat you, your friends and your family. They use these tools to decide if you will receive a great offer or a poor one, if you should be placed at the front or the back of the queue, if you will be subject to a tax audit or treated as a suspect in a criminal case. Therefore, it\'92s not a bad idea to know something about these things so that you can understand why an organization may have treated you in one way and not in another. The other reason to learn about artificial intelligence, and the one that is the main focus of this book, is that it is now a mainstream business tool. Not that long ago, artificial intelligence was the domain of a few nerdy specialists working mainly in academia, financial services or large marketing departments. These days, regardless of what business you are in, applications of artificial intelligence, based on machine learning, can be found across the full range of business activities. This covers everything from employee vetting, answering customer queries and target marketing through to robots on the production line, warehouse management and customer deliveries. As a consequence, artificial intelligence is supporting or supplanting human expertise in many domains. Some examples include: replacing underwriters when setting insurance premiums, helping HR professionals decide who to hire/fire, automatically identifying customers as they walk into your store and supporting doctors when diagnosing illnesses such as cancer and heart disease. Artificial intelligence has arrived big time. It\'92s no fad and it\'92s here to stay. Those organizations which can use it to solve business problems, improve efficiency and cut costs will benefit at the expense of their rivals. This doesn\'92t mean that you need to learn all the things that a Introduction 3 technical specialist (a data scientist) needs to know. However, having a working knowledge of what artificial intelligence and machine learning are, and knowing how they can be used to help your organization deliver better products and services, will be beneficial. Not least, because in order to make effective use of these tools they need to be focused on business objectives to address specific problems that organizations face. If on the other hand you happen to be an equation quoting, formula juggling, bad ass mathematical genius who thinks they know all there is to know about artificial intelligence, then this book may also have some value for you too. Possibly even more than those who know nothing at all. Why? Because if all you care about are the theoretical aspects of the subject then you face a real risk of hitting a brick wall when it comes to delivering useful solutions in the minefield that is the real world; a world populated with social, ethical and political issues. This, together with a growing raft of privacy and data protection legislation, could derail your solutions no matter how good they are mathematically. Without consideration of these \'93soft issues\'94 the best case is that the solutions you develop don\'92t get to be deployed. The worst-case scenario is that you design an artificial intelligence based system that lands you in court because it unfairly discriminates against minorities, women or some other group of people. Maybe you can skip a few of the earlier chapters, but you should certainly read the later ones. To get the most out of artificial intelligence, data scientists need to engage with business users to understand their problems. Data scientists also need to understand an organization\'92s culture and its approach to the adoption of new ideas, technologies and working practices. Legal and regulatory issues in the region(s) in which their clients operate also require due consideration. It doesn\'92t matter how good a solution is in terms of cutting edge hardware and software, if it\'92s not aligned with an organization\'92s business objectives and operational processes, then it\'92s all just a waste of time and money. Lots of solution suppliers can bamboozle you with their fancy tech and the latest terminology, which is often just a rebranding of last year\'92s tech with a new twist. However, the suppliers who add value will be those who spend time understanding how you and your Artificial Intelligence and Machine Learning for Business 4 organization work. They will then determine if and how their solutions can be used to improve what you do and explain this to you in simple language which you can understand without needing to reach for Wikipedia. Successful artificial intelligence is a two-way thing. Data scientists need to know something about your organization and what it does, and you need to understand a little bit about artificial intelligence and machine learning. Without this joint understanding it\'92s unlikely that you or your organization will be able to realize the full benefits that artificial intelligence has to offer. OK. So, what will you learn from reading this book? The key topics that we are going to cover in the following chapters are: \'95 What machine learning and artificial intelligence are. \'95 The sort of things organizations use artificial intelligence for. \'95 What a predictive model looks like. \'95 The relationship between artificial intelligence, machine learning and \'93Big Data.\'94 \'95 The people and tools needed to apply artificial intelligence. \'95 How to use artificial intelligence to improve business processes and the bottom line. \'95 The legal and ethical issues that need to be considered when developing artificial intelligence based solutions that are going to be used to make decisions about people. \'95 How advanced forms of machine learning are applied to drive artificial intelligence applications such as object recognition and language translation. \'95 The current limitations of machine learning and artificial intelligence. Introduction 5 Recommended further reading and a glossary of common machine learning/artificial intelligence terms are provided in Appendices B and C respectively. 6 2. What are Machine Learning and Artificial Intelligence (AI)? Machine learning is the use of mathematical procedures (algorithms) to analyze data. The aim is to discover useful patterns (relationships or correlations) between different items of data. Once the relationships have been identified, these can be used to make inferences about the behavior of new cases when they present themselves. In essence, this is analogous to the way people learn. We observe what goes on around us and draw conclusions from our experiences about how the world works. We then apply what we have learnt to help us deal with new situations that we find ourselves in. The more we experience and learn, the better our ability to make decisions becomes. One application of machine learning is object recognition. The goal is to develop systems that can identify everyday objects from images the system is presented with. The data used to develop an object recognition system consists of pictures of different objects such as chairs, umbrellas, washing machines and so on. Each picture presented to the machine learning algorithm is labeled to identify which type of object it contains. For each type of object there may be hundreds or thousands of different images, representing alternative forms of that object from different perspectives (you\'92d be surprised at just how many variants of an umbrella there are!) By analyzing the different images, machine learning algorithms recognize that certain objects are associated with certain features (patterns). Chairs tend to have protuberances (legs) coming from a flat, often squarish base (the seat). They are also differentiated from What are Machine Learning and Artificial Intelligence (AI)? 7 stools by having a back rest. Washing machines tend to be cube shaped with knobs on and are almost never pink or orange (please, do let me know if you ever come across somewhere where I can buy a pink washing machine!) Similarly, umbrellas are long and thin (when closed), are often, but not always, black and so on. One of the most common, and arguably the first, application of machine learning is prediction. It\'92s about using machine learning to determine something that you don\'92t currently know, based on the information that you currently have available. The patterns that one finds relate to the relationships between behaviors and outcomes. Very often this relates to people\'92s past behavior and what they subsequently went on to do. Having identified the relationships that exist, it is then possible to make predictions about someone\'92s future behavior based on their current state of being. If you give me a sample of peoples\'92 previous purchasing history, I can utilize machine learning to identify patterns in their purchase behavior. I can then use these patterns to predict what goods someone is likely to buy next; i.e. future purchases are the outcome that I want to predict. This allows me to target them with tailored promotional offers for those specific products. Using machine learning for prediction is sometimes referred to as predictive modelling or Predictive Analytics (PA). In fact, predictive analytics is such a common application of machine learning that many people (rightly or wrongly) often use the two terms interchangeably. Predicting the future behavior of individuals is what people usually associate with machine learning, but there are other situations and problems to which machine learning can be applied. All you need is some unknown event or thing that you want to determine (predict), and this could be in the past, present or future. Doctors examine their patients, carry out tests and question them about their symptoms in order to gather evidence (data). They then use this data to come to a view as to what they think is wrong with the patient. They are not making a prediction about the patient\'92s future health but trying to work out what\'92s wrong with them today. Doctors can do this with a high degree of accuracy because they cross reference the patient information that they have obtained against what they Artificial Intelligence and Machine Learning for Business 8 have learnt from years of training and practice. In other words, they are looking for how the patient\'92s symptoms correlate with their knowledge of known illnesses. Machine learning can be applied in the same way. Given a host of detailed information about the symptoms of different illnesses, machine learning can be used to estimate the probability that someone has a certain condition, based on the symptoms that they present. Another way to think about machine learning/predictive analytics is as a method of reducing uncertainty. There are a whole host of possible outcomes that could occur in any given situation. Machine learning won\'92t tell you with absolute certainty which outcome will occur, but it can provide some insight into the likelihood, or odds, of each outcome. You may know that when someone goes grocery shopping they often buy bread, wine and chicken, but with machine learning you can determine that there is say, an 80% chance that the next product they buy is bread, a 15% chance that they buy wine and a 5% chance that they buy chicken. Therefore, if you want to encourage them to make their next purchase in your store, you are far more likely to win their custom with a bread offer rather than a wine or chicken offer. A predictive model (or just model going forward) is the output generated by the machine learning process. The model captures the relationships (patterns) that have been uncovered by the analytics process. Once a model has been created, it can be used to generate new predictions. Organizations then use the model\'92s predictions to decide what to do or how to treat people. So, machine learning is a process and a predictive model is the end product of that process. There are lots of different types of predictive model, and there are dozens, if not hundreds, of machine learning techniques and algorithms that can be used to generate a model. However, regardless of the type of model or the mathematics used to create it, a model\'92s predictions are almost always represented by a number - a score. The higher the score the more likely someone is to behave in the way the model predicts, the lower the score the less likely they are to behave in that way. Machine learning can be applied in all sorts of situations and to many types of problem. However, the most common business What are Machine Learning and Artificial Intelligence (AI)? 9 applications of machine learning, and the ones that are the main focus of this book, relate to what people are going to do or how they will behave in the future, based on what you know about them today1 . One very well-known application of machine learning is credit scoring. When someone wants a loan, credit card or mortgage the lender asks the individual questions about themselves and their lifestyle. They then combine this with information from a credit report containing details about the individual\'92s previous borrowing history, provided by a credit reference agency such as Experian or Equifax2 . The information is then fed into a predictive model to generate a credit score. If you live in the USA you will probably be familiar with FICO and/or Vantage scores. A high score (>750) is a prediction that someone is very likely to repay any money they borrow; i.e. that they are creditworthy. A low score (<500) indicates that someone is very uncreditworthy. Banks and finance companies the world over use similar credit scoring methods. Another common application of machine learning is target marketing. Given information about someone\'92s age, gender, income, web-browsing, purchase history, location and so on, a marketing department can predict if the person is interested in a particular product or not. They then use that prediction to decide whether or not to target them with promotional offers. Likewise, predictive models can also be used to infer how much people are willing to pay for products like insurance. This information is then used to tailor a personalized pricing strategy to each person\'92s individual circumstances. A further example of machine learning in action is preventative health care. Traditional health care systems are reactive. People seek medical assistance when they feel ill. Doctors then do their best to treat the illnesses they are presented with \'96 treatments that can be very costly and time consuming. These days, advanced health care systems are increasingly focusing their attention on prevention rather than cure. This vastly reduces costs and improves patient outcomes. Machine learning is used to assess people\'92s medical records and predict the likelihood of them developing specific Artificial Intelligence and Machine Learning for Business 10 conditions such as heart disease or diabetes, often years in advance. Individuals who come at the top of the pile; i.e. those that the model predicts are most likely to get the disease, are contacted with a view of initiating preventative action. For example, making lifestyle changes or taking preventative medication. A final example of machine learning in action is determining what type of news (and other) articles to recommend to people. Social media providers use machine learning to analyze what articles you\'92ve read in the past and the type of topics you discuss with friends. This then drives the content that they promote to you. That\'92s just a few ways in which machine learning is being used. Today, machine learning supports a huge range of applications. In fact, almost any aspect of life that involves decision-making in one form or another. The algorithms that match people on dating sites, the technology used to detect credit card fraud and systems for identifying terrorist suspects all utilize predictive models derived using machine learning. If you want a more comprehensive list of applications then see the book by Eric Siegel3 , which details more than 120 different applications of predictive models in use today \'96 and that\'92s not a comprehensive list! That brings us on to the question as to what one means by Artificial Intelligence or AI. There are many and varied definitions of what AI is, and as with predictive analytics, many people use the terms AI and machine learning interchangeably. In terms of the overall scope of AI research, machine learning is a key field of study, but there are many others. True artificial intelligence is about much much more than just pattern recognition and prediction. Some experts also question if true AI can ever be achieved by just following a \'93Brute force\'94 approach of developing ever more complex algorithms using ever more powerful computer hardware. Is there some additional (as yet unknown) element required for human-like intelligence and self-awareness which can\'92t be replicated via computation alone4? So, in one sense it\'92s incorrect to say that machine learning and artificial intelligence are the same thing. However, in practice almost every AI system in use today relies heavily on machine learning. Therefore, for the purposes of this book a simple working definition What are Machine Learning and Artificial Intelligence (AI)? 11 of AI that we shall adhere to is: \'95 Artificial Intelligence (AI) is the replication of human analytical and/or decision-making capabilities. A good AI application is one that can perform as well or better than the average person when faced with everyday tasks. For example, the ability to identify people from their Facebook photos, being able to assess someone\'92s creditworthiness more accurately than an experienced underwriter, the ability to beat the best Go and chess players or being better able to spot the signs of cancer on a medical scan than an expert radiologist. At one level, AI applications can seem almost magical to the layperson. However, like most things, once you get under the bonnet the mystique evaporates. In practice, all of the news stories one hears about the amazing applications of \'93AI\'94 are really just very sophisticated applications of machine leaning. A key mistake to avoid is thinking that current AI applications are in any way intelligent in a human conscious way. Sure, they are very complex, exceedingly clever and can be creepily lifelike at times, but it\'92s all just math at the end of the day. Most experts agree, we are years away from being able to create a machine with a human-like sense of self, or which could pass itself off as human day in, day out. That\'92s not to say there aren\'92t some very good chatbots out there! All of the AI applications in use today are what the industry refers to as Narrow AI. They are very good at behaving intelligently when applied to one well defined area of expertise. However, these systems are miles away from General AI. General AI is a system that can learn and act intelligently across a wide range of environments and problems in a similar way to a person. An AI application that is used to detect tax avoidance for example, is useless at detecting the signs of cancer from medical scans. However, a person could learn to do both these tasks if they were given suitable training. In a similar vein, a system such as Google Translate is great at understanding the spoken word but wouldn\'92t be much use when it comes to assessing if someone on a dating site might be compatible Artificial Intelligence and Machine Learning for Business 12 with you. The core components that drive most AI/machine learning applications are: \'95 Data input. This can be sensory inputs from cameras (eyes), microphones (ears) or other sources. It also includes preprocessed data such as the information captured when someone fills in a form online, details of what someone has bought using their credit card or an individual\'92s credit history provided by a credit reference agency. \'95 Data (pre)processing. The raw data input needs to be processed into a standard \'93computer friendly\'94 format before it is ready to be used. \'95 Predictive models. These are generated by the machine learning process using past experiences; i.e. large amounts of historic data. Pre-processed data for new cases is fed into the models in order to generate fresh predictions going forward. \'95 Decision rules (rule sets). A prediction on its own is useless. You have to decide how to use it. Decision rules are used in conjunction with data inputs and the scores from predictive models to decide what to do. Sometimes these rules are derived automatically by the machine learning algorithm, but often they will include additional rules defined by human experts/business users. \'95 Response/output. Action needs to be taken based upon the decision(s) that have been made. If the decision is that someone is creditworthy, then a credit card needs to be issued. If the decision is that someone should be hired, then they need to be sent an offer letter, given a contract to sign and so on. It\'92s the combination of these individual components that give us the \'93AI.\'94 What are Machine Learning and Artificial Intelligence (AI)? 13 What makes some AI applications appear so clever is the sheer complexity of the algorithms that underpin them, combined with a slick user interface to gather data and deliver the required responses in a human friendly way. Combine these components with the latest generation of industrial machinery, or integrate them into cars and other vehicles, and one has robots that can interact with their environment and engage with us in a very human like way. Let\'92s begin by considering a marketing AI application for a drinks company. The application takes information gathered about individuals from social networks and feeds it into a predictive model to determine how likely they are to buy a particular brand of whisky. The system then applies a number of rules to decide if an individual should be marketed to. The rules that might exist are: 1. If the predictive model estimates that the chance of them buying whisky is more than 90% then do nothing. They will probably buy the whisky anyway. 2. If the predictive model estimates that the chance of them buying whisky is between 1% and 90% then send them a $5 discount coupon to try and make the whisky a more attractive offering; i.e. influence the customer\'92s behavior to increase the chance of them buying. 3. If the predictive model estimates the chance of them buying whisky to be less than 1% then don\'92t do anything. They probably won\'92t buy the whisky whatever you offer them. Therefore, it\'92s not worth the effort trying to persuade them. So, these rules would be derived based on some type of cost-benefit analysis, where the 1% and 90% cut-offs are deemed to be the optimal level at which to trigger marketing activity. However, other business rules would also come into play, such as: 1. NEVER make an offer to sell whisky to children, no matter what their propensity to buy it! Artificial Intelligence and Machine Learning for Business 14 2. DO NOT send offers to people with a history of alcohol dependency. Both groups referred to by these rules will contain lots of people who would like to buy whisky; i.e. >1% chance, but from an ethical perspective targeting children or people with alcohol problems is difficult to argue for. From a purely profit-orientated perspective, marketing to children is likely to be illegal and targeting such individuals could result in a significant amount of negative publicity. These two rules are a great example of why human expertise is required to support automated machine learning based systems, especially where systems are being used to make risky or controversial decisions about people. One highly publicized example of the algorithms running wild is the case of YouTube and its ad placement policy. In 2017, many large organizations withdrew their advertising from YouTube. This was because YouTube were found to be placing some of their adverts alongside material from terrorists and other unsavory sources. It was YouTube\'92s machine learning algorithms which decided which ads to place where that had caused the problem. Consequently, YouTube had to undertake a major review of its ad placement process5 . As a result, many months later in 2018, they decided to revert to a manual vetting process. Every video clip had to be reviewed and approved by a real person before it was included in YouTube\'92s service which paired advertisers with popular content6 . A reasonable question to ask is why it took almost a year for YouTube to figure out a solution? One can\'92t be certain as to the reason, but a very plausible answer is that they spent a lot of time trying to solve the problem using a solely automated (machine learning/AI) approach, before realizing that they needed to maintain a human element in the assessment process. The full version of this book is available in print and electronic format at Amazon, iBooks and all good book stores. 15 Notes 1 Predicting consumer behavior is a very common application of machine learning, but there many are others. For example, the same techniques are used to predict stock prices, when complex machines are likely to break down and which organizations are likely to become bankrupt. 2 The original role of credit reference agencies (also known as credit reporting agencies or credit bureaus) was as a central repository for data about debts and loan repayments. This is still at the core of what they do, but these days credit reference agencies also hold all sorts of other personal information. Consequently, a credit report can contain a wide variety of personal data in addition to information about a person\'92s credit history. Credit reference agencies were arguably the first \'93Big Data\'94 companies, decades before the term began to be applied to the likes of Google, Amazon, Facebook, et al. 3 Siegel, E. (2016). \'91Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die.\'92 2nd Edition. Wiley. 4 See, for example, the arguments made by Roger Penrose in his books: \'91The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics\'92 and \'91Shadows Of The Mind: A Search for the Missing Science of Consciousness.\'92 5 Cellan-Jones, R. (2017). \'91Can Google police YouTube?\'92 BBC http://www.bbc.co.uk/news/technology-39338009, accessed 12/05/2018. 6 Kelion, L. (2018). \'91YouTube toughens advert payment rules\'92. BBC http://www.bbc.co.uk/news/technology-42716393, accessed 12/05/2018. View publication stats\
\
What AI can and can\'92t do (yet) for your business Artificial intelligence is a moving target. Here\'92s how to take better aim. by Michael Chui, James Manyika, and Mehdi Miremadi Artificial intelligence (AI) seems to be everywhere. We experience it at home and on our phones. Before we know it\'97if entrepreneurs and business innovators are to be believed\'97AI will be in just about every product and service we buy and use. In addition, its application to business problem solving is growing in leaps and bounds. And at the same time, concerns about AI\'92s implications are rising: we worry about the impact of AI-enabled automation on the workplace, employment, and society. A reality sometimes lost amid both the fears and the headline triumphs, such as Alexa, Siri, and AlphaGo, is that the AI technologies themselves\'97namely, machine learning and its subset, deep learning\'97have plenty of limitations that will still require considerable effort to overcome. This is an article about those limitations, aimed at helping executives better understand what may be holding back their AI efforts. Along the way, we will also highlight promising advances that are poised to address some of the limitations and create a new wave of opportunities. Our perspectives rest on a combination of work at the front lines\'97researching, analyzing, and assessing hundreds of real-world use cases\'97and our collaborations with some of the thought leaders, pioneering scientists, January 2018 2 and engineers working at the frontiers of AI. We\'92ve sought to distill this experience to help executives who often, in our experience, are exposed only to their own initiatives and not well calibrated as to where the frontier is or what the pace setters are already doing with AI. Simply put, AI\'92s challenges and limitations are creating a \'93moving target\'94 problem for leaders: It is hard to reach a leading edge that\'92s always advancing. It is also disappointing when AI efforts run into real-world barriers, which can lessen the appetite for further investment or encourage a wait-and-see attitude, while others charge ahead. As recent McKinsey Global Institute research indicates, there\'92s a yawning divide between leaders and laggards in the application of AI both across and within sectors (Exhibit 1). Executives hoping to narrow the gap must be able to address AI in an informed way. In other words, they need to understand not just where AI can boost innovation, insight, and decision making; lead to revenue growth; and capture of efficiencies\'97but also where AI can\'92t yet provide value. What\'92s more, they must appreciate the relationship and distinctions between QWeb 2017 AI limitations Exhibit 1 of 2 Future AI demand trajectory, % change in AI spending over next 3 years\'b9 Leading sectors Falling behind Current AI adoption, % of companies\'b2 1Estimated average, weighted by company size; demand trajectory based on midpoint of range selected by survey respondent. 2Adopting 1 or more AI technologies at scale or in business core; weighted by company size. Source: McKinsey Global Institute AI adoption and use survey; McKinsey Global Institute analysis Media and entertainment Leaders in the adoption of AI also intend to invest more in the near future compared with laggards. 0 1 13 12 11 10 9 8 7 6 5 4 3 2 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 High tech and communications Automotive and assembly Financial services Energy and resources Transportation and logistics Consumer and packaged goods Building materials and construction Professional services Travel and tourism Retail Education Healthcare Exhibit 1 3 technical constraints and organizational ones, such as cultural barriers; a dearth of personnel capable of building business-ready, AI-powered applications; and the \'93last mile\'94 challenge of embedding AI in products and processes. If you want to become a leader who understands some of the critical technical challenges slowing AI\'92s advance and is prepared to exploit promising developments that could overcome those limitations and potentially bend the trajectory of AI\'97read on. CHALLENGES, LIMITATIONS, AND OPPORTUNITIES A useful starting point is to understand recent advances in deep-learning techniques. Arguably the most exciting developments in AI, these advances are delivering jumps in the accuracy of classification and prediction, and are doing so without the usual \'93feature engineering\'94 associated with traditional supervised learning. Deep learning uses large-scale neural networks that can contain millions of simulated \'93neurons\'94 structured in layers. The most common networks are called convolutional neural networks (CNNs) and recurrent neural networks (RNNs). These neural networks learn through the use of training data and backpropagation algorithms. While much progress has been made, more still needs to be done.1 A critical step is to fit the AI approach to the problem and the availability of data. Since these systems are \'93trained\'94 rather than programmed, the various processes often require huge amounts of labeled data to perform complex tasks accurately. Obtaining large data sets can be difficult. In some domains, they may simply not be available, but even when available, the labeling efforts can require enormous human resources. Further, it can be difficult to discern how a mathematical model trained by deep learning arrives at a particular prediction, recommendation, or decision. A black box, even one that does what it\'92s supposed to, may have limited utility, especially where the predictions or decisions impact society and hold ramifications that can affect individual well-being. In such cases, users sometimes need to know the \'93whys\'94 behind the workings, such as why an algorithm reached its recommendations\'97from making factual findings with legal repercussions to arriving at business decisions, such as lending, that have regulatory repercussions\'97and why certain factors (and not others) were so critical in a given instance. 1 Stuart Russel et al., \'93Research priorities for robust and beneficial artificial intelligence,\'94 AI Magazine, winter 2015, AAAI.org. 4 Let\'92s explore five interconnected ways in which these limitations, and the solutions emerging to address them, are starting to play out. Limitation 1: Data labeling Most current AI models are trained through \'93supervised learning.\'94 This means that humans must label and categorize the underlying data, which can be a sizable and error-prone chore. For example, companies developing selfdriving-car technologies are hiring hundreds of people to manually annotate hours of video feeds from prototype vehicles to help train these systems. At the same time, promising new techniques are emerging, such as in-stream supervision (demonstrated by Eric Horvitz and his colleagues at Microsoft Research), in which data can be labeled in the course of natural usage.2 Unsupervised or semisupervised approaches reduce the need for large, labeled data sets. Two promising techniques are reinforcement learning and generative adversarial networks. Reinforcement learning. This unsupervised technique allows algorithms to learn tasks simply by trial and error. The methodology hearkens to a \'93carrot and stick\'94 approach: for every attempt an algorithm makes at performing a task, it receives a \'93reward\'94 (such as a higher score) if the behavior is successful or a \'93punishment\'94 if it isn\'92t. With repetition, performance improves, in many cases surpassing human capabilities\'97so long as the learning environment is representative of the real world. Reinforcement learning has famously been used in training computers to play games\'97most recently, in conjunction with deep-learning techniques. In May 2017, for example, it helped the AI system AlphaGo to defeat world champion Ke Jie in the game of Go. In another example, Microsoft has fielded decision services that draw on reinforcement learning and adapt to user preferences. The potential application of reinforcement learning cuts across many business arenas. Possibilities include an AI-driven trading portfolio that acquires or loses points for gains or losses in value, respectively; a product-recommendation engine that receives points for every recommendation-driven sale; and truck-routing software that receives a reward for on-time deliveries or reducing fuel consumption. Reinforcement learning can also help AI transcend the natural and social limitations of human labeling by developing previously unimagined solutions and strategies that even seasoned practitioners might never have 2 Eric Horvitz, \'93Machine learning, reasoning, and intelligence in daily life: Directions and challenges,\'94 Proceedings of Artificial Intelligence Techniques for Ambient Intelligence, Hyderabad, India, January 2007. 5 considered. Recently, for example, the system AlphaGo Zero, using a novel form of reinforcement learning, defeated its predecessor AlphaGo after learning to play Go from scratch. That meant starting with completely random play against itself rather than training on Go games played by and with humans.3 Generative adversarial networks (GANs). In this semisupervised learning method, two networks compete against each other to improve and refine their understanding of a concept. To recognize what birds look like, for example, one network attempts to distinguish between genuine and fake images of birds, and its opposing network attempts to trick it by producing what look very much like images of birds, but aren\'92t. As the two networks square off, each model\'92s representation of a bird becomes more accurate. The ability of GANs to generate increasingly believable examples of data can significantly reduce the need for data sets labeled by humans. Training an algorithm to identify different types of tumors from medical images, for example, would typically require millions of human-labeled images with the type or stage of a given tumor. By using a GAN trained to generate increasingly realistic images of different types of tumors, researchers could train a tumor-detection algorithm that combines a much smaller humanlabeled data set with the GAN\'92s output. While the application of GANs in precise disease diagnoses is still a way off, researchers have begun using GANs in increasingly sophisticated contexts. These include understanding and producing artwork in the style of a particular artist and using satellite imagery, along with an understanding of geographical features, to create up-to-date maps of rapidly developing areas. Limitation 2: Obtaining massive training data sets It has already been shown that simple AI techniques using linear models can, in some cases, approximate the power of experts in medicine and other fields.4 The current wave of machine learning, however, requires training data sets that are not only labeled but also sufficiently large and comprehensive. Deep-learning methods call for thousands of data records for models to become relatively good at classification tasks and, in some cases, millions for them to perform at the level of humans.5 3 Demis Hassabis et al., AlphaGo Zero: Learning from scratch, deepmind.com. 4 Robyn M. Dawes, \'93The robust beauty of improper linear models in decision making,\'94 American Psychologist, 1979, Volume 34, Number 7, pp. 571\'9682. 5 Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep Learning, Cambridge, MA: MIT Press, 2016. 6 The complication is that massive data sets can be difficult to obtain or create for many business use cases (think: limited clinical-trial data to predict treatment outcomes more accurately). And each minor variation in an assigned task could require another large data set to conduct even more training. For example, teaching an autonomous vehicle to navigate a mining site where the weather continually changes will require a data set that encompasses the different environmental conditions the vehicle might encounter. One-shot learning is a technique that could reduce the need for large data sets, allowing an AI model to learn about a subject when it\'92s given a small number of real-world demonstrations or examples (even one, in some cases). AI\'92s capabilities will move closer to those of humans, who can recognize multiple instances of a category relatively accurately after having been shown just a single sample\'97for example, of a pickup truck. In this stilldeveloping methodology, data scientists would first pre-train a model in a simulated virtual environment that presents variants of a task or, in the case of image recognition, of what an object looks like. Then, after being shown just a few real-world variations that the AI model did not see in virtual training, the model would draw on its knowledge to reach the right solution.6 This sort of one-shot learning could eventually help power a system to scan texts for copyright violations or to identify a corporate logo in a video after being shown just one labeled example. Today, such applications are only in their early stages. But their utility and efficiency may well expand the use of AI quickly, across multiple industries. Limitation 3: The explainability problem Explainability is not a new issue for AI systems.7 But it has grown along with the success and adoption of deep learning, which has given rise both to more diverse and advanced applications and to more opaqueness. Larger and more complex models make it hard to explain, in human terms, why a certain decision was reached (and even harder when it was reached in real time). This is one reason that adoption of some AI tools remains low in application areas where explainability is useful or indeed required. Furthermore, as the application of AI expands, regulatory requirements could also drive the need for more explainable AI models.8 6 Yan Duan et al., One-shot imitation learning, December 2017, arxiv.org. 7 Eric Horvitz et al., \'93The use of a heuristic problem-solving hierarchy to facilitate the explanation of hypothesisdirected reasoning,\'94 Proceedings of Medinfo, October 1986, pp. 27\'9631. 8 See, for example, the European Union\'92s proposed General Data Protection Regulation, which would introduce new requirements for the use of data. 7 Two nascent approaches that hold promise for increasing model transparency are local-interpretable-model-agnostic explanations (LIME) and attention techniques (Exhibit 2). LIME attempts to identify which parts of input data a trained model relies on most to make predictions in developing a proxy interpretable model. This technique considers certain segments of data at a time and observes the resulting changes in prediction to fine-tune the proxy model and develop a more refined interpretation (for example, by excluding eyes rather than, say, noses to test which are more important for facial recognition). Attention techniques visualize those pieces of input data that a model considers most as it makes a particular decision (such as focusing on a mouth to determine if an image depicts a human being). Another technique that has been used for some time is the application of generalized additive models (GAMs). By using single-feature models, GAMs limit interactions between features, thereby making each one more easily Exhibit 2 QWeb 2017 AI limitations Exhibit 2 of 2 Turning off all but a few interpretable components of this image reveals the probability that the model will identify \'85 Words relevant to food quality \'85 \'85 or to service is a sensitivity analysis that reveals which parts of an input matter most to the eventual output. \'85 a tree frog 54% \'85 billiard balls 7% \'85 a balloon 5% shines a spotlight on where the model is looking when it makes Attention a particular decision. 1LIME = local-interpretable-model-agnostic explanations. Source: Carlos Guestrin, Marco Tulio Ribeiro, and Sameer Singh, \'93Introduction to local interpretable model-agnostic explanations (LIME),\'94 August 12, 2016, O\'92Reilly, oreilly.com; Minlie Huang, Yequan Wang, Li Zhao, and Xiaoyan Zhu, Attention-based LSTM for aspect-level sentiment classification, Tsinghua University; Pixabay They have one of the fastest delivery times in the city. New techniques hold promise for making AI more transparent. The fajita we tried was tasteless and burned and the mole sauce was way too sweet. 8 interpretable by users.9 Employing these techniques, among others, to demystify AI decisions is expected to go a long way toward increasing the adoption of AI. Limitation 4: Generalizability of learning Unlike the way humans learn, AI models have difficulty carrying their experiences from one set of circumstances to another. In effect, whatever a model has achieved for a given use case remains applicable to that use case only. As a result, companies must repeatedly commit resources to train yet another model, even when the use cases are very similar. One promising response to this challenge is transfer learning.10 In this approach, an AI model is trained to accomplish a certain task and then quickly applies that learning to a similar but distinct activity. DeepMind researchers have also shown promising results with transfer learning in experiments in which training done in simulation is then transferred to real robotic arms.11 As transfer learning and other generalized approaches mature, they could help organizations build new applications more quickly and imbue existing applications with more diverse functionality. In creating a virtual personal assistant, for example, transfer learning could generalize user preferences in one area (such as music) to others (books). And users are not restricted to digital natives. Transfer learning can enable an oil-and-gas producer, for instance, to expand its use of AI algorithms trained to provide predictive maintenance for wells to other equipment, such as pipelines and drilling platforms. Transfer learning even has the potential to revolutionize business intelligence: consider a data-analyzing AI tool that understands how to optimize airline revenues and can then adapt its model to changes in weather or local economics. Another approach is the use of something approximating a generalized structure that can be applied in multiple problems. DeepMind\'92s AlphaZero, for example, has made use of the same structure for three different games: it has been possible to train a new model with that generalized structure 9 Yin Lou, Rich Caruana, and Johannes Gehrke, \'93Intelligible models for classification and regression,\'94 Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York: ACM, 2012, pp. 150\'9658. 10 For an earlier example application, see John Guttag, Eric Horvitz, and Jenna Wiens, \'93A study in transfer learning: Leveraging data from multiple hospitals to enhance hospital-specific predictions,\'94 Journal of the American Medical Informatics Association, 2014, Volume 21, Number 4, pp. 699\'96706. 11 Andrei A. Rusu et al., Sim-to-real robot learning from pixels with progressive nets, October 2016, arxiv.org. 9 to learn chess in a single day, and it then soundly beat a world-champion chess program.12 Finally, consider the possibilities in emerging meta-learning techniques that attempt to automate the design of machine-learning models. The Google Brain team, for example, uses AutoML to automate the design of neural networks for classifying images in large-scale data sets. These techniques now perform as well as those designed by humans.13 That\'92s a promising development, particularly as talent continues to be in short supply for many organizations. It\'92s also possible that meta-learning approaches will surpass human capabilities and yield even better results. Importantly, however, these techniques are still in their early days. Limitation 5: Bias in data and algorithms So far, we\'92ve focused on limitations that could be overcome through technical solutions already in the works, some of which we have described. Bias is a different kind of challenge. Potentially devastating social repercussions can arise when human predilections (conscious or unaware) are brought to bear in choosing which data points to use and which to disregard. Furthermore, when the process and frequency of data collection itself are uneven across groups and observed behaviors, it\'92s easy for problems to arise in how algorithms analyze that data, learn, and make predictions.14 Negative consequences can include misinformed recruiting decisions, misrepresented scientific or medical prognoses, distorted financial models and criminal-justice decisions, and misapplied (virtual) fingers on legal scales.15 In many cases, these biases go unrecognized or disregarded under the veil of \'93advanced data sciences,\'94 \'93proprietary data and algorithms,\'94 or \'93objective analysis.\'94 As we deploy machine learning and AI algorithms in new areas, there probably will be more instances in which these issues of potential bias become baked into data sets and algorithms. Such biases have a tendency to stay embedded because recognizing them, and taking steps to address them, requires a deep mastery of data-science techniques, as well as a more meta-understanding of existing social forces, including data collection. In all, 12 David Silver et al., Mastering chess and shogi by self-play with a general reinforcement learning algorithm, December 2017, arxiv.org. 13 Google Research Blog, \'93AutoML for large scale image classification and object detection,\'94 blog entry by Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le, November 2, 2017, research.googleblog.com. 14 Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan, Inherent trade-offs in the fair determination of risk scores, November 2016, arxiv.org. 15 See the work of Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner, and Terry Parris Jr. of ProPublica. 10 debiasing is proving to be among the most daunting obstacles, and certainly the most socially fraught, to date. There are now multiple research efforts under way, as well as efforts to capture best practices, that address these issues in academic, nonprofit, and private-sector research. It\'92s none too soon, because the challenge is likely to become even more critical, and more questions will arise. Consider, for example, the fact that many of these learning and statistically based predictive approaches implicitly assume that the future will be like the past. What should we do in sociocultural settings where efforts are under way to spur change\'97and where making decisions based on past behavior could inhibit progress (or, worse, build in resistance to change)? A wide variety of leaders, including business leaders, may soon be called upon to answer such questions. HITTING THE MOVING TARGET Solutions to the limitations we have described, along with the widespread commercial implementation of many of the advances described here, could be years away. But the breathtaking range of possibilities from AI adoption suggests that the greatest constraint for AI may be imagination. Here are a few suggestions for leaders striving to stay ahead of\'97or at least not fall too far behind\'97the curve: Do your homework, get calibrated, and keep up. While most executives won\'92t need to know the difference between convolutional and recurrent neural networks, you should have a general familiarity with the capabilities of today\'92s tools, a sense of where short-term advances are likely to occur, and a perspective on what\'92s further beyond the horizon. Tap your data-science and machine-learning experts for their knowledge, talk to some AI pioneers to get calibrated, and attend an AI conference or two to help you get the real facts; news outlets can be helpful, but they can also be part of the hype machine. Ongoing tracking studies by knowledgeable practitioners, such as the AI Index (a project of the Stanford-based One Hundred Year Study on Artificial Intelligence), are another helpful way to keep up.16 Adopt a sophisticated data strategy. AI algorithms need assistance to unlock the valuable insights lurking in the data your systems generate. You can help by developing a comprehensive data strategy that focuses not only on the technology required to pool data from disparate systems but also on data availability and acquisition, data labeling, and data governance. Although newer techniques promise to reduce the amount of data required for training 16 See the AI Index (aiindex.org) and the One Hundred Year Study (ai100.stanford.edu). 11 AI algorithms, data-hungry supervised learning remains the most prevalent technique today. And even techniques that aim to minimize the amount of data required still need some data. So a key part of this is fully knowing your own data points and how to leverage them. Think laterally. Transfer-learning techniques remain in their infancy, but there are ways to leverage an AI solution in more than one area. If you solve a problem such as predictive maintenance for large warehouse equipment, can you also apply the same solution to consumer products? Can an effective next-product-to-buy solution be used in more than one distribution channel? Encourage business units to share knowledge that may reveal ways to use your best AI solutions and thinking in more than one area of the company. Be a trailblazer. Keeping up with today\'92s AI technologies and use cases is not enough to remain competitive for the long haul. Engage your data-science staff or partner with outside experts to solve a high-impact use case with nascent techniques, such as the ones discussed in this article, that are poised for a breakthrough. Further, stay informed about what\'92s possible and what\'92s available. Many machine-learning tools, data sets, and trained models for standard applications (including speech, vision, and emotion detection) are being made widely available. Sometimes they come in open source and in other cases through application programming interfaces (APIs) created by pioneering researchers and companies. Keep an eye on such possibilities to boost your odds of staking out a first-mover or early-adopter advantage. The promise of AI is immense, and the technologies, tools, and processes needed to fulfill that promise haven\'92t fully arrived. If you think you can let the technology develop and then be a successful fast follower, think again. It\'92s very difficult to leapfrog from a standing start, particularly when the target is moving so rapidly and you don\'92t understand what AI tools can and can\'92t do now. With researchers and AI pioneers poised to solve some of today\'92s thorniest problems, it\'92s time to start understanding what is happening at the AI frontier so you can position your organization to learn, exploit, and maybe even advance the new possibilities. Michael Chui is a partner of the McKinsey Global Institute (MGI) and is based in McKinsey\'92s San Francisco office; James Manyika is the chairman of MGI and a senior partner in the San Francisco office; and Mehdi Miremadi is a partner in the Chicago office. The authors wish to thank Jack Clark at OpenAI, Jeffrey Dean at Google Brain, Professor Barbara Grosz at Harvard University, Demis Hassabis at DeepMind, Eric Horvitz at Microsoft Research, and Martin Wicke for their insights on the ideas in this article. They also wish to thank their McKinsey colleagues Steven Adler, Ali Akhtar, Adib Ayay, Ira Chadha, Rita Chung, Nicolaus Henke, Sankalp Malhotra, and Pieter Nel for their contributions to this article. Copyright \'a9 2018 McKinsey & Company. All rights reserved.\
\
What AI can and can\'92t do (yet) for your business Artificial intelligence is a moving target. Here\'92s how to take better aim. by Michael Chui, James Manyika, and Mehdi Miremadi Artificial intelligence (AI) seems to be everywhere. We experience it at home and on our phones. Before we know it\'97if entrepreneurs and business innovators are to be believed\'97AI will be in just about every product and service we buy and use. In addition, its application to business problem solving is growing in leaps and bounds. And at the same time, concerns about AI\'92s implications are rising: we worry about the impact of AI-enabled automation on the workplace, employment, and society. A reality sometimes lost amid both the fears and the headline triumphs, such as Alexa, Siri, and AlphaGo, is that the AI technologies themselves\'97namely, machine learning and its subset, deep learning\'97have plenty of limitations that will still require considerable effort to overcome. This is an article about those limitations, aimed at helping executives better understand what may be holding back their AI efforts. Along the way, we will also highlight promising advances that are poised to address some of the limitations and create a new wave of opportunities. Our perspectives rest on a combination of work at the front lines\'97researching, analyzing, and assessing hundreds of real-world use cases\'97and our collaborations with some of the thought leaders, pioneering scientists, January 2018 2 and engineers working at the frontiers of AI. We\'92ve sought to distill this experience to help executives who often, in our experience, are exposed only to their own initiatives and not well calibrated as to where the frontier is or what the pace setters are already doing with AI. Simply put, AI\'92s challenges and limitations are creating a \'93moving target\'94 problem for leaders: It is hard to reach a leading edge that\'92s always advancing. It is also disappointing when AI efforts run into real-world barriers, which can lessen the appetite for further investment or encourage a wait-and-see attitude, while others charge ahead. As recent McKinsey Global Institute research indicates, there\'92s a yawning divide between leaders and laggards in the application of AI both across and within sectors (Exhibit 1). Executives hoping to narrow the gap must be able to address AI in an informed way. In other words, they need to understand not just where AI can boost innovation, insight, and decision making; lead to revenue growth; and capture of efficiencies\'97but also where AI can\'92t yet provide value. What\'92s more, they must appreciate the relationship and distinctions between QWeb 2017 AI limitations Exhibit 1 of 2 Future AI demand trajectory, % change in AI spending over next 3 years\'b9 Leading sectors Falling behind Current AI adoption, % of companies\'b2 1Estimated average, weighted by company size; demand trajectory based on midpoint of range selected by survey respondent. 2Adopting 1 or more AI technologies at scale or in business core; weighted by company size. Source: McKinsey Global Institute AI adoption and use survey; McKinsey Global Institute analysis Media and entertainment Leaders in the adoption of AI also intend to invest more in the near future compared with laggards. 0 1 13 12 11 10 9 8 7 6 5 4 3 2 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 High tech and communications Automotive and assembly Financial services Energy and resources Transportation and logistics Consumer and packaged goods Building materials and construction Professional services Travel and tourism Retail Education Healthcare Exhibit 1 3 technical constraints and organizational ones, such as cultural barriers; a dearth of personnel capable of building business-ready, AI-powered applications; and the \'93last mile\'94 challenge of embedding AI in products and processes. If you want to become a leader who understands some of the critical technical challenges slowing AI\'92s advance and is prepared to exploit promising developments that could overcome those limitations and potentially bend the trajectory of AI\'97read on. CHALLENGES, LIMITATIONS, AND OPPORTUNITIES A useful starting point is to understand recent advances in deep-learning techniques. Arguably the most exciting developments in AI, these advances are delivering jumps in the accuracy of classification and prediction, and are doing so without the usual \'93feature engineering\'94 associated with traditional supervised learning. Deep learning uses large-scale neural networks that can contain millions of simulated \'93neurons\'94 structured in layers. The most common networks are called convolutional neural networks (CNNs) and recurrent neural networks (RNNs). These neural networks learn through the use of training data and backpropagation algorithms. While much progress has been made, more still needs to be done.1 A critical step is to fit the AI approach to the problem and the availability of data. Since these systems are \'93trained\'94 rather than programmed, the various processes often require huge amounts of labeled data to perform complex tasks accurately. Obtaining large data sets can be difficult. In some domains, they may simply not be available, but even when available, the labeling efforts can require enormous human resources. Further, it can be difficult to discern how a mathematical model trained by deep learning arrives at a particular prediction, recommendation, or decision. A black box, even one that does what it\'92s supposed to, may have limited utility, especially where the predictions or decisions impact society and hold ramifications that can affect individual well-being. In such cases, users sometimes need to know the \'93whys\'94 behind the workings, such as why an algorithm reached its recommendations\'97from making factual findings with legal repercussions to arriving at business decisions, such as lending, that have regulatory repercussions\'97and why certain factors (and not others) were so critical in a given instance. 1 Stuart Russel et al., \'93Research priorities for robust and beneficial artificial intelligence,\'94 AI Magazine, winter 2015, AAAI.org. 4 Let\'92s explore five interconnected ways in which these limitations, and the solutions emerging to address them, are starting to play out. Limitation 1: Data labeling Most current AI models are trained through \'93supervised learning.\'94 This means that humans must label and categorize the underlying data, which can be a sizable and error-prone chore. For example, companies developing selfdriving-car technologies are hiring hundreds of people to manually annotate hours of video feeds from prototype vehicles to help train these systems. At the same time, promising new techniques are emerging, such as in-stream supervision (demonstrated by Eric Horvitz and his colleagues at Microsoft Research), in which data can be labeled in the course of natural usage.2 Unsupervised or semisupervised approaches reduce the need for large, labeled data sets. Two promising techniques are reinforcement learning and generative adversarial networks. Reinforcement learning. This unsupervised technique allows algorithms to learn tasks simply by trial and error. The methodology hearkens to a \'93carrot and stick\'94 approach: for every attempt an algorithm makes at performing a task, it receives a \'93reward\'94 (such as a higher score) if the behavior is successful or a \'93punishment\'94 if it isn\'92t. With repetition, performance improves, in many cases surpassing human capabilities\'97so long as the learning environment is representative of the real world. Reinforcement learning has famously been used in training computers to play games\'97most recently, in conjunction with deep-learning techniques. In May 2017, for example, it helped the AI system AlphaGo to defeat world champion Ke Jie in the game of Go. In another example, Microsoft has fielded decision services that draw on reinforcement learning and adapt to user preferences. The potential application of reinforcement learning cuts across many business arenas. Possibilities include an AI-driven trading portfolio that acquires or loses points for gains or losses in value, respectively; a product-recommendation engine that receives points for every recommendation-driven sale; and truck-routing software that receives a reward for on-time deliveries or reducing fuel consumption. Reinforcement learning can also help AI transcend the natural and social limitations of human labeling by developing previously unimagined solutions and strategies that even seasoned practitioners might never have 2 Eric Horvitz, \'93Machine learning, reasoning, and intelligence in daily life: Directions and challenges,\'94 Proceedings of Artificial Intelligence Techniques for Ambient Intelligence, Hyderabad, India, January 2007. 5 considered. Recently, for example, the system AlphaGo Zero, using a novel form of reinforcement learning, defeated its predecessor AlphaGo after learning to play Go from scratch. That meant starting with completely random play against itself rather than training on Go games played by and with humans.3 Generative adversarial networks (GANs). In this semisupervised learning method, two networks compete against each other to improve and refine their understanding of a concept. To recognize what birds look like, for example, one network attempts to distinguish between genuine and fake images of birds, and its opposing network attempts to trick it by producing what look very much like images of birds, but aren\'92t. As the two networks square off, each model\'92s representation of a bird becomes more accurate. The ability of GANs to generate increasingly believable examples of data can significantly reduce the need for data sets labeled by humans. Training an algorithm to identify different types of tumors from medical images, for example, would typically require millions of human-labeled images with the type or stage of a given tumor. By using a GAN trained to generate increasingly realistic images of different types of tumors, researchers could train a tumor-detection algorithm that combines a much smaller humanlabeled data set with the GAN\'92s output. While the application of GANs in precise disease diagnoses is still a way off, researchers have begun using GANs in increasingly sophisticated contexts. These include understanding and producing artwork in the style of a particular artist and using satellite imagery, along with an understanding of geographical features, to create up-to-date maps of rapidly developing areas. Limitation 2: Obtaining massive training data sets It has already been shown that simple AI techniques using linear models can, in some cases, approximate the power of experts in medicine and other fields.4 The current wave of machine learning, however, requires training data sets that are not only labeled but also sufficiently large and comprehensive. Deep-learning methods call for thousands of data records for models to become relatively good at classification tasks and, in some cases, millions for them to perform at the level of humans.5 3 Demis Hassabis et al., AlphaGo Zero: Learning from scratch, deepmind.com. 4 Robyn M. Dawes, \'93The robust beauty of improper linear models in decision making,\'94 American Psychologist, 1979, Volume 34, Number 7, pp. 571\'9682. 5 Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep Learning, Cambridge, MA: MIT Press, 2016. 6 The complication is that massive data sets can be difficult to obtain or create for many business use cases (think: limited clinical-trial data to predict treatment outcomes more accurately). And each minor variation in an assigned task could require another large data set to conduct even more training. For example, teaching an autonomous vehicle to navigate a mining site where the weather continually changes will require a data set that encompasses the different environmental conditions the vehicle might encounter. One-shot learning is a technique that could reduce the need for large data sets, allowing an AI model to learn about a subject when it\'92s given a small number of real-world demonstrations or examples (even one, in some cases). AI\'92s capabilities will move closer to those of humans, who can recognize multiple instances of a category relatively accurately after having been shown just a single sample\'97for example, of a pickup truck. In this stilldeveloping methodology, data scientists would first pre-train a model in a simulated virtual environment that presents variants of a task or, in the case of image recognition, of what an object looks like. Then, after being shown just a few real-world variations that the AI model did not see in virtual training, the model would draw on its knowledge to reach the right solution.6 This sort of one-shot learning could eventually help power a system to scan texts for copyright violations or to identify a corporate logo in a video after being shown just one labeled example. Today, such applications are only in their early stages. But their utility and efficiency may well expand the use of AI quickly, across multiple industries. Limitation 3: The explainability problem Explainability is not a new issue for AI systems.7 But it has grown along with the success and adoption of deep learning, which has given rise both to more diverse and advanced applications and to more opaqueness. Larger and more complex models make it hard to explain, in human terms, why a certain decision was reached (and even harder when it was reached in real time). This is one reason that adoption of some AI tools remains low in application areas where explainability is useful or indeed required. Furthermore, as the application of AI expands, regulatory requirements could also drive the need for more explainable AI models.8 6 Yan Duan et al., One-shot imitation learning, December 2017, arxiv.org. 7 Eric Horvitz et al., \'93The use of a heuristic problem-solving hierarchy to facilitate the explanation of hypothesisdirected reasoning,\'94 Proceedings of Medinfo, October 1986, pp. 27\'9631. 8 See, for example, the European Union\'92s proposed General Data Protection Regulation, which would introduce new requirements for the use of data. 7 Two nascent approaches that hold promise for increasing model transparency are local-interpretable-model-agnostic explanations (LIME) and attention techniques (Exhibit 2). LIME attempts to identify which parts of input data a trained model relies on most to make predictions in developing a proxy interpretable model. This technique considers certain segments of data at a time and observes the resulting changes in prediction to fine-tune the proxy model and develop a more refined interpretation (for example, by excluding eyes rather than, say, noses to test which are more important for facial recognition). Attention techniques visualize those pieces of input data that a model considers most as it makes a particular decision (such as focusing on a mouth to determine if an image depicts a human being). Another technique that has been used for some time is the application of generalized additive models (GAMs). By using single-feature models, GAMs limit interactions between features, thereby making each one more easily Exhibit 2 QWeb 2017 AI limitations Exhibit 2 of 2 Turning off all but a few interpretable components of this image reveals the probability that the model will identify \'85 Words relevant to food quality \'85 \'85 or to service is a sensitivity analysis that reveals which parts of an input matter most to the eventual output. \'85 a tree frog 54% \'85 billiard balls 7% \'85 a balloon 5% shines a spotlight on where the model is looking when it makes Attention a particular decision. 1LIME = local-interpretable-model-agnostic explanations. Source: Carlos Guestrin, Marco Tulio Ribeiro, and Sameer Singh, \'93Introduction to local interpretable model-agnostic explanations (LIME),\'94 August 12, 2016, O\'92Reilly, oreilly.com; Minlie Huang, Yequan Wang, Li Zhao, and Xiaoyan Zhu, Attention-based LSTM for aspect-level sentiment classification, Tsinghua University; Pixabay They have one of the fastest delivery times in the city. New techniques hold promise for making AI more transparent. The fajita we tried was tasteless and burned and the mole sauce was way too sweet. 8 interpretable by users.9 Employing these techniques, among others, to demystify AI decisions is expected to go a long way toward increasing the adoption of AI. Limitation 4: Generalizability of learning Unlike the way humans learn, AI models have difficulty carrying their experiences from one set of circumstances to another. In effect, whatever a model has achieved for a given use case remains applicable to that use case only. As a result, companies must repeatedly commit resources to train yet another model, even when the use cases are very similar. One promising response to this challenge is transfer learning.10 In this approach, an AI model is trained to accomplish a certain task and then quickly applies that learning to a similar but distinct activity. DeepMind researchers have also shown promising results with transfer learning in experiments in which training done in simulation is then transferred to real robotic arms.11 As transfer learning and other generalized approaches mature, they could help organizations build new applications more quickly and imbue existing applications with more diverse functionality. In creating a virtual personal assistant, for example, transfer learning could generalize user preferences in one area (such as music) to others (books). And users are not restricted to digital natives. Transfer learning can enable an oil-and-gas producer, for instance, to expand its use of AI algorithms trained to provide predictive maintenance for wells to other equipment, such as pipelines and drilling platforms. Transfer learning even has the potential to revolutionize business intelligence: consider a data-analyzing AI tool that understands how to optimize airline revenues and can then adapt its model to changes in weather or local economics. Another approach is the use of something approximating a generalized structure that can be applied in multiple problems. DeepMind\'92s AlphaZero, for example, has made use of the same structure for three different games: it has been possible to train a new model with that generalized structure 9 Yin Lou, Rich Caruana, and Johannes Gehrke, \'93Intelligible models for classification and regression,\'94 Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York: ACM, 2012, pp. 150\'9658. 10 For an earlier example application, see John Guttag, Eric Horvitz, and Jenna Wiens, \'93A study in transfer learning: Leveraging data from multiple hospitals to enhance hospital-specific predictions,\'94 Journal of the American Medical Informatics Association, 2014, Volume 21, Number 4, pp. 699\'96706. 11 Andrei A. Rusu et al., Sim-to-real robot learning from pixels with progressive nets, October 2016, arxiv.org. 9 to learn chess in a single day, and it then soundly beat a world-champion chess program.12 Finally, consider the possibilities in emerging meta-learning techniques that attempt to automate the design of machine-learning models. The Google Brain team, for example, uses AutoML to automate the design of neural networks for classifying images in large-scale data sets. These techniques now perform as well as those designed by humans.13 That\'92s a promising development, particularly as talent continues to be in short supply for many organizations. It\'92s also possible that meta-learning approaches will surpass human capabilities and yield even better results. Importantly, however, these techniques are still in their early days. Limitation 5: Bias in data and algorithms So far, we\'92ve focused on limitations that could be overcome through technical solutions already in the works, some of which we have described. Bias is a different kind of challenge. Potentially devastating social repercussions can arise when human predilections (conscious or unaware) are brought to bear in choosing which data points to use and which to disregard. Furthermore, when the process and frequency of data collection itself are uneven across groups and observed behaviors, it\'92s easy for problems to arise in how algorithms analyze that data, learn, and make predictions.14 Negative consequences can include misinformed recruiting decisions, misrepresented scientific or medical prognoses, distorted financial models and criminal-justice decisions, and misapplied (virtual) fingers on legal scales.15 In many cases, these biases go unrecognized or disregarded under the veil of \'93advanced data sciences,\'94 \'93proprietary data and algorithms,\'94 or \'93objective analysis.\'94 As we deploy machine learning and AI algorithms in new areas, there probably will be more instances in which these issues of potential bias become baked into data sets and algorithms. Such biases have a tendency to stay embedded because recognizing them, and taking steps to address them, requires a deep mastery of data-science techniques, as well as a more meta-understanding of existing social forces, including data collection. In all, 12 David Silver et al., Mastering chess and shogi by self-play with a general reinforcement learning algorithm, December 2017, arxiv.org. 13 Google Research Blog, \'93AutoML for large scale image classification and object detection,\'94 blog entry by Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le, November 2, 2017, research.googleblog.com. 14 Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan, Inherent trade-offs in the fair determination of risk scores, November 2016, arxiv.org. 15 See the work of Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner, and Terry Parris Jr. of ProPublica. 10 debiasing is proving to be among the most daunting obstacles, and certainly the most socially fraught, to date. There are now multiple research efforts under way, as well as efforts to capture best practices, that address these issues in academic, nonprofit, and private-sector research. It\'92s none too soon, because the challenge is likely to become even more critical, and more questions will arise. Consider, for example, the fact that many of these learning and statistically based predictive approaches implicitly assume that the future will be like the past. What should we do in sociocultural settings where efforts are under way to spur change\'97and where making decisions based on past behavior could inhibit progress (or, worse, build in resistance to change)? A wide variety of leaders, including business leaders, may soon be called upon to answer such questions. HITTING THE MOVING TARGET Solutions to the limitations we have described, along with the widespread commercial implementation of many of the advances described here, could be years away. But the breathtaking range of possibilities from AI adoption suggests that the greatest constraint for AI may be imagination. Here are a few suggestions for leaders striving to stay ahead of\'97or at least not fall too far behind\'97the curve: Do your homework, get calibrated, and keep up. While most executives won\'92t need to know the difference between convolutional and recurrent neural networks, you should have a general familiarity with the capabilities of today\'92s tools, a sense of where short-term advances are likely to occur, and a perspective on what\'92s further beyond the horizon. Tap your data-science and machine-learning experts for their knowledge, talk to some AI pioneers to get calibrated, and attend an AI conference or two to help you get the real facts; news outlets can be helpful, but they can also be part of the hype machine. Ongoing tracking studies by knowledgeable practitioners, such as the AI Index (a project of the Stanford-based One Hundred Year Study on Artificial Intelligence), are another helpful way to keep up.16 Adopt a sophisticated data strategy. AI algorithms need assistance to unlock the valuable insights lurking in the data your systems generate. You can help by developing a comprehensive data strategy that focuses not only on the technology required to pool data from disparate systems but also on data availability and acquisition, data labeling, and data governance. Although newer techniques promise to reduce the amount of data required for training 16 See the AI Index (aiindex.org) and the One Hundred Year Study (ai100.stanford.edu). 11 AI algorithms, data-hungry supervised learning remains the most prevalent technique today. And even techniques that aim to minimize the amount of data required still need some data. So a key part of this is fully knowing your own data points and how to leverage them. Think laterally. Transfer-learning techniques remain in their infancy, but there are ways to leverage an AI solution in more than one area. If you solve a problem such as predictive maintenance for large warehouse equipment, can you also apply the same solution to consumer products? Can an effective next-product-to-buy solution be used in more than one distribution channel? Encourage business units to share knowledge that may reveal ways to use your best AI solutions and thinking in more than one area of the company. Be a trailblazer. Keeping up with today\'92s AI technologies and use cases is not enough to remain competitive for the long haul. Engage your data-science staff or partner with outside experts to solve a high-impact use case with nascent techniques, such as the ones discussed in this article, that are poised for a breakthrough. Further, stay informed about what\'92s possible and what\'92s available. Many machine-learning tools, data sets, and trained models for standard applications (including speech, vision, and emotion detection) are being made widely available. Sometimes they come in open source and in other cases through application programming interfaces (APIs) created by pioneering researchers and companies. Keep an eye on such possibilities to boost your odds of staking out a first-mover or early-adopter advantage. The promise of AI is immense, and the technologies, tools, and processes needed to fulfill that promise haven\'92t fully arrived. If you think you can let the technology develop and then be a successful fast follower, think again. It\'92s very difficult to leapfrog from a standing start, particularly when the target is moving so rapidly and you don\'92t understand what AI tools can and can\'92t do now. With researchers and AI pioneers poised to solve some of today\'92s thorniest problems, it\'92s time to start understanding what is happening at the AI frontier so you can position your organization to learn, exploit, and maybe even advance the new possibilities. Michael Chui is a partner of the McKinsey Global Institute (MGI) and is based in McKinsey\'92s San Francisco office; James Manyika is the chairman of MGI and a senior partner in the San Francisco office; and Mehdi Miremadi is a partner in the Chicago office. The authors wish to thank Jack Clark at OpenAI, Jeffrey Dean at Google Brain, Professor Barbara Grosz at Harvard University, Demis Hassabis at DeepMind, Eric Horvitz at Microsoft Research, and Martin Wicke for their insights on the ideas in this article. They also wish to thank their McKinsey colleagues Steven Adler, Ali Akhtar, Adib Ayay, Ira Chadha, Rita Chung, Nicolaus Henke, Sankalp Malhotra, and Pieter Nel for their contributions to this article. Copyright \'a9 2018 McKinsey & Company. All rights reserved.\
\
Machine Learning in Business Use Cases Artificial intelligence solutions that can be applied today Publication Date: 20 Apr 2015 | Product code: IT0022-000335 Michael Azoff Machine Learning in Business Use Cases Summary Catalyst The subfield in artificial intelligence (AI) known as machine learning or cognitive computing has in recent years become highly active as techniques such as deep learning and IBM\'92s Watson initiative have given rise to improved performance in their generalization capabilities. Deep learning in particular has benefited from being ported to run on Nvidia\'92s programmable graphics processing units (GPUs), reducing the neural network system training time from weeks to a day or less, and giving rise to the most sophisticated neural networks yet devised that have broken established benchmarks in image classification and speech recognition. As a result, there has been a surge of interest from startups to established businesses looking to exploit these new techniques. Large enterprises such as Google, Microsoft, and Facebook are investing in AI, and the large IT services and SI players are investing in robotic process automation (RPA) to transform the office workplace. Visible changes due to AI can be expected in society in the next five to 10 years. Ovum view The AI field is going through a new step change in capabilities, similar to Backpropagation, with deep learning systems offering improved discrimination capabilities when separating signal from noise. These systems are accelerated by running on Nvidia\'92s new-generation programmable GPUs, reducing training time from weeks to a day or less in some cases. Typical deep learning applications cover image recognition (tracking a person in a crowd, for example), as well as speech recognition and understanding, including understanding in a first-time exposure to a voice (the system has not been trained to understanding only one person\'92s speech pattern), a Holy Grail in AI. Current best accuracy is the 95% region using deep learning. According to Andrew Ng at Baidu, achieving 99% accuracy appears within reach and will transform human-machine interaction, with voice commands able to be distinguished by machines even in highly noisy environments. Today\'92s 95% accuracy is already seeing business applications available on the market. The use of robotics in manufacturing (albeit with limited intelligence) is a mature field, and RPA, which is technology that enables robots to learn applications in the business workplace and automate transaction processing and other data-related tasks, is the office equivalent that is already gaining early adoption. Robots exploiting advances in AI will also fuel growth in the burgeoning robotics industry, providing robots with sufficient intelligence to perform physical tasks. The future for robots is therefore particularly promising: robot domestic servants in the home (for cleaning, assisting elderly with heavy lifting tasks, and so on), robot workers in the office, and (more ominously if you have watched the Terminator films) robot soldiers. The new generation of robots is proving its usefulness in being able to navigate complex terrain encountered for the first time, and being able to interact with humans through speech and vision. Because human-machine interfaces have not changed much since the invention of the computer mouse, there is likely to be a step change in how humans interact with machines with the improvements that deep learning introduces. AI has been quietly proving itself for many years in fields such as business intelligence with predictive analytics, application performance analytics, credit risk profiling and scoring, and fraud detection. The new generation of solutions exploiting deep learning \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 2 Machine Learning in Business Use Cases will enable humans to talk to software applications and robots, and these machines will have an intelligent understanding of their environment through visual observation. The combination of the invention of deep learning algorithms, the acceleration in training these systems on GPUs, and the rapid dissemination of AI knowledge through an open community that shares papers, source code, and AI frameworks, has created a step change that will see AI advances permeate society in multiple ways. Key messages 
\f2 \uc0\u61607 
\f0  Deep learning-based neural networks have created a step change improvement in the accuracy of AI systems, enabling human-machine interface through speech. 
\f2 \uc0\u61607 
\f0  Business applications powered by deep learning are now available. 
\f2 \uc0\u61607 
\f0  The combination of deep learning algorithms accelerated on GPUs has been the pivotal breakthrough that has also accelerated progress in the field. 
\f2 \uc0\u61607 
\f0  There are two kinds of AI: Type A, which aims to be a true artificial brain, and Type B, which has a scope limited to performing useful specific tasks intelligently. Recommendations Recommendations for enterprises Type A AI aims to create a thinking machine on a par with human brain capabilities and is still a distant goal not likely to be achieved for many years. Type B AI has a limited but still significant aim to perform specific well-defined tasks in an intelligent way. The progress in Type B is where the excitement exists today, with AI systems available now from startups to well established companies transforming many tasks with rapid automated intelligence. Every business should consider how AI will affect their domain and should look for opportunities to enhance their field with AI systems. The areas first to be affected will relate to speech recognition, computer vision, and robotics, both physical robots and RPA systems residing digitally. So, for example, enhanced computer-machine interfaces using voice will become available. The 99% accuracy in voice recognition will complete the transformation and is likely to be achieved within the next couple of years. Assisted driving with AI systems will become standard, reducing accident rates. The examples of the startups in this report provide an indication of early breakthroughs and applications. Businesses, especially large enterprises with sufficient resources, may wish to create their own AI initiatives internally, making use of deep learning frameworks such as Caffe, Theano, Torch, Minerva and others, as well as GPUs and the cuDNN library from Nvidia. Microsoft has made available AI components on Azure for use by developers. Alternatively, opportunities exist for early adopters of the AI services and products offered by startups and other businesses. There are examples of AI systems to assist in company compliance and regulation, detect fraud, improve security, create advanced human-machine interfaces through speech and vision, assist vehicle drivers, provide assistance to the medical profession, mine big data, and many more applications. \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 3 Machine Learning in Business Use Cases Recommendations for vendors AI systems pose a threat and opportunity to most business. Because AI has penetrated all types of businesses, at a minimum it will change how we interact with the IT systems. Competitive advantage will go to businesses that can see how AI systems will impact their domain and can exploit the upcoming changes. The market for supporting businesses in working with and building AI systems will also grow. While automation in the workplace will grow and reduce affected jobs, there will be a growth in support jobs, as well as new roles emerging requiring expertise in using AI systems for business advantage. Deep learning-based neural networks have created a step change improvement in the accuracy of AI systems The impact of deep learning The AI field, and in particular the segment preoccupied with massively distributed parallel systems rather than symbolic computation, has gone through a number of cycles in its history, which started in the mid-1940s. The creation of the backpropagation algorithm in 1986, which allowed quite complex AI models such as neural networks to be trained for real-world problems, led to a new impetus in the field and hype around its capabilities, but once the hype died down the field settled into a plateau and AI techniques found their way into specialized products. One example is the credit card risk profiling system devised by FICO (formerly known as Fair Isaac Corporation), where a trained neural network would detect anomalous spend behavior on a credit card and alert administrators for further investigation. Another example is the LeNet neural network that found applications in check handwriting recognition. The Backpropagation algorithm allowed a multi-layered neural network (also known as the Perceptron) to be \'93solved\'94. In this context solved means trainable, so that the network learns and is able to accurately generalize with fresh inputs not seen during the training phase when the weights in the network are first hardened. These weights are free parameters that start with random values and are gradually fine-tuned during the training to produce a learning capability such as pattern recognition. In the last five years a new learning algorithm, deep learning, coupled with accelerated running time on Nvidia GPUs has begun to gain momentum and has rekindled a new surge of enthusiasm in neural networks. The learning and generalization capabilities of this new generation of neural networks has raised their capability a notch upward and this is where business-oriented applications are beginning to emerge. \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 4 Machine Learning in Business Use Cases Figure 1: A schematic diagram of a deep learning neural network system Source: Andrew Ng, Baidu The most successful deep learning systems learn basic features, such as edges in imaging tasks, and then achieve higher and higher abstractions with progress through the hierarchical system structure (see Figure 1). From our knowledge in neuroscience, this is similar to how the brain processes images. Deep learning face-recognition systems will find a natural recognition of a face which can be achieved without supervision by the human trainer. Figure 2 shows how such a network internalizes an ideal face for which it will trigger the highest recognition. Figure 2: Deep learning system with unsupervised networks for face recognition Source: Quoc V Lee, Stanford University and Google \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 5 Machine Learning in Business Use Cases Business applications powered by deep learning are now available Startups are emerging with deep learning-based applications The startup field in AI is blossoming, here are some names powered by GPUs and deep learning. 
\f2 \uc0\u61607 
\f0  Chase IT: Intelligent Voice for compliance and eDiscovery solutions 
\f2 \uc0\u61607 
\f0  Clarifai: visual recognition system 
\f2 \uc0\u61607 
\f0  Dato: multiclass classification 
\f2 \uc0\u61607 
\f0  Emotient: emotion detection and sentiment analysis 
\f2 \uc0\u61607 
\f0  Enlitic: medical diagnostics 
\f2 \uc0\u61607 
\f0  Ersaltz Labs: data mining 
\f2 \uc0\u61607 
\f0  EyeEm: photography 
\f2 \uc0\u61607 
\f0  GeekSys: retail store performance management 
\f2 \uc0\u61607 
\f0  HertaSecurity: facial recognition system 
\f2 \uc0\u61607 
\f0  Iflytek: speech engine 
\f2 \uc0\u61607 
\f0  InsilicoMedicine: genomics and big data analysis 
\f2 \uc0\u61607 
\f0  Jibo: family robot 
\f2 \uc0\u61607 
\f0  iQIYI: online video platform 
\f2 \uc0\u61607 
\f0  Megvii: Face++ platform for face recognition 
\f2 \uc0\u61607 
\f0  Metamind: natural language processing 
\f2 \uc0\u61607 
\f0  Nervana Systems: hardware for deep learning 
\f2 \uc0\u61607 
\f0  Orbeus: image-to-text technology to index videos 
\f2 \uc0\u61607 
\f0  Paracosm: vision for robots and augmented reality 
\f2 \uc0\u61607 
\f0  QM Scientific: shopping intelligence 
\f2 \uc0\u61607 
\f0  Replica Labs: computer vision 
\f2 \uc0\u61607 
\f0  Sensetime: computer vision 
\f2 \uc0\u61607 
\f0  Sogou: Sogou search engine 
\f2 \uc0\u61607 
\f0  Zebra Medical Vision: big data medical imaging Established companies also working with GPUs and deep learning include Adobe, Alibaba.com, Amazon, Baidu, Cycorp, Facebook, FICO, Flickr, Yahoo!, Google, IBM, Microsoft, Nuance, Scanadu, and Twitter. These players are also buying up startups and others. For example, Google recently acquired Boston Dynamics, an MIT robotics spin-off launched in 1992, and in 2014 it acquired DeepMind, which was founded in 2011. The auto industry is also investing in these technologies with driver-assisted systems. Tesla Motors has GPUs under the bonnet running advanced software, and Audi, BMW, Volkswagon, and others are also active in this area. Three examples from the startups listed above will be are examined here in more detail to indicate the type of markets being addressed. \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 6 Machine Learning in Business Use Cases Chase IT This London-based startup offers Intelligent Voice for monitoring telephone conversations and turning voice into text that is then mined for information. It means that telephone calls can be searched like any text search, and the JumpTo feature provides information on unusual conversations that appear different from the norm in the given environment. The product is in use in London\'92s investment banks where conversations are monitored for compliance reasons. Clarifai This service offers image recognition and was the ImageNet winner in 2013, an open competition to test AI systems. The company says its products now go beyond those results, and probably for commercial confidentiality reasons it no longer participates in the ImageNet competition. Based in New York, the startup offers improved speed, vocabulary size, and memory footprint, and has expanded beyond images to extract knowledge from other forms of data. It is possible to see for yourself the service in action by entering an image file or URL and letting the solution return similar images: http://www.clarifai.com/#demo. Herta Security Part of the Everis Aerospace, Defense and Security group, the startup originated in Barcelona. It offers fast and accurate video surveillance and access control solutions exploiting deep learning and GPUs. It has international projects that include safe-cities, airports, train and metro stations, prisons, banks, casinos, sports stadiums, shopping malls, military, police and forensic applications. Herta Security has partners in 25 countries. The following is an example from a mature company. IBM IBM Watson, which made headlines as an intelligent machine that won the US TV game show Jeopardy, has progressed from a single machine (a box in a room) to the cloud, where it draws on a collection of AI algorithms and is also scalable on the cloud, with IBM offering commercial cognitive services. It is suitable for a wide variety of applications, and IBM is working with partners to address an increasing number of these, including: 
\f2 \uc0\u61607 
\f0  Medical diagnosis and action: Assists medical professionals diagnosing a particular case and patient to identify a condition and suggest next steps. 
\f2 \uc0\u61607 
\f0  Contact center support: Machine responses offering personalized self-service experience for clients by dynamically developing personal profiles from unstructured data. 
\f2 \uc0\u61607 
\f0  Research and discovery: Identification of rare studies and information sources while building a case for original research, such as, for example, assisting the pharmaceutical industry to discover new drugs. 
\f2 \uc0\u61607 
\f0  Process optimization: Identification of areas for improvement in business processes by analyzing unstructured data that documents and describes process steps and output. 
\f2 \uc0\u61607 
\f0  Fraud and risk management: Identification of early signs of fraud and management of risk in order to lower overall liability and costs of doing business. \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 7 Machine Learning in Business Use Cases The next decade: a market for AI systems worth billions AI will matter as a result of initiatives in Type B AI that are showing success. These technologies will disrupt existing solutions on the market as AI-based systems outperform conventional technologies. They will be embedded in many applications, services, and devices. These software applications will assist humans and also replace humans in certain tasks that can be automated and require a low level of intelligence but benefit from enhanced (possibly super-human) pattern-recognition capabilities. Cognitive computing places the emphasis on computation, with advanced computer systems playing a key role. Natural language processing (NLP) is the application of computational linguistics to human language technology, such as, for example, the extraction of structured information from unstructured text. Neural networks are inspired by the brain\'92s neural structure, and run mostly as computer simulations or sometimes on special hardware, and are able to perform advanced AI tasks such as face recognition. The market for expert AI systems is not new. Early work in the 1990s saw successful expert AI systems, such as medical assistants, but these were rejected by professionals because they were perceived as job threatening. Now the situation has reversed, with professionals swamped with information and finding it difficult to stay abreast of essential new developments in their field. In these situations, an expert AI system can support a professional and make their work successful and their jobs more secure. The use of AI-based face recognition was first used by casinos to identify known Blackjack counters and other inconvenient winners. The technology is now used in surveillance in multiple ways, such as, for example, to recognize when crowds in confined spaces are becoming dangerously dense with a danger of suffocation, or recognizing and tracking individuals. Technology startups are springing up and also being acquired by the likes of Google, Facebook, and others, introducing a new generation of products that can exploit these AI advances. Many are applicable to data mining, big data, and predictive analytics. In the next decade these technologies will become less expensive and more prevalent in everyday experiences, including home, office, and travel. An under-explored area is the use of AI by artists in the creative arts (computer games and CGI apart), and we may see a new generation of artists inventing novel ways to use AI. Table 1: 2025 projection: technology analysis matrix for Artificial Intelligence Geography Customer 2015-2017 2018-2021 2022-2025 Emerging market Business AI-based decision support systems (IBM Watson, for example) as commercial services. Verticals include medicine/healthcare, science research, education, and others. AI advances will feed into robot brains. These machines will begin to be seen in offices and factories and beyond. There will be small improvement increments, still mining the deep learning algorithms. Creative AI will appear where artists explore new possibilities. Consumer AI-based consumer products/apps, such as music-recognition apps. AI is already used in Apple Siri/Google Voice. The Intelligent home is one where AI assists and automates, such as to record television programs, to cook, to play games. A breakthrough in instant speech recognition with 99% accuracy is likely, even under noisy environments. Developed market Business Financial algorithmic trading dominates high-frequency trading, and the Use of advanced AI assistants will be a norm in many products and software Robot brains infused with AI advances will become a normal sight in offices and \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 8 Machine Learning in Business Use Cases steady rise in use of AI systems will continue. applications, such as data mining. homes. Consumer Games have long been an enclave for AI and the ingenuity in AI game opponents will continue. Intelligent personal apps for use on smart devices will be a mushrooming consumer market. The intelligent home will create a market opportunity for AI machines, applications, maintenance requirements, and so on. Source: Ovum Increasing research in the past five years has been in the application of Type B AI to create hardware-based Type A AI-like systems. For example, Neuromorphic chips are being used to emulate as much of the brain\'92s structure as we currently understand. Some initiatives are less \'93blue-sky\'94 and more commercially focused, such as IBM\'92s TrueNorth chip. The next decade will see such chips assisting AI systems with massively parallel-processing capabilities. Table 1 shows the potential for AI over the next decade. The combination of deep learning algorithms accelerated on GPUs has been the pivotal breakthrough The ImageNet competition shows a 26% to 5% error rate improvement in the last five years The time it takes to train a neural network has an impact on the possible research undertaken. Clearly, the faster the training, the faster is progress achieved in the research. Progress before GPU usage was painfully slow, whereas GPUs now accelerate research. The range of deep learning-based neural network training times and how they affect research is summarized here. 
\f2 \uc0\u61607 
\f0  A month or greater: inhibitive to research, and characterizes pre-GPU days for large-scale networks. 
\f2 \uc0\u61607 
\f0  One to four weeks: High-value experiments only, still a significant lag on research and characterizes pre-GPU days for small to medium size networks. 
\f2 \uc0\u61607 
\f0  One to four days: This is now in the GPU enabled zone. Is acceptable and enables high degree of experimentation. 
\f2 \uc0\u61607 
\f0  Minutes to hours: In the real-time interactive research range, made possible with GPUs. Allows the fastest possible progress. One technique used in building a deep learning system includes using combinations of four of the major types of neural network learning algorithms: supervised learning, unsupervised learning, reinforcement learning, and recurrent learning. Convolution neural networks (CNNs, which go back to 1995) are a major part of deep learning systems and were found to yield a step change improvement when first used in 2012 by Krizhesvky et al. In 2015, all ImageNet contestants used deep learning with convolution neural networks. \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 9 Machine Learning in Business Use Cases A benchmark in testing AI systems is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), acompetition that started in 2010. It uses a fresh image library each year designed to stretch the capabilities of the AI systems. Each machine contestant is marked on the top 5 retrievals that are produced by the machine for a test image that it has never seen before. The most recent winning systems are shown in Table 2, and note the score achieved by a high-performing human. Table 2: ImageNet competition results: Year Winning entry % Top 5 error rate 2010 NEC Labs America 28.2 2011 Xerox Research Centre Europe 25.8 2012 Krizhesvky et al. 16.4 2013 Zeiler/Clarifai 11.7 2014 GoogLeNet 6.7 January 2015 Baidu 5.98 Human: Andrej Karpathy 5.1 February 2015 Microsoft Research 4.94 February 2015 Google 4.82 Source: Ovum combination of ImageNet results and Jeff Dean slide Google uses its research in deep learning to power Google Plus Photo search to find images related to a keyword, as well as other applications. For example, Google has a system that observes the real-world environment and reads the name of shops and other signs, and provides a text commentary or translates the text. The most recent examples, from Stanford University and Google, can observe an image, recognize its content, and write a descriptive caption, with a level of accuracy equaling humans (see Figure 3). Figure 3: Computer vision: Machine image recognition and descriptive captions generated Source: Oriol Vinyals (Google). RNN is a recurrent neural network. Nvidia is the main player in the GPU accelerated AI market Nvidia has a few years\'92 lead over its nearest rivals in the programmable GPU market with applications in AI, as well as other markets such as high-performance computing. In 2007 it launched its compute unified device architecture (CUDA), a programmable parallel computing platform implemented in GPUs. CUDA programming provides the fastest and most versatile way to access the power of Nvidia \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 10 Machine Learning in Business Use Cases GPUs, with core supported languages C, C++, and Fortran. Third-party wrappers are available for Python, Java, Ruby, Lua, Haskell, R, Matlab, Mathematica, and others. In 2008 there were 150,000 downloads, 27 CUDA applications, 4,000 academic papers citing CUDA GPUs, 6,000 Tesla GPUs, and 77 supercomputing TeraFLOPS. In March 2015 there were 3 million CUDA downloads, 319 CUDA applications, 60,000 academic papers citing the platform, 450,000 Tesla GPUs in the market, and 54,000 supercomputing TeraFLOPS. Nvidia announced its latest and fastest GPU at the GPU Technology Conference 2015. Titan X contains 3,072 CUDA cores, 8 billion transistors, 12 GB memory, and can achieve 7 TeraFLOPS in single precision. Its cost is $1,000. When combined with the new CUDA deep learning library (cuDNN), which provides primitives for connecting deep learning frameworks with CUDA, AlexNet, an example of a deep learning system, reduced its training time from a month to 2.5 days (see Figure 4). This is a huge degree of acceleration. Figure 4: Titan X GPU: training time for the deep learning system AlexNet Source: Nvidia The most popular deep learning frameworks that can be integrated with CUDA via cuDNN include Caffe, Theano, and Torch. The first two use the Python programming language, and the third uses Lua. Figure 5 shows a rendering inside an actual deep learning system of the images being processed at multiple layers, starting with the first layer on the left side which is shown many real-world images at the same time. The connections between the layers are not shown. The final layers on the right with the greatest detail are the first fully connected layers that combine the abstractions produced in earlier layers. This visualization demonstrates the hierarchical processing of images from edges to higher abstract entities. Nvidia has also launched DIGITS, an open source Interactive deep learning GPU Training System, which is a complete system aimed at data scientists and others, without having to write code. Its features include: \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 11 Machine Learning in Business Use Cases 
\f2 \uc0\u61607 
\f0  Visualization of deep neural network (DNN) topology and how training data activates a neural network. 
\f2 \uc0\u61607 
\f0  Management of training of many DNNs in parallel on multi-GPU systems. 
\f2 \uc0\u61607 
\f0  Simple setup and launch. 
\f2 \uc0\u61607 
\f0  Import of a wide variety of image formats and sources. 
\f2 \uc0\u61607 
\f0  Monitoring of network training in real time. 
\f2 \uc0\u61607 
\f0  Open source so DIGITS can be customized and extended as needed. Figure 5: Visualizing the inner workings of a deep learning system processing images Source: Nvidia Open source and sharing of knowledge is helping accelerate AI innovation Noticeable is the impact of the open source movement on the AI field. All the key academic papers on deep learning and related areas are now openly accessible. Source code is shared between research teams and deep learning open source libraries such as Caffe, Theano, and Torch are helping spread knowledge in AI. This sharing of knowledge is helping accelerate the take-up of these techniques and leading to startups with business applications. There are two kinds of AI and only one is practical for businesses Defining AI AI can generally mean one of two things: Type A or Type B. 
\f2 \uc0\u61607 
\f0  Type A means the creation of an artificial human brain. This is not the simulation of some aspect of a human brain, but instead a fully functioning human-like brain that thinks like we do. It may or may not have consciousness or self-awareness. One can get into philosophical \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 12 Machine Learning in Business Use Cases debates about whether consciousness is necessary for intelligence, so we leave that aside. However, there is one important point: science as it stands today does not fully understand how the human brain works, in fact we do not understand how the most primitive brains function in creatures with a very small number of neurons (worms, flies, for example), let alone in an average human brain with 100 billion neurons. It is unlikely that a type A AI will be built until we at least understand how nature\'92s \'93wet-ware\'94 brains work. 
\f2 \uc0\u61607 
\f0  Type B means the creation of advanced computer/machine learning systems, pattern recognition systems, and expert systems for largely scientific, medical, and engineering purposes. Type A research typically influences Type B research, but while Type A is still a distant goal, Type B has achieved some notable successes. Many names used for Type B AI are just synonyms, and reflect the preferences of different research groups including cognitive computing, machine learning, and computational intelligence. The next level down from AI the field is split between symbolic reasoning and massively parallel distributed processing. Much research effort and successes are occurring in the latter branch (Bayesian networks, evolutionary computation, genetic algorithms, and neural networks belong to this group). Dark side of AI AI advocates talk of the \'93singularity\'94 when humans will one day create a genuine Type A breakthrough, then their creation will have the capacity to create/invent the next level of AI brain, and this continues with ever more intelligent AI brains, transcending humans with every step. There is a genuine threat to human existence in such a scenario, if, for example, intelligent machines view humans as competitors to scarce resources. Given that the inner workings of neurons are still a mystery, we do not expect we will witness the AI singularity in the next 20, 30, or more years, but we do believe it is just a question of time and further research until the question of how the human brain thinks is cracked, and then the singularity becomes a genuine possibility. It is likely that in the next decade or two these types of concerns will lead to legislation and control of AI research as Type B successes bring AI to the fore. Advanced AI brain research will require licensing and auditing, and legislation will require in-built safety mechanisms to ensure humans are not harmed. For readers familiar with the \'93I Robot\'94 science fiction of Isaac Asimov, this is familiar territory. Asimov wrote about future robots that had a set of three in-built laws designed to prevent harm to humans. The paradox here is that it is likely that military robots will be built to replace soldiers on the battlefield. Appendix Further Reading \'93Nvidia emphasizes deep learning as a future market for GPUs\'94, IT0022-000331, March 2015 \'93Open source is accelerating artificial intelligence innovation\'94, IT0022-000328, March 2015 Beyond the Hype: Assessing the Evolution of Robotic Process Automation, IT0019-003367, September 2014 Author Michael Azoff, Principal Analyst, IT Infrastructure Solutions \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 13 Machine Learning in Business Use Cases michael.azoff@ovum.com Ovum Consulting We hope that this analysis will help you make informed and imaginative business decisions. If you have further requirements, Ovum\'92s consulting team may be able to help you. For more information about Ovum\'92s consulting capabilities, please contact us directly at consulting@ovum.com. Copyright notice and disclaimer The contents of this product are protected by international copyright laws, database rights and other intellectual property rights. The owner of these rights is Informa Telecoms and Media Limited, our affiliates or other third party licensors. All product and company names and logos contained within or appearing on this product are the trademarks, service marks or trading names of their respective owners, including Informa Telecoms and Media Limited. This product may not be copied, reproduced, distributed or transmitted in any form or by any means without the prior permission of Informa Telecoms and Media Limited. Whilst reasonable efforts have been made to ensure that the information and content of this product was correct as at the date of first publication, neither Informa Telecoms and Media Limited nor any person engaged or employed by Informa Telecoms and Media Limited accepts any liability for any errors, omissions or other inaccuracies. Readers should independently verify any facts and figures as no liability can be accepted in this regard \'96 readers assume full responsibility and risk accordingly for their use of such information and content. Any views and/or opinions expressed in this product by individual authors or contributors are their personal views and/or opinions and do not necessarily reflect the views and/or opinions of Informa Telecoms and Media Limited. \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 14 Machine Learning in Business Use Cases CONTACT US www.ovum.com analystsupport@ovum.com INTERNATIONAL OFFICES Beijing Dubai Hong Kong Hyderabad Johannesburg London Melbourne New York San Francisco Sao Paulo Tokyo \'a9 Ovum. All rights reserved. Unauthorized reproduction prohibited. Page 15}